{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80247d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime, date\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from model_classes_ import GCN_classification, GCN_regression\n",
    "from baseline_mdl_classes import MLP_classification, MLP_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c68d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62490240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_classification(model, graph, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer):\n",
    "    '''\n",
    "    Trains model for classification task\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model [GCN_classification]: Instantiation of model class\n",
    "    graph [PyG Data class]: PyTorch Geometric Data object representing the graph\n",
    "    max_epoch [int]: Maximum number of training epochs\n",
    "    learning_rate [float]: Learning rate\n",
    "    targetNode_mask [tensor]: Subgraph mask for training nodes\n",
    "    train_idx [array]: Node IDs corresponding to training set\n",
    "    valid_idx [array]: Node IDs corresponding to validation set\n",
    "    optimizer [PyTorch optimizer class]: PyTorch optimization algorithm\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_loss_vec [array]: Training loss for each epoch\n",
    "    train_AUROC_vec [array]: Training AUROC score for each epoch\n",
    "    valid_loss_vec [array]: Validation loss for each epoch\n",
    "    valid_AUROC_vec [array]: Validation AUROC score for each epoch\n",
    "\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "\n",
    "    optimizer = optimizer\n",
    "    \n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_AUROC_vec = np.zeros(np.shape(np.arange(max_epoch)))\n",
    "    valid_loss_list = []\n",
    "    valid_AUROC_vec = np.zeros(np.shape(np.arange(max_epoch)))\n",
    "\n",
    "    model.train()\n",
    "    train_status = True\n",
    "    \n",
    "    print('\\n')\n",
    "    for e in list(range(max_epoch)):\n",
    "        \n",
    "        if e%100 == 0:\n",
    "            print(\"Epoch\", str(e), 'out of', str(max_epoch))\n",
    "        \n",
    "        model.train()\n",
    "        train_status = True\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ### Only trains on nodes with genes due to masking\n",
    "        forward_scores = model(graph.x.float(), graph.edge_index, train_status)[targetNode_mask]\n",
    "        \n",
    "        train_scores = forward_scores[train_idx]\n",
    "\n",
    "        train_loss  = model.loss(train_scores, torch.LongTensor(train_labels).to(device))\n",
    "\n",
    "        train_softmax, _ = model.calc_softmax_pred(train_scores)\n",
    "\n",
    "        train_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "            \n",
    "        ### Calculate training and validation loss, AUROC scores\n",
    "        model.eval()\n",
    "        \n",
    "        valid_scores = forward_scores[valid_idx]\n",
    "        valid_loss  = model.loss(valid_scores, torch.LongTensor(valid_labels).to(device))\n",
    "        valid_softmax, _ = model.calc_softmax_pred(valid_scores) \n",
    "\n",
    "        train_loss_list.append(train_loss.item())\n",
    "        train_softmax = to_cpu_npy(train_softmax)\n",
    "        train_AUROC = roc_auc_score(train_labels, train_softmax[:,1], average=\"micro\")\n",
    "\n",
    "        valid_loss_list.append(valid_loss.item())\n",
    "        valid_softmax = to_cpu_npy(valid_softmax)\n",
    "        valid_AUROC = roc_auc_score(valid_labels, valid_softmax[:,1], average=\"micro\")\n",
    "        \n",
    "        train_AUROC_vec[e] = train_AUROC\n",
    "        valid_AUROC_vec[e] = valid_AUROC\n",
    "\n",
    "    train_loss_vec = np.reshape(np.array(train_loss_list), (-1, 1))\n",
    "    valid_loss_vec = np.reshape(np.array(valid_loss_list), (-1, 1))\n",
    "\n",
    "    return train_loss_vec, train_AUROC_vec, valid_loss_vec, valid_AUROC_vec\n",
    "\n",
    "\n",
    "def eval_model_classification(model, graph, targetNode_mask, train_idx, valid_idx, test_idx):\n",
    "    '''\n",
    "    Runs fully trained classification model and compute evaluation statistics\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model [GCN_classification]: Instantiation of model class\n",
    "    graph [PyG Data class]: PyTorch Geometric Data object representing the graph\n",
    "    targetNode_mask [tensor]: Mask ensuring model only trains on nodes with genes\n",
    "    train_idx [array]: Node IDs corresponding to training set;\n",
    "        analogous for valid_idx and test_idx\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    test_AUROC [float]: Test set AUROC score;\n",
    "        analogous for train_AUROC (training set) and valid_AUPR (validation set)\n",
    "    test_AUPR [float]: Test set AUPR score\n",
    "        analogous for train_AUPR (training set) and valid_AUPR (validation set)\n",
    "    test_pred [array]: Test set predictions;\n",
    "        analogous for train_pred (training set) and valid_pred (validation set)\n",
    "    test_labels [array]: Test set labels;\n",
    "        analagous for train_labels (training set) and valid_labels (validation set)\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "    test_labels = to_cpu_npy(graph.y[targetNode_mask[test_idx]])\n",
    "    \n",
    "    model.eval()\n",
    "    train_status=False\n",
    "\n",
    "    forward_scores = model(graph.x.float(), graph.edge_index, train_status)[targetNode_mask]\n",
    "\n",
    "    test_scores = forward_scores[test_idx]\n",
    "    test_softmax, test_pred = model.calc_softmax_pred(test_scores) \n",
    "    \n",
    "    test_softmax = to_cpu_npy(test_softmax)\n",
    "    test_pred = to_cpu_npy(test_pred)\n",
    "    test_AUROC = roc_auc_score(test_labels, test_softmax[:,1], average=\"micro\")\n",
    "    test_precision, test_recall, thresholds = precision_recall_curve(test_labels, test_softmax[:,1])\n",
    "    test_AUPR = auc(test_recall, test_precision)\n",
    "    # test_F1 = f1_score(test_labels, test_pred, average=\"micro\")\n",
    "    \n",
    "    train_scores = forward_scores[train_idx]\n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    train_softmax, train_pred = model.calc_softmax_pred(train_scores) \n",
    "    train_pred = to_cpu_npy(train_pred)\n",
    "    train_softmax = to_cpu_npy(train_softmax)\n",
    "    train_precision, train_recall, thresholds = precision_recall_curve(train_labels, train_softmax[:,1])\n",
    "    train_AUPR = auc(train_recall, train_precision)\n",
    "    # train_F1 = f1_score(train_labels, train_pred, average=\"micro\")\n",
    "\n",
    "    valid_scores = forward_scores[valid_idx]\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    valid_softmax, valid_pred = model.calc_softmax_pred(valid_scores) \n",
    "    valid_pred = to_cpu_npy(valid_pred)\n",
    "    valid_softmax = to_cpu_npy(valid_softmax)\n",
    "    valid_precision, valid_recall, thresholds = precision_recall_curve(valid_labels, valid_softmax[:,1])\n",
    "    valid_AUPR = auc(valid_recall, valid_precision)\n",
    "    # valid_F1 = f1_score(valid_labels, valid_pred, average=\"micro\")\n",
    "\n",
    "    return test_AUROC, test_AUPR, test_pred, test_labels, train_AUPR, train_pred, train_labels, \\\n",
    "        valid_AUPR, valid_pred, valid_labels\n",
    "\n",
    "\n",
    "def train_model_regression(model, graph, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer):\n",
    "    '''\n",
    "    Trains model for regression task\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model [GCN_classification]: Instantiation of model class\n",
    "    graph [PyG Data class]: PyTorch Geometric Data object representing the graph\n",
    "    max_epoch [int]: Maximum number of training epochs\n",
    "    learning_rate [float]: Learning rate\n",
    "    targetNode_mask [tensor]: Subgraph mask for training nodes\n",
    "    train_idx [array]: Node IDs corresponding to training set\n",
    "    valid_idx [array]: Node IDs corresponding to validation set\n",
    "    optimizer [PyTorch optimizer class]: PyTorch optimization algorithm\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_loss_vec [array]: Training loss for each epoch;\n",
    "        analagous for valid_loss_vec (validation set)\n",
    "    train_pearson_vec [array]: Training PCC for each epoch;\n",
    "        analogous for valid_pearson_vec (validation set)\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "\n",
    "    optimizer = optimizer\n",
    "    \n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_pearson_vec = np.zeros(np.shape(np.arange(max_epoch)))\n",
    "    valid_loss_list = []\n",
    "    valid_pearson_vec = np.zeros(np.shape(np.arange(max_epoch)))\n",
    "\n",
    "    model.train()\n",
    "    train_status = True\n",
    "    \n",
    "    print('\\n')\n",
    "    for e in list(range(max_epoch)):\n",
    "        \n",
    "        if e%100 == 0:\n",
    "            print(\"Epoch\", str(e), 'out of', str(max_epoch))\n",
    "        \n",
    "        model.train()\n",
    "        train_status = True\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ### Only trains on nodes with genes due to masking\n",
    "        forward_scores = model(graph.x.float(), graph.edge_index, train_status)[targetNode_mask]\n",
    "        \n",
    "        train_scores = forward_scores[train_idx]\n",
    "\n",
    "        train_loss  = model.loss(train_scores, torch.FloatTensor(train_labels).to(device))\n",
    "\n",
    "        train_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "            \n",
    "        ### Calculate training and validation loss, AUROC scores\n",
    "        model.eval()\n",
    "        \n",
    "        train_scores = to_cpu_npy(train_scores)\n",
    "        train_pearson = calc_pearson(train_scores, train_labels)\n",
    "        train_loss_list.append(train_loss.item())\n",
    "        \n",
    "        valid_scores = forward_scores[valid_idx]\n",
    "        valid_loss  = model.loss(valid_scores, torch.FloatTensor(valid_labels).to(device))\n",
    "        valid_scores = to_cpu_npy(valid_scores)\n",
    "        valid_pearson  = calc_pearson(valid_scores, valid_labels)\n",
    "        valid_loss_list.append(valid_loss.item())\n",
    "        \n",
    "        train_pearson_vec[e] = train_pearson\n",
    "        valid_pearson_vec[e] = valid_pearson\n",
    "\n",
    "    train_loss_vec = np.reshape(np.array(train_loss_list), (-1, 1))\n",
    "    valid_loss_vec = np.reshape(np.array(valid_loss_list), (-1, 1))\n",
    "\n",
    "    return train_loss_vec, train_pearson_vec, valid_loss_vec, valid_pearson_vec\n",
    "\n",
    "\n",
    "def eval_model_regression(model, graph, targetNode_mask, train_idx, valid_idx, test_idx):\n",
    "    '''\n",
    "    Runs fully trained regression model and compute evaluation statistics\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model [GCN_classification]: Instantiation of model class\n",
    "    graph [PyG Data class]: PyTorch Geometric Data object representing the graph\n",
    "    targetNode_mask [tensor]: Mask ensuring model only trains on nodes with genes\n",
    "    train_idx [array]: Node IDs corresponding to training set;\n",
    "        analogous for valid_idx and test_idx\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    test_pearson [float]: PCC for test set;\n",
    "        analogous for train_pearson (training set) and valid_pearson (validation set)\n",
    "    test_pred [array]: Test set predictions;\n",
    "        analogous for train_pred (training set) and valid_pred (validation set)\n",
    "    test_labels [array]: Test set labels (expression values);\n",
    "        analagous for train_labels (training set) and valid_labels (validation set)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    train_status=False\n",
    "\n",
    "    forward_scores = model(graph.x.float(), graph.edge_index, train_status)[targetNode_mask]\n",
    "\n",
    "    test_scores = forward_scores[test_idx]\n",
    "    test_pred = to_cpu_npy(test_scores)\n",
    "    test_labels = to_cpu_npy(graph.y[targetNode_mask[test_idx]])\n",
    "    test_pearson = calc_pearson(test_pred, test_labels)\n",
    "\n",
    "    train_scores = forward_scores[train_idx]\n",
    "    train_pred = to_cpu_npy(train_scores)\n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    train_pearson = calc_pearson(train_pred, train_labels)\n",
    "\n",
    "    valid_scores = forward_scores[valid_idx]\n",
    "    valid_pred = to_cpu_npy(valid_scores)\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    valid_pearson = calc_pearson(valid_pred, valid_labels)\n",
    "\n",
    "    return test_pearson, test_pred, test_labels, train_pearson, train_pred, train_labels, \\\n",
    "        valid_pearson, valid_pred, valid_labels\n",
    "        \n",
    "\n",
    "def calc_pearson(scores, targets):\n",
    "    '''\n",
    "    Calculates Pearson correlation coefficient (PCC) between predicted \\\n",
    "        expression levels and true expression levels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scores [array]: Predicted expression levels\n",
    "    targets [array]: True expression levels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pcc [float]: Pearson correlation coefficient\n",
    "\n",
    "    '''\n",
    "    pcc, _ = pearsonr(scores, targets)\n",
    "            \n",
    "    return pcc\n",
    "    \n",
    "    \n",
    "def to_cpu_npy(x):\n",
    "    '''\n",
    "    Simple helper function to transfer GPU tensors to CPU numpy matrices\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x [tensor]: PyTorch tensor stored on GPU\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_x [array]: Numpy array stored on CPU\n",
    "\n",
    "    '''\n",
    "\n",
    "    new_x = x.cpu().detach().numpy()\n",
    "    \n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f71be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_line = 'E116'\n",
    "max_epoch = 1000\n",
    "learning_rate = 1e-4\n",
    "num_graph_conv_layers = 2\n",
    "graph_conv_embed_size = 256\n",
    "num_lin_layers = 3\n",
    "lin_hidden_size = 256\n",
    "regression_flag = 0\n",
    "random_seed = 0\n",
    "\n",
    "chip_res = 10000\n",
    "hic_res = 10000\n",
    "num_hm = 6\n",
    "num_feat = int((hic_res/chip_res)*num_hm)\n",
    "num_classes = 2 if regression_flag == 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d644292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell line: E116\n",
      "ChIP-seq resolution: 10000\n",
      "\n",
      "\n",
      "Training set: 70%\n",
      "Validation set: 15%\n",
      "Testing set: 15%\n",
      "\n",
      "\n",
      "Model hyperparameters: \n",
      "Number of epochs: 1000\n",
      "Learning rate: 0.0001\n",
      "Number of graph convolutional layers: 2\n",
      "Graph convolutional embedding size: 256\n",
      "Number of linear layers: 3\n",
      "Linear hidden layer size: 256\n"
     ]
    }
   ],
   "source": [
    "print('Cell line:', cell_line)\n",
    "print('ChIP-seq resolution:', str(chip_res))\n",
    "print('\\n')\n",
    "print('Training set: 70%')\n",
    "print('Validation set: 15%')\n",
    "print('Testing set: 15%')\n",
    "print('\\n')\n",
    "print('Model hyperparameters: ')\n",
    "print('Number of epochs:', max_epoch)\n",
    "print('Learning rate:', learning_rate)\n",
    "print('Number of graph convolutional layers:', str(num_graph_conv_layers))\n",
    "print('Graph convolutional embedding size:', graph_conv_embed_size)\n",
    "print('Number of linear layers:', str(num_lin_layers))\n",
    "print('Linear hidden layer size:', lin_hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a148bdd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fba5620df10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random_seed = random.randint(0,10000)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5526b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "today = date.today()\n",
    "mdy = today.strftime(\"%Y-%m-%d\")\n",
    "clock = datetime.now()\n",
    "hms = clock.strftime(\"%H-%M-%S\")\n",
    "hm = clock.strftime(\"%Hh-%Mm\")\n",
    "hm_colon = clock.strftime(\"%H:%M\")\n",
    "date_and_time = mdy + '-at-' + hms\n",
    "\n",
    "\n",
    "###Test for GPU availability\n",
    "cuda_flag = torch.cuda.is_available()\n",
    "if cuda_flag:  \n",
    "  dev = \"cuda\" \n",
    "else:\n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev)  \n",
    "\n",
    "\n",
    "###Load input files\n",
    "def prepare_data(cell_line, regression_flag):\n",
    "    base_path = os.getcwd()\n",
    "    save_dir = os.path.join(base_path, 'data', cell_line, 'saved_runs')\n",
    "    hic_sparse_mat_file = os.path.join(base_path, 'data', cell_line, 'hic_sparse.npz')\n",
    "    np_nodes_lab_genes_file = os.path.join(base_path, 'data',  cell_line, \\\n",
    "        'np_nodes_lab_genes_reg' + str(regression_flag) + '.npy')\n",
    "    np_hmods_norm_all_file = os.path.join(base_path, 'data', cell_line, \\\n",
    "        'np_hmods_norm_chip_' + str(chip_res) + 'bp.npy')\n",
    "    df_genes_file = os.path.join(base_path, 'data', cell_line, 'df_genes_reg' + str(regression_flag) + '.pkl')\n",
    "    df_genes = pd.read_pickle(df_genes_file)\n",
    "    \n",
    "    mat = load_npz(hic_sparse_mat_file)\n",
    "    allNodes_hms = np.load(np_hmods_norm_all_file)\n",
    "    hms = allNodes_hms[:, 1:] #only includes features, not node ids\n",
    "    X = torch.tensor(hms).float().reshape(-1, num_feat) \n",
    "    allNodes = allNodes_hms[:, 0].astype(int)\n",
    "    geneNodes_labs = np.load(np_nodes_lab_genes_file)\n",
    "\n",
    "    geneNodes = geneNodes_labs[:, -2].astype(int)\n",
    "    allLabs = -1*np.ones(np.shape(allNodes))\n",
    "\n",
    "    targetNode_mask = torch.tensor(geneNodes).long()\n",
    "\n",
    "    if regression_flag == 0:\n",
    "        geneLabs = geneNodes_labs[:, -1].astype(int)\n",
    "        allLabs[geneNodes] = geneLabs\n",
    "        Y = torch.tensor(allLabs).long()\n",
    "    else:\n",
    "        geneLabs = geneNodes_labs[:, -1].astype(float)\n",
    "        allLabs[geneNodes] = geneLabs\n",
    "        Y = torch.tensor(allLabs).float()\n",
    "\n",
    "    extract = torch_geometric.utils.from_scipy_sparse_matrix(mat)\n",
    "    data = torch_geometric.data.Data(edge_index = extract[0], edge_attr = extract[1], x = X, y = Y)\n",
    "    G = data\n",
    "    \n",
    "    ###Randomize node order and split into 70%/15%/15% training/validation/test sets\n",
    "    pred_idx_shuff = torch.randperm(targetNode_mask.shape[0])\n",
    "    fin_train = np.floor(0.7*pred_idx_shuff.shape[0]).astype(int)\n",
    "    fin_valid = np.floor(0.85*pred_idx_shuff.shape[0]).astype(int)\n",
    "    train_idx = pred_idx_shuff[:fin_train]\n",
    "    valid_idx = pred_idx_shuff[fin_train:fin_valid]\n",
    "    test_idx = pred_idx_shuff[fin_valid:]\n",
    "    \n",
    "    return G, targetNode_mask, train_idx, valid_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93670d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 256, 256, 2]\n"
     ]
    }
   ],
   "source": [
    "lin_hidden_sizes = [num_feat] + [int(max(lin_hidden_size, num_classes)) for i in np.arange(1, num_lin_layers, 1)] + [num_classes]\n",
    "\n",
    "print(lin_hidden_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a970e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "G, targetNode_mask, train_idx, valid_idx, test_idx = prepare_data(cell_line='E116', regression_flag = regression_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b069a49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model's state_dict:\n",
      "lin1.weight \t torch.Size([256, 6])\n",
      "lin1.bias \t torch.Size([256])\n",
      "lin2.weight \t torch.Size([256, 256])\n",
      "lin2.bias \t torch.Size([256])\n",
      "lin3.weight \t torch.Size([2, 256])\n",
      "lin3.bias \t torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "model = MLP_classification(num_feat, num_lin_layers, lin_hidden_sizes, num_classes)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), \n",
    "                            lr = learning_rate)\n",
    "\n",
    "print(\"\\n\"+\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b6cb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 0 out of 1000\n"
     ]
    }
   ],
   "source": [
    "### Train model\n",
    "train_loss_vec, train_AUROC_vec, valid_loss_vec, valid_AUROC_vec = \\\n",
    "    train_model_classification(model, G, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer)\n",
    "\n",
    "### Evaluate model\n",
    "test_AUROC, test_AUPR, test_pred, test_labels, train_AUPR, train_pred, train_labels, \\\n",
    "        valid_AUPR, valid_pred, valid_labels = \\\n",
    "            eval_model_classification(model, G, targetNode_mask, train_idx, valid_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7052305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Classification Performance for {cell_line}:\\n')\n",
    "print('Test AUROC:', test_AUROC, '\\n')\n",
    "print('Test AUPR:', test_AUPR, '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a49e9263",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLP_Regression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_150/2256671119.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetNode_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_line\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'E116'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP_Regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_lin_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlin_hidden_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), \n",
      "\u001b[0;31mNameError\u001b[0m: name 'MLP_Regression' is not defined"
     ]
    }
   ],
   "source": [
    "G, targetNode_mask, train_idx, valid_idx, test_idx = prepare_data(cell_line='E116', regression_flag = 1)\n",
    "\n",
    "model = MLP_regression(num_feat, num_lin_layers, lin_hidden_sizes, num_classes)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), \n",
    "                            lr = learning_rate)\n",
    "\n",
    "print(\"\\n\"+\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc31b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_vec, train_pearson_vec, valid_loss_vec, valid_pearson_vec = \\\n",
    "        train_model_regression(model, G, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer)\n",
    "    \n",
    "test_pearson, test_pred, test_labels, train_pearson, train_pred, train_labels, \\\n",
    "        valid_pearson, valid_pred, valid_labels = \\\n",
    "            eval_model_regression(model, G, targetNode_mask, train_idx, valid_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c55fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Regression Performance for {cell_line}:\\n')\n",
    "print('Test Pearson:', test_pearson, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a238fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_conv_embed_size = 256\n",
    "num_graph_conv_layers = 2\n",
    "\n",
    "graph_conv_layer_sizes = [num_feat] + \\\n",
    "    [int(max(graph_conv_embed_size, lin_hidden_size)) \\\n",
    "          for i in np.arange(1, num_graph_conv_layers, 1)] + [lin_hidden_size]\n",
    "\n",
    "graph_lin_hidden_sizes = [graph_conv_layer_sizes[-1]] + \\\n",
    "    [int(max(lin_hidden_size, num_classes)) \\\n",
    "          for i in np.arange(1, num_lin_layers, 1)] + [num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "417d308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_lines = ['E116', 'E122', 'E123']\n",
    "classification_res = pd.DataFrame(columns=cell_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "734be5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Cell Line E116...\n",
      "\n",
      "\n",
      "Epoch 0 out of 1000\n",
      "Epoch 100 out of 1000\n",
      "Epoch 200 out of 1000\n",
      "Epoch 300 out of 1000\n",
      "Epoch 400 out of 1000\n",
      "Epoch 500 out of 1000\n",
      "Epoch 600 out of 1000\n",
      "Epoch 700 out of 1000\n",
      "Epoch 800 out of 1000\n",
      "Epoch 900 out of 1000\n",
      "\n",
      "Training Cell Line E122...\n",
      "\n",
      "\n",
      "Epoch 0 out of 1000\n",
      "Epoch 100 out of 1000\n",
      "Epoch 200 out of 1000\n",
      "Epoch 300 out of 1000\n",
      "Epoch 400 out of 1000\n",
      "Epoch 500 out of 1000\n",
      "Epoch 600 out of 1000\n",
      "Epoch 700 out of 1000\n",
      "Epoch 800 out of 1000\n",
      "Epoch 900 out of 1000\n",
      "\n",
      "Training Cell Line E123...\n",
      "\n",
      "\n",
      "Epoch 0 out of 1000\n",
      "Epoch 100 out of 1000\n",
      "Epoch 200 out of 1000\n",
      "Epoch 300 out of 1000\n",
      "Epoch 400 out of 1000\n",
      "Epoch 500 out of 1000\n",
      "Epoch 600 out of 1000\n",
      "Epoch 700 out of 1000\n",
      "Epoch 800 out of 1000\n",
      "Epoch 900 out of 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E116</th>\n",
       "      <th>E122</th>\n",
       "      <th>E123</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GCN</th>\n",
       "      <td>0.910601</td>\n",
       "      <td>0.901727</td>\n",
       "      <td>0.926081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         E116      E122      E123\n",
       "GCN  0.910601  0.901727  0.926081"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn_auroc = []\n",
    "for cell_line in cell_lines:\n",
    "    print(f'\\nTraining Cell Line {cell_line}...')\n",
    "    G, targetNode_mask, train_idx, valid_idx, test_idx = prepare_data(cell_line=cell_line, regression_flag = regression_flag)\n",
    "    \n",
    "    model = GCN_classification(num_feat, num_graph_conv_layers, graph_conv_layer_sizes, num_lin_layers, graph_lin_hidden_sizes, num_classes)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), lr = learning_rate)\n",
    "    \n",
    "    train_loss_vec, train_AUROC_vec, valid_loss_vec, valid_AUROC_vec = \\\n",
    "    train_model_classification(model, G, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer)\n",
    "\n",
    "    test_AUROC, test_AUPR, test_pred, test_labels, train_AUPR, train_pred, train_labels, \\\n",
    "        valid_AUPR, valid_pred, valid_labels = \\\n",
    "            eval_model_classification(model, G, targetNode_mask, train_idx, valid_idx, test_idx)\n",
    "    \n",
    "    gcn_auroc.append(test_AUROC)\n",
    "    \n",
    "classification_res.loc['GCN'] = gcn_auroc\n",
    "classification_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b00bfec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch 0 out of 1000\n",
      "Epoch 100 out of 1000\n",
      "Epoch 200 out of 1000\n",
      "Epoch 300 out of 1000\n",
      "Epoch 400 out of 1000\n",
      "Epoch 500 out of 1000\n",
      "Epoch 600 out of 1000\n",
      "Epoch 700 out of 1000\n",
      "Epoch 800 out of 1000\n",
      "Epoch 900 out of 1000\n",
      "\n",
      "\n",
      "Epoch 0 out of 1000\n",
      "Epoch 100 out of 1000\n",
      "Epoch 200 out of 1000\n",
      "Epoch 300 out of 1000\n",
      "Epoch 400 out of 1000\n",
      "Epoch 500 out of 1000\n",
      "Epoch 600 out of 1000\n",
      "Epoch 700 out of 1000\n",
      "Epoch 800 out of 1000\n",
      "Epoch 900 out of 1000\n",
      "\n",
      "\n",
      "Epoch 0 out of 1000\n",
      "Epoch 100 out of 1000\n",
      "Epoch 200 out of 1000\n",
      "Epoch 300 out of 1000\n",
      "Epoch 400 out of 1000\n",
      "Epoch 500 out of 1000\n",
      "Epoch 600 out of 1000\n",
      "Epoch 700 out of 1000\n",
      "Epoch 800 out of 1000\n",
      "Epoch 900 out of 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E116</th>\n",
       "      <th>E122</th>\n",
       "      <th>E123</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GCN</th>\n",
       "      <td>0.910601</td>\n",
       "      <td>0.901727</td>\n",
       "      <td>0.926081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>0.905455</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.909438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         E116      E122      E123\n",
       "GCN  0.910601  0.901727  0.926081\n",
       "MLP  0.905455  0.893617  0.909438"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_auroc = []\n",
    "\n",
    "for cell_line in ['E116', 'E122', 'E123']:\n",
    "    print(f'\\nTraining Cell Line {cell_line}...')\n",
    "    G, targetNode_mask, train_idx, valid_idx, test_idx = prepare_data(cell_line=cell_line, regression_flag = regression_flag)\n",
    "    \n",
    "    model = MLP_classification(num_feat, num_lin_layers, lin_hidden_sizes, num_classes)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), lr = learning_rate)\n",
    "    \n",
    "    train_loss_vec, train_AUROC_vec, valid_loss_vec, valid_AUROC_vec = \\\n",
    "    train_model_classification(model, G, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer)\n",
    "\n",
    "    test_AUROC, test_AUPR, test_pred, test_labels, train_AUPR, train_pred, train_labels, \\\n",
    "        valid_AUPR, valid_pred, valid_labels = \\\n",
    "            eval_model_classification(model, G, targetNode_mask, train_idx, valid_idx, test_idx)\n",
    "    \n",
    "    mlp_auroc.append(test_AUROC)\n",
    "    \n",
    "classification_res.loc['MLP'] = mlp_auroc\n",
    "classification_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c3eb46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cell_line_alias = {'E116': 'GM1287', 'E123': 'K562', 'E122': 'HUVEC'}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(classification_res.columns))\n",
    "\n",
    "# Plot each model's scores as a separate set of bars\n",
    "for i, model in enumerate(classification_res.index):\n",
    "    ax.bar(index + i * bar_width, classification_res.loc[model], bar_width, label=model)\n",
    "\n",
    "# Add labels, title, and legend\n",
    "ax.set_xlabel('Cell Lines')\n",
    "ax.set_ylabel('AUROC')\n",
    "ax.set_xticks(index + bar_width * (len(classification_res.index) - 1) / 2)\n",
    "ax.set_ylim(0.8, 0.95)\n",
    "ax.set_xticklabels([cell_line_alias[cell_line] for cell_line in classification_res.columns] )\n",
    "ax.legend(title='Model')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451f237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
