{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef160cd-b3ca-45b7-b73f-0e6d26299fa9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.typing import OptPairTensor, Adj, Size\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.nn.conv import GATConv\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "from typing import Union, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve, auc\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from all_models import GCN_regression, GCN_classification, GCN_classification_weighted, MLP_regression, MLP_classification, GAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886693c-e669-4d8d-b741-06f646008aee",
   "metadata": {},
   "source": [
    "### Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b162ee-30b6-4570-afc1-96c343d94bd7",
   "metadata": {},
   "source": [
    "#### Helper Functions to train/evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e06f7fe3-8f7d-41f9-ad4d-8be4cb7aac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_cpu_npy(x):\n",
    "    return x.cpu().detach().numpy()\n",
    "\n",
    "def train_model_classification(model, graph, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer):\n",
    "    '''\n",
    "    Trains model for classification task\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model [GCN_classification]: Instantiation of model class\n",
    "    graph [PyG Data class]: PyTorch Geometric Data object representing the graph\n",
    "    max_epoch [int]: Maximum number of training epochs\n",
    "    learning_rate [float]: Learning rate\n",
    "    targetNode_mask [tensor]: Subgraph mask for training nodes\n",
    "    train_idx [array]: Node IDs corresponding to training set\n",
    "    valid_idx [array]: Node IDs corresponding to validation set\n",
    "    optimizer [PyTorch optimizer class]: PyTorch optimization algorithm\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_loss_vec [array]: Training loss for each epoch\n",
    "    train_AUROC_vec [array]: Training AUROC score for each epoch\n",
    "    valid_loss_vec [array]: Validation loss for each epoch\n",
    "    valid_AUROC_vec [array]: Validation AUROC score for each epoch\n",
    "\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "\n",
    "    optimizer = optimizer\n",
    "    \n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_AUROC_vec = np.zeros(np.shape(np.arange(max_epoch)))\n",
    "    valid_loss_list = []\n",
    "    valid_AUROC_vec = np.zeros(np.shape(np.arange(max_epoch)))\n",
    "\n",
    "    model.train()\n",
    "    train_status = True\n",
    "    \n",
    "    print('\\n')\n",
    "    for e in list(range(max_epoch)):\n",
    "        \n",
    "        if e%100 == 0:\n",
    "            print(\"Epoch\", str(e), 'out of', str(max_epoch))\n",
    "        \n",
    "        model.train()\n",
    "        train_status = True\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ### Only trains on nodes with genes due to masking\n",
    "        forward_scores = model(graph.x.float(), graph.edge_index, train_status)[targetNode_mask]\n",
    "        \n",
    "        train_scores = forward_scores[train_idx]\n",
    "\n",
    "        train_loss  = model.loss(train_scores, torch.LongTensor(train_labels).to(device))\n",
    "\n",
    "        train_softmax, _ = model.calc_softmax_pred(train_scores)\n",
    "\n",
    "        train_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "            \n",
    "        ### Calculate training and validation loss, AUROC scores\n",
    "        model.eval()\n",
    "        \n",
    "        valid_scores = forward_scores[valid_idx]\n",
    "        valid_loss  = model.loss(valid_scores, torch.LongTensor(valid_labels).to(device))\n",
    "        valid_softmax, _ = model.calc_softmax_pred(valid_scores) \n",
    "\n",
    "        train_loss_list.append(train_loss.item())\n",
    "        train_softmax = to_cpu_npy(train_softmax)\n",
    "        train_AUROC = roc_auc_score(train_labels, train_softmax[:,1], average=\"micro\")\n",
    "\n",
    "        valid_loss_list.append(valid_loss.item())\n",
    "        valid_softmax = to_cpu_npy(valid_softmax)\n",
    "        valid_AUROC = roc_auc_score(valid_labels, valid_softmax[:,1], average=\"micro\")\n",
    "        \n",
    "        train_AUROC_vec[e] = train_AUROC\n",
    "        valid_AUROC_vec[e] = valid_AUROC\n",
    "\n",
    "    train_loss_vec = np.reshape(np.array(train_loss_list), (-1, 1))\n",
    "    valid_loss_vec = np.reshape(np.array(valid_loss_list), (-1, 1))\n",
    "\n",
    "    return train_loss_vec, train_AUROC_vec, valid_loss_vec, valid_AUROC_vec\n",
    "\n",
    "\n",
    "def eval_model_classification(model, graph, targetNode_mask, train_idx, valid_idx, test_idx):\n",
    "    '''\n",
    "    Runs fully trained classification model and compute evaluation statistics\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model [GCN_classification]: Instantiation of model class\n",
    "    graph [PyG Data class]: PyTorch Geometric Data object representing the graph\n",
    "    targetNode_mask [tensor]: Mask ensuring model only trains on nodes with genes\n",
    "    train_idx [array]: Node IDs corresponding to training set;\n",
    "        analogous for valid_idx and test_idx\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    test_AUROC [float]: Test set AUROC score;\n",
    "        analogous for train_AUROC (training set) and valid_AUPR (validation set)\n",
    "    test_AUPR [float]: Test set AUPR score\n",
    "        analogous for train_AUPR (training set) and valid_AUPR (validation set)\n",
    "    test_pred [array]: Test set predictions;\n",
    "        analogous for train_pred (training set) and valid_pred (validation set)\n",
    "    test_labels [array]: Test set labels;\n",
    "        analagous for train_labels (training set) and valid_labels (validation set)\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "    test_labels = to_cpu_npy(graph.y[targetNode_mask[test_idx]])\n",
    "    \n",
    "    model.eval()\n",
    "    train_status=False\n",
    "\n",
    "    forward_scores = model(graph.x.float(), graph.edge_index, train_status)[targetNode_mask]\n",
    "\n",
    "    test_scores = forward_scores[test_idx]\n",
    "    test_softmax, test_pred = model.calc_softmax_pred(test_scores) \n",
    "    \n",
    "    test_softmax = to_cpu_npy(test_softmax)\n",
    "    test_pred = to_cpu_npy(test_pred)\n",
    "    test_AUROC = roc_auc_score(test_labels, test_softmax[:,1], average=\"micro\")\n",
    "    test_precision, test_recall, thresholds = precision_recall_curve(test_labels, test_softmax[:,1])\n",
    "    test_AUPR = auc(test_recall, test_precision)\n",
    "    # test_F1 = f1_score(test_labels, test_pred, average=\"micro\")\n",
    "    \n",
    "    train_scores = forward_scores[train_idx]\n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    train_softmax, train_pred = model.calc_softmax_pred(train_scores) \n",
    "    train_pred = to_cpu_npy(train_pred)\n",
    "    train_softmax = to_cpu_npy(train_softmax)\n",
    "    train_precision, train_recall, thresholds = precision_recall_curve(train_labels, train_softmax[:,1])\n",
    "    train_AUPR = auc(train_recall, train_precision)\n",
    "    # train_F1 = f1_score(train_labels, train_pred, average=\"micro\")\n",
    "\n",
    "    valid_scores = forward_scores[valid_idx]\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    valid_softmax, valid_pred = model.calc_softmax_pred(valid_scores) \n",
    "    valid_pred = to_cpu_npy(valid_pred)\n",
    "    valid_softmax = to_cpu_npy(valid_softmax)\n",
    "    valid_precision, valid_recall, thresholds = precision_recall_curve(valid_labels, valid_softmax[:,1])\n",
    "    valid_AUPR = auc(valid_recall, valid_precision)\n",
    "    # valid_F1 = f1_score(valid_labels, valid_pred, average=\"micro\")\n",
    "\n",
    "    return test_AUROC, test_AUPR, test_pred, test_labels, train_AUPR, train_pred, train_labels, \\\n",
    "        valid_AUPR, valid_pred, valid_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e0e7c11-ee20-4a04-a02a-1d2b76ec044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_classification_GAT(model, loss, graph, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer):\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "\n",
    "    optimizer = optimizer\n",
    "    \n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    \n",
    "    model.train()\n",
    "    train_status = True\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for e in list(range(max_epoch)):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        all_scores = model(graph)[targetNode_mask]\n",
    "        train_scores = all_scores[train_idx]\n",
    "        \n",
    "        train_loss = loss(train_scores, torch.LongTensor(train_labels).to(device))\n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        valid_scores = all_scores[valid_idx]\n",
    "        valid_loss = loss(valid_scores, torch.LongTensor(valid_labels).to(device))\n",
    "        valid_losses.append(valid_loss.item())\n",
    "\n",
    "        if e%100 == 0:\n",
    "            print(f'Epoch {e}: Train Loss = {train_loss}, Valid Loss = {valid_loss}')\n",
    "\n",
    "    return train_losses, valid_losses\n",
    "\n",
    "def eval_model_classification_GAT(model, graph, targetNode_mask, train_idx, valid_idx, test_idx):\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "    test_labels = to_cpu_npy(graph.y[targetNode_mask[test_idx]])\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    forward_scores = model(G)[targetNode_mask]\n",
    "\n",
    "    test_scores = forward_scores[test_idx]\n",
    "    test_softmax = F.softmax(test_scores, dim=1)\n",
    "    test_preds = torch.argmax(test_softmax, dim=1)\n",
    "    \n",
    "    test_softmax = to_cpu_npy(test_softmax)\n",
    "    test_preds = to_cpu_npy(test_preds)\n",
    "    test_AUROC = roc_auc_score(test_labels, test_softmax[:,1], average=\"micro\")\n",
    "    test_acc = np.mean(test_preds == test_labels)\n",
    "\n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    train_scores = forward_scores[train_idx]\n",
    "    train_softmax = F.softmax(train_scores, dim=1)\n",
    "    train_preds = torch.argmax(train_softmax, dim=1)\n",
    "    \n",
    "    train_softmax = to_cpu_npy(train_softmax)\n",
    "    train_preds = to_cpu_npy(train_preds)\n",
    "    train_AUROC = roc_auc_score(train_labels, train_softmax[:,1], average=\"micro\")\n",
    "    train_acc = np.mean(train_preds == train_labels)\n",
    "\n",
    "\n",
    "    return {'train_AUROC': train_AUROC, 'train_acc': train_acc, 'test_AUROC': test_AUROC, 'test_acc': test_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f78d9a9-56a0-46bd-bf90-00d9f8f7e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_classification_GAT_Neighbors(model, loss, train_loader, valid_loader, max_epoch, optimizer, train_idx, valid_idx):\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            train_batch_mask = torch.isin(batch.n_id.to(device), targetNode_mask.to(device))\n",
    "            train_batch_scores = model(batch)[train_batch_mask]\n",
    "            train_batch_labels = to_cpu_npy(batch.y[train_batch_mask])\n",
    "            train_batch_loss = loss(train_batch_scores, torch.LongTensor(train_batch_labels).to(device))\n",
    "            train_losses.append(train_batch_loss.item())\n",
    "            train_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                batch = batch.to(device)\n",
    "                valid_batch_mask = torch.isin(batch.n_id.to(device), targetNode_mask.to(device))\n",
    "                valid_batch_scores = model(batch)[valid_batch_mask]\n",
    "                valid_batch_labels = to_cpu_npy(batch.y[valid_batch_mask])\n",
    "                valid_batch_loss = loss(valid_batch_scores, torch.LongTensor(valid_batch_labels).to(device))\n",
    "                valid_losses.append(valid_batch_loss.item())\n",
    "                \n",
    "        print(f'Epoch {epoch}: Train Loss = {train_batch_loss}, Valid Loss = {valid_batch_loss}')\n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40861ba0-d302-49c3-841f-67c27ecbfa38",
   "metadata": {},
   "source": [
    "#### GC-Merge Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffcd6bfa-78a4-44f1-8d39-eb5dad45b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_line = 'E116'\n",
    "max_epoch = 1000\n",
    "learning_rate = 1e-4\n",
    "num_graph_conv_layers = 2\n",
    "graph_conv_embed_size = 256\n",
    "num_lin_layers = 3\n",
    "lin_hidden_size = 256\n",
    "regression_flag = 0\n",
    "random_seed = 0\n",
    "\n",
    "chip_res = 10000\n",
    "hic_res = 10000\n",
    "num_hm = 6\n",
    "num_feat = int((hic_res/chip_res)*num_hm)\n",
    "num_classes = 2 if regression_flag == 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fdb9bed-e012-4a72-8075-140e702766b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = os.getcwd()\n",
    "save_dir = os.path.join(src_dir, 'data', cell_line, 'saved_runs')\n",
    "hic_sparse_mat_file = os.path.join(src_dir, 'data', cell_line, 'hic_sparse.npz')\n",
    "np_nodes_lab_genes_file = os.path.join(src_dir, 'data',  cell_line, \\\n",
    "    'np_nodes_lab_genes_reg' + str(regression_flag) + '.npy')\n",
    "np_hmods_norm_all_file = os.path.join(src_dir, 'data', cell_line, \\\n",
    "    'np_hmods_norm_chip_' + str(chip_res) + 'bp.npy')\n",
    "df_genes_file = os.path.join(src_dir, 'data', cell_line, 'df_genes_reg' + str(regression_flag) + '.pkl')\n",
    "df_genes = pd.read_pickle(df_genes_file)\n",
    "\n",
    "mat = load_npz(hic_sparse_mat_file)\n",
    "allNodes_hms = np.load(np_hmods_norm_all_file)\n",
    "hms = allNodes_hms[:, 1:] #only includes features, not node ids\n",
    "X = torch.tensor(hms).float().reshape(-1, num_feat) \n",
    "allNodes = allNodes_hms[:, 0].astype(int)\n",
    "geneNodes_labs = np.load(np_nodes_lab_genes_file)\n",
    "\n",
    "geneNodes = geneNodes_labs[:, -2].astype(int)\n",
    "allLabs = -1*np.ones(np.shape(allNodes))\n",
    "\n",
    "targetNode_mask = torch.tensor(geneNodes).long()\n",
    "\n",
    "if regression_flag == 0:\n",
    "    geneLabs = geneNodes_labs[:, -1].astype(int)\n",
    "    allLabs[geneNodes] = geneLabs\n",
    "    Y = torch.tensor(allLabs).long()\n",
    "else:\n",
    "    geneLabs = geneNodes_labs[:, -1].astype(float)\n",
    "    allLabs[geneNodes] = geneLabs\n",
    "    Y = torch.tensor(allLabs).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6c91693-1e30-466b-bcdf-a3daee3ff9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChIP-seq resolution: 10000\n",
      "\n",
      "\n",
      "Training set: 70%\n",
      "Validation set: 15%\n",
      "Testing set: 15%\n",
      "\n",
      "\n",
      "Model hyperparameters: \n",
      "Number of epochs: 1000\n",
      "Learning rate: 0.0001\n",
      "Number of graph convolutional layers: 2\n",
      "Graph convolutional embedding size: 256\n",
      "Number of linear layers: 3\n",
      "Linear hidden layer size: 256\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter and model definitions\n",
    "max_epoch = 1000\n",
    "learning_rate = 1e-4\n",
    "num_graph_conv_layers = 2\n",
    "graph_conv_embed_size = 256\n",
    "num_lin_layers = 3\n",
    "lin_hidden_size = 256\n",
    "regression_flag = 0\n",
    "random_seed = 0\n",
    "\n",
    "chip_res = 10000\n",
    "hic_res = 10000\n",
    "num_hm = 6\n",
    "num_feat = int((hic_res/chip_res)*num_hm)\n",
    "num_classes = 2 if regression_flag == 0 else 1\n",
    "\n",
    "print('ChIP-seq resolution:', str(chip_res))\n",
    "print('\\n')\n",
    "print('Training set: 70%')\n",
    "print('Validation set: 15%')\n",
    "print('Testing set: 15%')\n",
    "print('\\n')\n",
    "print('Model hyperparameters: ')\n",
    "print('Number of epochs:', max_epoch)\n",
    "print('Learning rate:', learning_rate)\n",
    "print('Number of graph convolutional layers:', str(num_graph_conv_layers))\n",
    "print('Graph convolutional embedding size:', graph_conv_embed_size)\n",
    "print('Number of linear layers:', str(num_lin_layers))\n",
    "print('Linear hidden layer size:', lin_hidden_size)\n",
    "\n",
    "# random_seed = random.randint(0,10000)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "###Test for GPU availability\n",
    "cuda_flag = torch.cuda.is_available()\n",
    "if cuda_flag:  \n",
    "  dev = \"cuda\" \n",
    "else:\n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6179338e-4bde-468b-929e-f233255aae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Load input files\n",
    "def prepare_data(cell_line, regression_flag, base_path):\n",
    "    save_dir = os.path.join(base_path, 'data', cell_line, 'saved_runs')\n",
    "    hic_sparse_mat_file = os.path.join(base_path, 'data', cell_line, 'hic_sparse.npz')\n",
    "    np_nodes_lab_genes_file = os.path.join(base_path, 'data',  cell_line, \\\n",
    "        'np_nodes_lab_genes_reg' + str(regression_flag) + '.npy')\n",
    "    np_hmods_norm_all_file = os.path.join(base_path, 'data', cell_line, \\\n",
    "        'np_hmods_norm_chip_' + str(chip_res) + 'bp.npy')\n",
    "    df_genes_file = os.path.join(base_path, 'data', cell_line, 'df_genes_reg' + str(regression_flag) + '.pkl')\n",
    "    df_genes = pd.read_pickle(df_genes_file)\n",
    "    \n",
    "    mat = load_npz(hic_sparse_mat_file)\n",
    "    allNodes_hms = np.load(np_hmods_norm_all_file)\n",
    "    hms = allNodes_hms[:, 1:] #only includes features, not node ids\n",
    "    X = torch.tensor(hms).float().reshape(-1, num_feat) \n",
    "    allNodes = allNodes_hms[:, 0].astype(int)\n",
    "    geneNodes_labs = np.load(np_nodes_lab_genes_file)\n",
    "\n",
    "    geneNodes = geneNodes_labs[:, -2].astype(int)\n",
    "    allLabs = -1*np.ones(np.shape(allNodes))\n",
    "\n",
    "    targetNode_mask = torch.tensor(geneNodes).long()\n",
    "\n",
    "    if regression_flag == 0:\n",
    "        geneLabs = geneNodes_labs[:, -1].astype(int)\n",
    "        allLabs[geneNodes] = geneLabs\n",
    "        Y = torch.tensor(allLabs).long()\n",
    "    else:\n",
    "        geneLabs = geneNodes_labs[:, -1].astype(float)\n",
    "        allLabs[geneNodes] = geneLabs\n",
    "        Y = torch.tensor(allLabs).float()\n",
    "\n",
    "    extract = torch_geometric.utils.from_scipy_sparse_matrix(mat)\n",
    "    data = torch_geometric.data.Data(edge_index = extract[0], edge_attr = extract[1], x = X, y = Y)\n",
    "    G = data\n",
    "    \n",
    "    ###Randomize node order and split into 70%/15%/15% training/validation/test sets\n",
    "    torch.manual_seed(0)\n",
    "    pred_idx_shuff = torch.randperm(targetNode_mask.shape[0])\n",
    "    fin_train = np.floor(0.7*pred_idx_shuff.shape[0]).astype(int)\n",
    "    fin_valid = np.floor(0.85*pred_idx_shuff.shape[0]).astype(int)\n",
    "    \n",
    "    return G, targetNode_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ded11a3c-9319-42b7-8d5a-f7b8720709c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_conv_embed_size = 256\n",
    "num_graph_conv_layers = 2\n",
    "\n",
    "graph_conv_layer_sizes = [num_feat] + \\\n",
    "    [int(max(graph_conv_embed_size, lin_hidden_size)) \\\n",
    "          for i in np.arange(1, num_graph_conv_layers, 1)] + [lin_hidden_size]\n",
    "\n",
    "graph_lin_hidden_sizes = [graph_conv_layer_sizes[-1]] + \\\n",
    "    [int(max(lin_hidden_size, num_classes)) \\\n",
    "          for i in np.arange(1, num_lin_layers, 1)] + [num_classes]\n",
    "\n",
    "graph_lin_hidden_sizes_reg = [graph_conv_layer_sizes[-1]] + \\\n",
    "    [int(max(lin_hidden_size, num_classes)) \\\n",
    "          for i in np.arange(1, num_lin_layers, 1)] + [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5f667e1-cc81-4fbe-b767-46ec9e24169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 256, 256, 2]\n"
     ]
    }
   ],
   "source": [
    "num_classes_reg = 1\n",
    "lin_hidden_sizes = [num_feat] + [int(max(lin_hidden_size, num_classes)) for i in np.arange(1, num_lin_layers, 1)] + [num_classes]\n",
    "lin_hidden_sizes_reg = [num_feat] + [int(max(lin_hidden_size, num_classes_reg)) for i in np.arange(1, num_lin_layers, 1)] + [num_classes_reg]\n",
    "cell_lines = ['E116', 'E122', 'E123']\n",
    "\n",
    "print(lin_hidden_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c873dc-eee4-4174-8301-299987e721d3",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e2b62b8-c985-4b21-9f02-85ffa0195d35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Cell Line E116...\n",
      "Number of Parameters: 266754\n",
      "\n",
      "\n",
      "Epoch 0 out of 1000\n",
      "Epoch 100 out of 1000\n",
      "Epoch 200 out of 1000\n",
      "Epoch 300 out of 1000\n",
      "Epoch 400 out of 1000\n",
      "Epoch 500 out of 1000\n",
      "Epoch 600 out of 1000\n",
      "Epoch 700 out of 1000\n",
      "Epoch 800 out of 1000\n",
      "Epoch 900 out of 1000\n",
      "\n",
      "Training Cell Line E122...\n",
      "Number of Parameters: 266754\n",
      "\n",
      "\n",
      "Epoch 0 out of 1000\n",
      "Epoch 100 out of 1000\n",
      "Epoch 200 out of 1000\n",
      "Epoch 300 out of 1000\n",
      "Epoch 400 out of 1000\n",
      "Epoch 500 out of 1000\n",
      "Epoch 600 out of 1000\n",
      "Epoch 700 out of 1000\n",
      "Epoch 800 out of 1000\n",
      "Epoch 900 out of 1000\n",
      "\n",
      "Training Cell Line E123...\n",
      "Number of Parameters: 266754\n",
      "\n",
      "\n",
      "Epoch 0 out of 1000\n",
      "Epoch 100 out of 1000\n",
      "Epoch 200 out of 1000\n",
      "Epoch 300 out of 1000\n",
      "Epoch 400 out of 1000\n",
      "Epoch 500 out of 1000\n",
      "Epoch 600 out of 1000\n",
      "Epoch 700 out of 1000\n",
      "Epoch 800 out of 1000\n",
      "Epoch 900 out of 1000\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 1000\n",
    "gcn_auroc = []\n",
    "\n",
    "for cell_line in cell_lines:\n",
    "    print(f'\\nTraining Cell Line {cell_line}...')\n",
    "    \n",
    "    train_idx = torch.load(f'./train-test-split/{cell_line}/train_idx.pt')\n",
    "    valid_idx = torch.load(f'./train-test-split/{cell_line}/valid_idx.pt')\n",
    "    test_idx = torch.load(f'./train-test-split/{cell_line}/test_idx.pt')\n",
    "    \n",
    "    G, targetNode_mask = prepare_data(cell_line=cell_line, regression_flag = regression_flag, base_path = src_dir)\n",
    "    \n",
    "    model = GCN_classification(num_feat, num_graph_conv_layers, graph_conv_layer_sizes, num_lin_layers, graph_lin_hidden_sizes, num_classes)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), lr = learning_rate)\n",
    "\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    print(f'Number of Parameters: {sum([np.prod(p.size()) for p in model_parameters])}')\n",
    "\n",
    "    train_loss_vec, train_AUROC_vec, valid_loss_vec, valid_AUROC_vec = \\\n",
    "    train_model_classification(model, G, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer)\n",
    "\n",
    "    test_AUROC, test_AUPR, test_pred, test_labels, train_AUPR, train_pred, train_labels, \\\n",
    "        valid_AUPR, valid_pred, valid_labels = \\\n",
    "            eval_model_classification(model, G, targetNode_mask, train_idx, valid_idx, test_idx)\n",
    "    \n",
    "    gcn_auroc.append(test_AUROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69d6d7c7-153d-4df8-8ea4-2c3575461948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E116</th>\n",
       "      <th>E122</th>\n",
       "      <th>E123</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GCN</th>\n",
       "      <td>0.912025</td>\n",
       "      <td>0.906868</td>\n",
       "      <td>0.92633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         E116      E122     E123\n",
       "GCN  0.912025  0.906868  0.92633"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_res = pd.DataFrame(columns = cell_lines)\n",
    "classification_res.loc['GCN'] = gcn_auroc\n",
    "classification_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1660da5-1481-4473-be27-1c0d50858d5d",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8ac6c94-e7cd-4033-be0f-c9fa2cfa1ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Cell Line E116...\n",
      "\n",
      "\n",
      "Epoch 0 out of 1000\n",
      "Epoch 100 out of 1000\n",
      "Epoch 200 out of 1000\n",
      "Epoch 300 out of 1000\n",
      "Epoch 400 out of 1000\n",
      "Epoch 500 out of 1000\n",
      "Epoch 600 out of 1000\n",
      "Epoch 700 out of 1000\n",
      "Epoch 800 out of 1000\n",
      "Epoch 900 out of 1000\n",
      "\n",
      "Training Cell Line E122...\n",
      "\n",
      "\n",
      "Epoch 0 out of 1000\n",
      "Epoch 100 out of 1000\n",
      "Epoch 200 out of 1000\n",
      "Epoch 300 out of 1000\n",
      "Epoch 400 out of 1000\n",
      "Epoch 500 out of 1000\n",
      "Epoch 600 out of 1000\n",
      "Epoch 700 out of 1000\n",
      "Epoch 800 out of 1000\n",
      "Epoch 900 out of 1000\n",
      "\n",
      "Training Cell Line E123...\n",
      "\n",
      "\n",
      "Epoch 0 out of 1000\n",
      "Epoch 100 out of 1000\n",
      "Epoch 200 out of 1000\n",
      "Epoch 300 out of 1000\n",
      "Epoch 400 out of 1000\n",
      "Epoch 500 out of 1000\n",
      "Epoch 600 out of 1000\n",
      "Epoch 700 out of 1000\n",
      "Epoch 800 out of 1000\n",
      "Epoch 900 out of 1000\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 1000\n",
    "mlp_auroc = []\n",
    "\n",
    "for cell_line in cell_lines:\n",
    "\n",
    "    train_idx = torch.load(f'./train-test-split/{cell_line}/train_idx.pt')\n",
    "    valid_idx = torch.load(f'./train-test-split/{cell_line}/valid_idx.pt')\n",
    "    test_idx = torch.load(f'./train-test-split/{cell_line}/test_idx.pt')\n",
    "    \n",
    "    print(f'\\nTraining Cell Line {cell_line}...')\n",
    "    G, targetNode_mask = prepare_data(cell_line=cell_line, regression_flag = regression_flag, base_path = src_dir)\n",
    "    \n",
    "    model = MLP_classification(num_feat, num_lin_layers, lin_hidden_sizes, num_classes)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), lr = learning_rate)\n",
    "    \n",
    "    train_loss_vec, train_AUROC_vec, valid_loss_vec, valid_AUROC_vec = \\\n",
    "    train_model_classification(model, G, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer)\n",
    "\n",
    "    test_AUROC, test_AUPR, test_pred, test_labels, train_AUPR, train_pred, train_labels, \\\n",
    "        valid_AUPR, valid_pred, valid_labels = \\\n",
    "            eval_model_classification(model, G, targetNode_mask, train_idx, valid_idx, test_idx)\n",
    "    \n",
    "    mlp_auroc.append(test_AUROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "691831de-d28b-4684-956e-7b1cce4a331d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E116</th>\n",
       "      <th>E122</th>\n",
       "      <th>E123</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GCN</th>\n",
       "      <td>0.912025</td>\n",
       "      <td>0.906868</td>\n",
       "      <td>0.926330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>0.908909</td>\n",
       "      <td>0.896096</td>\n",
       "      <td>0.918319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         E116      E122      E123\n",
       "GCN  0.912025  0.906868  0.926330\n",
       "MLP  0.908909  0.896096  0.918319"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_res.loc['MLP'] = mlp_auroc\n",
    "classification_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80411972-de16-4bd2-87b9-f5f9552fb043",
   "metadata": {},
   "source": [
    "### Weighted Aggregation GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15ae3847-fe9b-4471-9dc2-8b45e280c8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Cell Line E116...\n",
      "Number of Parameters: 8080582\n",
      "\n",
      "\n",
      "Epoch 0 out of 1000\n",
      "Epoch 100 out of 1000\n",
      "Epoch 200 out of 1000\n",
      "Epoch 300 out of 1000\n",
      "Epoch 400 out of 1000\n",
      "Epoch 500 out of 1000\n",
      "Epoch 600 out of 1000\n",
      "Epoch 700 out of 1000\n",
      "Epoch 800 out of 1000\n",
      "Epoch 900 out of 1000\n",
      "\n",
      "Training Cell Line E122...\n",
      "Number of Parameters: 7091466\n",
      "\n",
      "\n",
      "Epoch 0 out of 1000\n",
      "Epoch 100 out of 1000\n",
      "Epoch 200 out of 1000\n",
      "Epoch 300 out of 1000\n",
      "Epoch 400 out of 1000\n",
      "Epoch 500 out of 1000\n",
      "Epoch 600 out of 1000\n",
      "Epoch 700 out of 1000\n",
      "Epoch 800 out of 1000\n",
      "Epoch 900 out of 1000\n",
      "\n",
      "Training Cell Line E123...\n",
      "Number of Parameters: 7284882\n",
      "\n",
      "\n",
      "Epoch 0 out of 1000\n",
      "Epoch 100 out of 1000\n",
      "Epoch 200 out of 1000\n",
      "Epoch 300 out of 1000\n",
      "Epoch 400 out of 1000\n",
      "Epoch 500 out of 1000\n",
      "Epoch 600 out of 1000\n",
      "Epoch 700 out of 1000\n",
      "Epoch 800 out of 1000\n",
      "Epoch 900 out of 1000\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 1000\n",
    "w_gcn_auroc = []\n",
    "\n",
    "for cell_line in cell_lines:\n",
    "    print(f'\\nTraining Cell Line {cell_line}...')\n",
    "\n",
    "    train_idx = torch.load(f'train-test-split/{cell_line}/train_idx.pt')\n",
    "    valid_idx = torch.load(f'train-test-split/{cell_line}/valid_idx.pt')\n",
    "    test_idx = torch.load(f'train-test-split/{cell_line}/test_idx.pt')\n",
    "    \n",
    "    G, targetNode_mask = prepare_data(cell_line = cell_line, regression_flag = regression_flag, base_path = src_dir)\n",
    "    \n",
    "    model = GCN_classification_weighted(num_feat, num_graph_conv_layers, graph_conv_layer_sizes, num_lin_layers, graph_lin_hidden_sizes, num_classes, num_nodes=G.x.shape[0], edge_attr=G.edge_attr)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), lr = learning_rate)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    print(f'Number of Parameters: {sum([np.prod(p.size()) for p in model_parameters])}')\n",
    "\n",
    "    train_out = train_model_classification(model, G, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer)\n",
    "    test_AUROC, test_AUPR, test_pred, test_labels, train_AUPR, train_pred, train_labels, \\\n",
    "        valid_AUPR, valid_pred, valid_labels = eval_model_classification(model, G, targetNode_mask, train_idx, valid_idx, test_idx)\n",
    "\n",
    "    w_gcn_auroc.append(test_AUROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e8e0483-d56f-4603-9441-d9494193aa87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E116</th>\n",
       "      <th>E122</th>\n",
       "      <th>E123</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GCN</th>\n",
       "      <td>0.912025</td>\n",
       "      <td>0.906868</td>\n",
       "      <td>0.926330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>0.908909</td>\n",
       "      <td>0.896096</td>\n",
       "      <td>0.918319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted Agg GCN</th>\n",
       "      <td>0.911979</td>\n",
       "      <td>0.907122</td>\n",
       "      <td>0.926355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted_Agg_GCN</th>\n",
       "      <td>0.911979</td>\n",
       "      <td>0.907122</td>\n",
       "      <td>0.926355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      E116      E122      E123\n",
       "GCN               0.912025  0.906868  0.926330\n",
       "MLP               0.908909  0.896096  0.918319\n",
       "Weighted Agg GCN  0.911979  0.907122  0.926355\n",
       "Weighted_Agg_GCN  0.911979  0.907122  0.926355"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_res.loc['Weighted_Agg_GCN'] = w_gcn_auroc\n",
    "classification_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d82f131-b42f-4d41-85b5-d0c53f4045b1",
   "metadata": {},
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5da30a6-1f9c-4177-9cf5-5be30ee37a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChIP-seq resolution: 10000\n",
      "\n",
      "\n",
      "Training set: 70%\n",
      "Validation set: 15%\n",
      "Testing set: 15%\n",
      "\n",
      "\n",
      "Model hyperparameters: \n",
      "Number of epochs: 1000\n",
      "Learning rate: 0.0001\n",
      "Number of graph convolutional layers: 2\n",
      "Graph convolutional embedding size: 256\n",
      "Number of linear layers: 3\n",
      "Linear hidden layer size: 256\n",
      "[6, 256, 256, 2]\n"
     ]
    }
   ],
   "source": [
    "# resetting states\n",
    "\n",
    "cell_line = 'E116'\n",
    "max_epoch = 1000\n",
    "learning_rate = 1e-4\n",
    "num_graph_conv_layers = 2\n",
    "graph_conv_embed_size = 256\n",
    "num_lin_layers = 3\n",
    "lin_hidden_size = 256\n",
    "regression_flag = 0\n",
    "random_seed = 0\n",
    "\n",
    "chip_res = 10000\n",
    "hic_res = 10000\n",
    "num_hm = 6\n",
    "num_feat = int((hic_res/chip_res)*num_hm)\n",
    "num_classes = 2 if regression_flag == 0 else 1\n",
    "\n",
    "src_dir = os.getcwd()\n",
    "save_dir = os.path.join(src_dir, 'data', cell_line, 'saved_runs')\n",
    "hic_sparse_mat_file = os.path.join(src_dir, 'data', cell_line, 'hic_sparse.npz')\n",
    "np_nodes_lab_genes_file = os.path.join(src_dir, 'data',  cell_line, \\\n",
    "    'np_nodes_lab_genes_reg' + str(regression_flag) + '.npy')\n",
    "np_hmods_norm_all_file = os.path.join(src_dir, 'data', cell_line, \\\n",
    "    'np_hmods_norm_chip_' + str(chip_res) + 'bp.npy')\n",
    "df_genes_file = os.path.join(src_dir, 'data', cell_line, 'df_genes_reg' + str(regression_flag) + '.pkl')\n",
    "df_genes = pd.read_pickle(df_genes_file)\n",
    "\n",
    "mat = load_npz(hic_sparse_mat_file)\n",
    "allNodes_hms = np.load(np_hmods_norm_all_file)\n",
    "hms = allNodes_hms[:, 1:] #only includes features, not node ids\n",
    "X = torch.tensor(hms).float().reshape(-1, num_feat) \n",
    "allNodes = allNodes_hms[:, 0].astype(int)\n",
    "geneNodes_labs = np.load(np_nodes_lab_genes_file)\n",
    "\n",
    "geneNodes = geneNodes_labs[:, -2].astype(int)\n",
    "allLabs = -1*np.ones(np.shape(allNodes))\n",
    "\n",
    "targetNode_mask = torch.tensor(geneNodes).long()\n",
    "\n",
    "if regression_flag == 0:\n",
    "    geneLabs = geneNodes_labs[:, -1].astype(int)\n",
    "    allLabs[geneNodes] = geneLabs\n",
    "    Y = torch.tensor(allLabs).long()\n",
    "else:\n",
    "    geneLabs = geneNodes_labs[:, -1].astype(float)\n",
    "    allLabs[geneNodes] = geneLabs\n",
    "    Y = torch.tensor(allLabs).float()\n",
    "\n",
    "#hyperparameter and model definitions\n",
    "max_epoch = 1000\n",
    "learning_rate = 1e-4\n",
    "num_graph_conv_layers = 2\n",
    "graph_conv_embed_size = 256\n",
    "num_lin_layers = 3\n",
    "lin_hidden_size = 256\n",
    "regression_flag = 0\n",
    "random_seed = 0\n",
    "\n",
    "chip_res = 10000\n",
    "hic_res = 10000\n",
    "num_hm = 6\n",
    "num_feat = int((hic_res/chip_res)*num_hm)\n",
    "num_classes = 2 if regression_flag == 0 else 1\n",
    "\n",
    "print('ChIP-seq resolution:', str(chip_res))\n",
    "print('\\n')\n",
    "print('Training set: 70%')\n",
    "print('Validation set: 15%')\n",
    "print('Testing set: 15%')\n",
    "print('\\n')\n",
    "print('Model hyperparameters: ')\n",
    "print('Number of epochs:', max_epoch)\n",
    "print('Learning rate:', learning_rate)\n",
    "print('Number of graph convolutional layers:', str(num_graph_conv_layers))\n",
    "print('Graph convolutional embedding size:', graph_conv_embed_size)\n",
    "print('Number of linear layers:', str(num_lin_layers))\n",
    "print('Linear hidden layer size:', lin_hidden_size)\n",
    "\n",
    "# random_seed = random.randint(0,10000)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "###Test for GPU availability\n",
    "cuda_flag = torch.cuda.is_available()\n",
    "if cuda_flag:  \n",
    "  dev = \"cuda\" \n",
    "else:\n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev)\n",
    "\n",
    "graph_conv_embed_size = 256\n",
    "num_graph_conv_layers = 2\n",
    "\n",
    "graph_conv_layer_sizes = [num_feat] + \\\n",
    "    [int(max(graph_conv_embed_size, lin_hidden_size)) \\\n",
    "          for i in np.arange(1, num_graph_conv_layers, 1)] + [lin_hidden_size]\n",
    "\n",
    "graph_lin_hidden_sizes = [graph_conv_layer_sizes[-1]] + \\\n",
    "    [int(max(lin_hidden_size, num_classes)) \\\n",
    "          for i in np.arange(1, num_lin_layers, 1)] + [num_classes]\n",
    "\n",
    "graph_lin_hidden_sizes_reg = [graph_conv_layer_sizes[-1]] + \\\n",
    "    [int(max(lin_hidden_size, num_classes)) \\\n",
    "          for i in np.arange(1, num_lin_layers, 1)] + [1]\n",
    "\n",
    "num_classes_reg = 1\n",
    "lin_hidden_sizes = [num_feat] + [int(max(lin_hidden_size, num_classes)) for i in np.arange(1, num_lin_layers, 1)] + [num_classes]\n",
    "lin_hidden_sizes_reg = [num_feat] + [int(max(lin_hidden_size, num_classes_reg)) for i in np.arange(1, num_lin_layers, 1)] + [num_classes_reg]\n",
    "cell_lines = ['E116', 'E122', 'E123']\n",
    "\n",
    "print(lin_hidden_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b25fb10-b477-485c-a0fe-52ef3d0deb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Cell Line E116...\n",
      "\n",
      "\n",
      "Epoch 0: Train Loss = 0.6946897506713867, Valid Loss = 0.6951975226402283\n",
      "Epoch 100: Train Loss = 0.5264937877655029, Valid Loss = 0.5406813025474548\n",
      "Epoch 200: Train Loss = 0.5013999938964844, Valid Loss = 0.523849368095398\n",
      "Epoch 300: Train Loss = 0.47166815400123596, Valid Loss = 0.49847838282585144\n",
      "Epoch 400: Train Loss = 0.4441436231136322, Valid Loss = 0.47495073080062866\n",
      "Epoch 500: Train Loss = 0.43035799264907837, Valid Loss = 0.4615248143672943\n",
      "Epoch 600: Train Loss = 0.4219149649143219, Valid Loss = 0.4527394473552704\n",
      "Epoch 700: Train Loss = 0.41287437081336975, Valid Loss = 0.4497264623641968\n",
      "Epoch 800: Train Loss = 0.40680357813835144, Valid Loss = 0.44435736536979675\n",
      "Epoch 900: Train Loss = 0.4009764790534973, Valid Loss = 0.44255053997039795\n",
      "\n",
      "Training Cell Line E122...\n",
      "\n",
      "\n",
      "Epoch 0: Train Loss = 0.7350696921348572, Valid Loss = 0.735917866230011\n",
      "Epoch 100: Train Loss = 0.5280822515487671, Valid Loss = 0.5432596802711487\n",
      "Epoch 200: Train Loss = 0.5067785978317261, Valid Loss = 0.5164926648139954\n",
      "Epoch 300: Train Loss = 0.47805699706077576, Valid Loss = 0.47990724444389343\n",
      "Epoch 400: Train Loss = 0.4565383791923523, Valid Loss = 0.45044150948524475\n",
      "Epoch 500: Train Loss = 0.4441533088684082, Valid Loss = 0.4364353120326996\n",
      "Epoch 600: Train Loss = 0.4348130226135254, Valid Loss = 0.42810726165771484\n",
      "Epoch 700: Train Loss = 0.4294110834598541, Valid Loss = 0.4267115592956543\n",
      "Epoch 800: Train Loss = 0.4243534207344055, Valid Loss = 0.42611053586006165\n",
      "Epoch 900: Train Loss = 0.4185042381286621, Valid Loss = 0.4211235046386719\n",
      "\n",
      "Training Cell Line E123...\n",
      "\n",
      "\n",
      "Epoch 0: Train Loss = 0.7140535712242126, Valid Loss = 0.7017903923988342\n",
      "Epoch 100: Train Loss = 0.5340352058410645, Valid Loss = 0.5407454967498779\n",
      "Epoch 200: Train Loss = 0.5080872178077698, Valid Loss = 0.5109408497810364\n",
      "Epoch 300: Train Loss = 0.4754565954208374, Valid Loss = 0.48286956548690796\n",
      "Epoch 400: Train Loss = 0.44513440132141113, Valid Loss = 0.4597799479961395\n",
      "Epoch 500: Train Loss = 0.43059054017066956, Valid Loss = 0.4479289948940277\n",
      "Epoch 600: Train Loss = 0.42317813634872437, Valid Loss = 0.44388335943222046\n",
      "Epoch 700: Train Loss = 0.41477200388908386, Valid Loss = 0.44280219078063965\n",
      "Epoch 800: Train Loss = 0.4113948941230774, Valid Loss = 0.44115859270095825\n",
      "Epoch 900: Train Loss = 0.40693405270576477, Valid Loss = 0.44068464636802673\n"
     ]
    }
   ],
   "source": [
    "extract = torch_geometric.utils.from_scipy_sparse_matrix(mat)\n",
    "\n",
    "data = torch_geometric.data.Data(edge_index = extract[0], edge_attr = extract[1], x = X, y = Y)\n",
    "G = data\n",
    "\n",
    "num_heads = 4\n",
    "learning_rate = 0.002\n",
    "max_epoch = 1000\n",
    "dropout = 0.1\n",
    "loss = nn.CrossEntropyLoss()\n",
    "hidden_channels=[6, 30]\n",
    "wd = 1e-05\n",
    "\n",
    "gat_auroc = []\n",
    "for cell_line in cell_lines:\n",
    "    print(f'\\nTraining Cell Line {cell_line}...')\n",
    "\n",
    "    train_idx = torch.load(f'train-test-split/{cell_line}/train_idx.pt')\n",
    "    valid_idx = torch.load(f'train-test-split/{cell_line}/valid_idx.pt')\n",
    "    test_idx = torch.load(f'train-test-split/{cell_line}/test_idx.pt')\n",
    "\n",
    "    gat = GAT(in_channels = 6, hidden_channels = hidden_channels, num_heads = num_heads, dropout = dropout)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, gat.parameters()), lr = learning_rate, weight_decay = wd)\n",
    "    \n",
    "    train_losses, valid_losses = train_model_classification_GAT(gat, loss, G, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer)\n",
    "    \n",
    "    out = eval_model_classification_GAT(gat, G, targetNode_mask, train_idx, valid_idx, test_idx)\n",
    "    \n",
    "    gat_auroc.append(out['test_AUROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "330e4045-20e1-4c0c-9340-b6f96cb5d867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E116</th>\n",
       "      <th>E122</th>\n",
       "      <th>E123</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GCN</th>\n",
       "      <td>0.912025</td>\n",
       "      <td>0.906868</td>\n",
       "      <td>0.926330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLP</th>\n",
       "      <td>0.908909</td>\n",
       "      <td>0.896096</td>\n",
       "      <td>0.918319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weighted_Agg_GCN</th>\n",
       "      <td>0.911979</td>\n",
       "      <td>0.907122</td>\n",
       "      <td>0.926355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GAT</th>\n",
       "      <td>0.883976</td>\n",
       "      <td>0.885955</td>\n",
       "      <td>0.879030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      E116      E122      E123\n",
       "GCN               0.912025  0.906868  0.926330\n",
       "MLP               0.908909  0.896096  0.918319\n",
       "Weighted_Agg_GCN  0.911979  0.907122  0.926355\n",
       "GAT               0.883976  0.885955  0.879030"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_res.loc['GAT'] = gat_auroc\n",
    "classification_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c368b4bb-9acc-41f8-a0c8-5d9368f2aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_res.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c43dbf-f5b0-41ec-afe3-2884336c780b",
   "metadata": {},
   "source": [
    "### GAT w/ Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e4bee31-4ffc-4038-ba7d-f49dbf5de76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChIP-seq resolution: 10000\n",
      "\n",
      "\n",
      "Training set: 70%\n",
      "Validation set: 15%\n",
      "Testing set: 15%\n",
      "\n",
      "\n",
      "Model hyperparameters: \n",
      "Number of epochs: 1000\n",
      "Learning rate: 0.0001\n",
      "Number of graph convolutional layers: 2\n",
      "Graph convolutional embedding size: 256\n",
      "Number of linear layers: 3\n",
      "Linear hidden layer size: 256\n",
      "[6, 256, 256, 2]\n"
     ]
    }
   ],
   "source": [
    "# resetting states\n",
    "\n",
    "cell_line = 'E116'\n",
    "max_epoch = 1000\n",
    "learning_rate = 1e-4\n",
    "num_graph_conv_layers = 2\n",
    "graph_conv_embed_size = 256\n",
    "num_lin_layers = 3\n",
    "lin_hidden_size = 256\n",
    "regression_flag = 0\n",
    "random_seed = 0\n",
    "\n",
    "chip_res = 10000\n",
    "hic_res = 10000\n",
    "num_hm = 6\n",
    "num_feat = int((hic_res/chip_res)*num_hm)\n",
    "num_classes = 2 if regression_flag == 0 else 1\n",
    "\n",
    "src_dir = os.getcwd()\n",
    "save_dir = os.path.join(src_dir, 'data', cell_line, 'saved_runs')\n",
    "hic_sparse_mat_file = os.path.join(src_dir, 'data', cell_line, 'hic_sparse.npz')\n",
    "np_nodes_lab_genes_file = os.path.join(src_dir, 'data',  cell_line, \\\n",
    "    'np_nodes_lab_genes_reg' + str(regression_flag) + '.npy')\n",
    "np_hmods_norm_all_file = os.path.join(src_dir, 'data', cell_line, \\\n",
    "    'np_hmods_norm_chip_' + str(chip_res) + 'bp.npy')\n",
    "df_genes_file = os.path.join(src_dir, 'data', cell_line, 'df_genes_reg' + str(regression_flag) + '.pkl')\n",
    "df_genes = pd.read_pickle(df_genes_file)\n",
    "\n",
    "mat = load_npz(hic_sparse_mat_file)\n",
    "allNodes_hms = np.load(np_hmods_norm_all_file)\n",
    "hms = allNodes_hms[:, 1:] #only includes features, not node ids\n",
    "X = torch.tensor(hms).float().reshape(-1, num_feat) \n",
    "allNodes = allNodes_hms[:, 0].astype(int)\n",
    "geneNodes_labs = np.load(np_nodes_lab_genes_file)\n",
    "\n",
    "geneNodes = geneNodes_labs[:, -2].astype(int)\n",
    "allLabs = -1*np.ones(np.shape(allNodes))\n",
    "\n",
    "targetNode_mask = torch.tensor(geneNodes).long()\n",
    "\n",
    "if regression_flag == 0:\n",
    "    geneLabs = geneNodes_labs[:, -1].astype(int)\n",
    "    allLabs[geneNodes] = geneLabs\n",
    "    Y = torch.tensor(allLabs).long()\n",
    "else:\n",
    "    geneLabs = geneNodes_labs[:, -1].astype(float)\n",
    "    allLabs[geneNodes] = geneLabs\n",
    "    Y = torch.tensor(allLabs).float()\n",
    "\n",
    "#hyperparameter and model definitions\n",
    "max_epoch = 1000\n",
    "learning_rate = 1e-4\n",
    "num_graph_conv_layers = 2\n",
    "graph_conv_embed_size = 256\n",
    "num_lin_layers = 3\n",
    "lin_hidden_size = 256\n",
    "regression_flag = 0\n",
    "random_seed = 0\n",
    "\n",
    "chip_res = 10000\n",
    "hic_res = 10000\n",
    "num_hm = 6\n",
    "num_feat = int((hic_res/chip_res)*num_hm)\n",
    "num_classes = 2 if regression_flag == 0 else 1\n",
    "\n",
    "print('ChIP-seq resolution:', str(chip_res))\n",
    "print('\\n')\n",
    "print('Training set: 70%')\n",
    "print('Validation set: 15%')\n",
    "print('Testing set: 15%')\n",
    "print('\\n')\n",
    "print('Model hyperparameters: ')\n",
    "print('Number of epochs:', max_epoch)\n",
    "print('Learning rate:', learning_rate)\n",
    "print('Number of graph convolutional layers:', str(num_graph_conv_layers))\n",
    "print('Graph convolutional embedding size:', graph_conv_embed_size)\n",
    "print('Number of linear layers:', str(num_lin_layers))\n",
    "print('Linear hidden layer size:', lin_hidden_size)\n",
    "\n",
    "# random_seed = random.randint(0,10000)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "###Test for GPU availability\n",
    "cuda_flag = torch.cuda.is_available()\n",
    "if cuda_flag:  \n",
    "  dev = \"cuda\" \n",
    "else:\n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev)\n",
    "\n",
    "graph_conv_embed_size = 256\n",
    "num_graph_conv_layers = 2\n",
    "\n",
    "graph_conv_layer_sizes = [num_feat] + \\\n",
    "    [int(max(graph_conv_embed_size, lin_hidden_size)) \\\n",
    "          for i in np.arange(1, num_graph_conv_layers, 1)] + [lin_hidden_size]\n",
    "\n",
    "graph_lin_hidden_sizes = [graph_conv_layer_sizes[-1]] + \\\n",
    "    [int(max(lin_hidden_size, num_classes)) \\\n",
    "          for i in np.arange(1, num_lin_layers, 1)] + [num_classes]\n",
    "\n",
    "graph_lin_hidden_sizes_reg = [graph_conv_layer_sizes[-1]] + \\\n",
    "    [int(max(lin_hidden_size, num_classes)) \\\n",
    "          for i in np.arange(1, num_lin_layers, 1)] + [1]\n",
    "\n",
    "num_classes_reg = 1\n",
    "lin_hidden_sizes = [num_feat] + [int(max(lin_hidden_size, num_classes)) for i in np.arange(1, num_lin_layers, 1)] + [num_classes]\n",
    "lin_hidden_sizes_reg = [num_feat] + [int(max(lin_hidden_size, num_classes_reg)) for i in np.arange(1, num_lin_layers, 1)] + [num_classes_reg]\n",
    "cell_lines = ['E116', 'E122', 'E123']\n",
    "\n",
    "print(lin_hidden_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6cd85b04-78eb-4f1c-9967-f5c4ca8da7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Cell Line E116...\n",
      "Epoch 0: Train Loss = 0.49647271633148193, Valid Loss = 0.5472369194030762\n",
      "Epoch 1: Train Loss = 0.4698091447353363, Valid Loss = 0.5677899718284607\n",
      "Epoch 2: Train Loss = 0.44664162397384644, Valid Loss = 0.637824535369873\n",
      "Epoch 3: Train Loss = 0.41795074939727783, Valid Loss = 0.6678191423416138\n",
      "Epoch 4: Train Loss = 0.5045174956321716, Valid Loss = 0.5813190937042236\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m gat \u001b[38;5;241m=\u001b[39m GAT(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, hidden_channels\u001b[38;5;241m=\u001b[39mhidden_channels, num_heads \u001b[38;5;241m=\u001b[39m num_heads, dropout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p : p\u001b[38;5;241m.\u001b[39mrequires_grad, gat\u001b[38;5;241m.\u001b[39mparameters()), lr \u001b[38;5;241m=\u001b[39m learning_rate, weight_decay \u001b[38;5;241m=\u001b[39m wd)\n\u001b[0;32m---> 23\u001b[0m train_losses, valid_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_classification_GAT_Neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_n_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_n_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m out \u001b[38;5;241m=\u001b[39m eval_model_classification_GAT(gat, G, targetNode_mask, train_idx, valid_idx, test_idx)\n\u001b[1;32m     27\u001b[0m gat_aurocs_neighbors\u001b[38;5;241m.\u001b[39mappend(out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_AUROC\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[32], line 11\u001b[0m, in \u001b[0;36mtrain_model_classification_GAT_Neighbors\u001b[0;34m(model, loss, train_loader, valid_loader, max_epoch, optimizer, train_idx, valid_idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m train_batch_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetNode_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m train_batch_scores \u001b[38;5;241m=\u001b[39m model(batch)[train_batch_mask]\n\u001b[1;32m     13\u001b[0m train_batch_labels \u001b[38;5;241m=\u001b[39m to_cpu_npy(batch\u001b[38;5;241m.\u001b[39my[train_batch_mask])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_heads = 4\n",
    "learning_rate = 0.001\n",
    "max_epoch = 20\n",
    "loss = nn.CrossEntropyLoss()\n",
    "hidden_channels=[6, 30]\n",
    "wd = 1e-05\n",
    "\n",
    "gat_auroc_neighbors = []\n",
    "for cell_line in cell_lines:\n",
    "    print(f'\\nTraining Cell Line {cell_line}...')\n",
    "\n",
    "    train_idx = torch.load(f'train-test-split/{cell_line}/train_idx.pt')\n",
    "    valid_idx = torch.load(f'train-test-split/{cell_line}/valid_idx.pt')\n",
    "    test_idx = torch.load(f'train-test-split/{cell_line}/test_idx.pt')\n",
    "\n",
    "    train_n_loader = NeighborLoader(G, num_neighbors = [10, 10], batch_size = 64, input_nodes = targetNode_mask[train_idx], shuffle = True)\n",
    "    valid_n_loader = NeighborLoader(G, num_neighbors = [10, 10], batch_size = 64, input_nodes = targetNode_mask[valid_idx], shuffle = False)\n",
    "\n",
    "    gat = GAT(in_channels=6, hidden_channels=hidden_channels, num_heads = num_heads, dropout = 0.5)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, gat.parameters()), lr = learning_rate, weight_decay = wd)\n",
    "    \n",
    "    train_losses, valid_losses = train_model_classification_GAT_Neighbors(gat, loss, train_n_loader, valid_n_loader, max_epoch, optimizer, train_idx, valid_idx)\n",
    "    \n",
    "    out = eval_model_classification_GAT(gat, G, targetNode_mask, train_idx, valid_idx, test_idx)\n",
    "    \n",
    "    gat_aurocs_neighbors.append(out['test_AUROC'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
