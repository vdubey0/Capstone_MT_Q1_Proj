{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3ac09cc-e4fa-46b4-bcab-f36dd6bb4a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime, date\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "385b5321-303c-4249-a556-8b303ef2c7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_classification(model, graph, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer):\n",
    "    '''\n",
    "    Trains model for classification task\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model [GCN_classification]: Instantiation of model class\n",
    "    graph [PyG Data class]: PyTorch Geometric Data object representing the graph\n",
    "    max_epoch [int]: Maximum number of training epochs\n",
    "    learning_rate [float]: Learning rate\n",
    "    targetNode_mask [tensor]: Subgraph mask for training nodes\n",
    "    train_idx [array]: Node IDs corresponding to training set\n",
    "    valid_idx [array]: Node IDs corresponding to validation set\n",
    "    optimizer [PyTorch optimizer class]: PyTorch optimization algorithm\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_loss_vec [array]: Training loss for each epoch\n",
    "    train_AUROC_vec [array]: Training AUROC score for each epoch\n",
    "    valid_loss_vec [array]: Validation loss for each epoch\n",
    "    valid_AUROC_vec [array]: Validation AUROC score for each epoch\n",
    "\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "\n",
    "    optimizer = optimizer\n",
    "    \n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_AUROC_vec = np.zeros(np.shape(np.arange(max_epoch)))\n",
    "    valid_loss_list = []\n",
    "    valid_AUROC_vec = np.zeros(np.shape(np.arange(max_epoch)))\n",
    "\n",
    "    model.train()\n",
    "    train_status = True\n",
    "    \n",
    "    print('\\n')\n",
    "    for e in list(range(max_epoch)):\n",
    "        \n",
    "        if e%100 == 0:\n",
    "            print(\"Epoch\", str(e), 'out of', str(max_epoch))\n",
    "        \n",
    "        model.train()\n",
    "        train_status = True\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ### Only trains on nodes with genes due to masking\n",
    "        forward_scores = model(graph.x.float(), graph.edge_index, train_status)[targetNode_mask]\n",
    "        \n",
    "        train_scores = forward_scores[train_idx]\n",
    "\n",
    "        train_loss  = model.loss(train_scores, torch.LongTensor(train_labels).to(device))\n",
    "\n",
    "        train_softmax, _ = model.calc_softmax_pred(train_scores)\n",
    "\n",
    "        train_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "            \n",
    "        ### Calculate training and validation loss, AUROC scores\n",
    "        model.eval()\n",
    "        \n",
    "        valid_scores = forward_scores[valid_idx]\n",
    "        valid_loss  = model.loss(valid_scores, torch.LongTensor(valid_labels).to(device))\n",
    "        valid_softmax, _ = model.calc_softmax_pred(valid_scores) \n",
    "\n",
    "        train_loss_list.append(train_loss.item())\n",
    "        train_softmax = to_cpu_npy(train_softmax)\n",
    "        train_AUROC = roc_auc_score(train_labels, train_softmax[:,1], average=\"micro\")\n",
    "\n",
    "        valid_loss_list.append(valid_loss.item())\n",
    "        valid_softmax = to_cpu_npy(valid_softmax)\n",
    "        valid_AUROC = roc_auc_score(valid_labels, valid_softmax[:,1], average=\"micro\")\n",
    "        \n",
    "        train_AUROC_vec[e] = train_AUROC\n",
    "        valid_AUROC_vec[e] = valid_AUROC\n",
    "\n",
    "    train_loss_vec = np.reshape(np.array(train_loss_list), (-1, 1))\n",
    "    valid_loss_vec = np.reshape(np.array(valid_loss_list), (-1, 1))\n",
    "\n",
    "    return train_loss_vec, train_AUROC_vec, valid_loss_vec, valid_AUROC_vec\n",
    "\n",
    "\n",
    "def eval_model_classification(model, graph, targetNode_mask, train_idx, valid_idx, test_idx):\n",
    "    '''\n",
    "    Runs fully trained classification model and compute evaluation statistics\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model [GCN_classification]: Instantiation of model class\n",
    "    graph [PyG Data class]: PyTorch Geometric Data object representing the graph\n",
    "    targetNode_mask [tensor]: Mask ensuring model only trains on nodes with genes\n",
    "    train_idx [array]: Node IDs corresponding to training set;\n",
    "        analogous for valid_idx and test_idx\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    test_AUROC [float]: Test set AUROC score;\n",
    "        analogous for train_AUROC (training set) and valid_AUPR (validation set)\n",
    "    test_AUPR [float]: Test set AUPR score\n",
    "        analogous for train_AUPR (training set) and valid_AUPR (validation set)\n",
    "    test_pred [array]: Test set predictions;\n",
    "        analogous for train_pred (training set) and valid_pred (validation set)\n",
    "    test_labels [array]: Test set labels;\n",
    "        analagous for train_labels (training set) and valid_labels (validation set)\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "    test_labels = to_cpu_npy(graph.y[targetNode_mask[test_idx]])\n",
    "    \n",
    "    model.eval()\n",
    "    train_status=False\n",
    "\n",
    "    forward_scores = model(graph.x.float(), graph.edge_index, train_status)[targetNode_mask]\n",
    "\n",
    "    test_scores = forward_scores[test_idx]\n",
    "    test_softmax, test_pred = model.calc_softmax_pred(test_scores) \n",
    "    \n",
    "    test_softmax = to_cpu_npy(test_softmax)\n",
    "    test_pred = to_cpu_npy(test_pred)\n",
    "    test_AUROC = roc_auc_score(test_labels, test_softmax[:,1], average=\"micro\")\n",
    "    test_precision, test_recall, thresholds = precision_recall_curve(test_labels, test_softmax[:,1])\n",
    "    test_AUPR = auc(test_recall, test_precision)\n",
    "    # test_F1 = f1_score(test_labels, test_pred, average=\"micro\")\n",
    "    \n",
    "    train_scores = forward_scores[train_idx]\n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    train_softmax, train_pred = model.calc_softmax_pred(train_scores) \n",
    "    train_pred = to_cpu_npy(train_pred)\n",
    "    train_softmax = to_cpu_npy(train_softmax)\n",
    "    train_precision, train_recall, thresholds = precision_recall_curve(train_labels, train_softmax[:,1])\n",
    "    train_AUPR = auc(train_recall, train_precision)\n",
    "    # train_F1 = f1_score(train_labels, train_pred, average=\"micro\")\n",
    "\n",
    "    valid_scores = forward_scores[valid_idx]\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    valid_softmax, valid_pred = model.calc_softmax_pred(valid_scores) \n",
    "    valid_pred = to_cpu_npy(valid_pred)\n",
    "    valid_softmax = to_cpu_npy(valid_softmax)\n",
    "    valid_precision, valid_recall, thresholds = precision_recall_curve(valid_labels, valid_softmax[:,1])\n",
    "    valid_AUPR = auc(valid_recall, valid_precision)\n",
    "    # valid_F1 = f1_score(valid_labels, valid_pred, average=\"micro\")\n",
    "\n",
    "    return test_AUROC, test_AUPR, test_pred, test_labels, train_AUPR, train_pred, train_labels, \\\n",
    "        valid_AUPR, valid_pred, valid_labels\n",
    "\n",
    "\n",
    "def train_model_regression(model, graph, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer):\n",
    "    '''\n",
    "    Trains model for regression task\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model [GCN_classification]: Instantiation of model class\n",
    "    graph [PyG Data class]: PyTorch Geometric Data object representing the graph\n",
    "    max_epoch [int]: Maximum number of training epochs\n",
    "    learning_rate [float]: Learning rate\n",
    "    targetNode_mask [tensor]: Subgraph mask for training nodes\n",
    "    train_idx [array]: Node IDs corresponding to training set\n",
    "    valid_idx [array]: Node IDs corresponding to validation set\n",
    "    optimizer [PyTorch optimizer class]: PyTorch optimization algorithm\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train_loss_vec [array]: Training loss for each epoch;\n",
    "        analagous for valid_loss_vec (validation set)\n",
    "    train_pearson_vec [array]: Training PCC for each epoch;\n",
    "        analogous for valid_pearson_vec (validation set)\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "\n",
    "    optimizer = optimizer\n",
    "    \n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    \n",
    "    train_loss_list = []\n",
    "    train_pearson_vec = np.zeros(np.shape(np.arange(max_epoch)))\n",
    "    valid_loss_list = []\n",
    "    valid_pearson_vec = np.zeros(np.shape(np.arange(max_epoch)))\n",
    "\n",
    "    model.train()\n",
    "    train_status = True\n",
    "    \n",
    "    print('\\n')\n",
    "    for e in list(range(max_epoch)):\n",
    "        \n",
    "        if e%100 == 0:\n",
    "            print(\"Epoch\", str(e), 'out of', str(max_epoch))\n",
    "        \n",
    "        model.train()\n",
    "        train_status = True\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ### Only trains on nodes with genes due to masking\n",
    "        forward_scores = model(graph.x.float(), graph.edge_index, train_status)[targetNode_mask]\n",
    "        \n",
    "        train_scores = forward_scores[train_idx]\n",
    "\n",
    "        train_loss  = model.loss(train_scores, torch.FloatTensor(train_labels).to(device))\n",
    "\n",
    "        train_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "            \n",
    "        ### Calculate training and validation loss, AUROC scores\n",
    "        model.eval()\n",
    "        \n",
    "        train_scores = to_cpu_npy(train_scores)\n",
    "        train_pearson = calc_pearson(train_scores, train_labels)\n",
    "        train_loss_list.append(train_loss.item())\n",
    "        \n",
    "        valid_scores = forward_scores[valid_idx]\n",
    "        valid_loss  = model.loss(valid_scores, torch.FloatTensor(valid_labels).to(device))\n",
    "        valid_scores = to_cpu_npy(valid_scores)\n",
    "        valid_pearson  = calc_pearson(valid_scores, valid_labels)\n",
    "        valid_loss_list.append(valid_loss.item())\n",
    "        \n",
    "        train_pearson_vec[e] = train_pearson\n",
    "        valid_pearson_vec[e] = valid_pearson\n",
    "\n",
    "    train_loss_vec = np.reshape(np.array(train_loss_list), (-1, 1))\n",
    "    valid_loss_vec = np.reshape(np.array(valid_loss_list), (-1, 1))\n",
    "\n",
    "    return train_loss_vec, train_pearson_vec, valid_loss_vec, valid_pearson_vec\n",
    "\n",
    "\n",
    "def eval_model_regression(model, graph, targetNode_mask, train_idx, valid_idx, test_idx):\n",
    "    '''\n",
    "    Runs fully trained regression model and compute evaluation statistics\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model [GCN_classification]: Instantiation of model class\n",
    "    graph [PyG Data class]: PyTorch Geometric Data object representing the graph\n",
    "    targetNode_mask [tensor]: Mask ensuring model only trains on nodes with genes\n",
    "    train_idx [array]: Node IDs corresponding to training set;\n",
    "        analogous for valid_idx and test_idx\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    test_pearson [float]: PCC for test set;\n",
    "        analogous for train_pearson (training set) and valid_pearson (validation set)\n",
    "    test_pred [array]: Test set predictions;\n",
    "        analogous for train_pred (training set) and valid_pred (validation set)\n",
    "    test_labels [array]: Test set labels (expression values);\n",
    "        analagous for train_labels (training set) and valid_labels (validation set)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    graph = graph.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    train_status=False\n",
    "\n",
    "    forward_scores = model(graph.x.float(), graph.edge_index, train_status)[targetNode_mask]\n",
    "\n",
    "    test_scores = forward_scores[test_idx]\n",
    "    test_pred = to_cpu_npy(test_scores)\n",
    "    test_labels = to_cpu_npy(graph.y[targetNode_mask[test_idx]])\n",
    "    test_pearson = calc_pearson(test_pred, test_labels)\n",
    "\n",
    "    train_scores = forward_scores[train_idx]\n",
    "    train_pred = to_cpu_npy(train_scores)\n",
    "    train_labels = to_cpu_npy(graph.y[targetNode_mask[train_idx]])\n",
    "    train_pearson = calc_pearson(train_pred, train_labels)\n",
    "\n",
    "    valid_scores = forward_scores[valid_idx]\n",
    "    valid_pred = to_cpu_npy(valid_scores)\n",
    "    valid_labels = to_cpu_npy(graph.y[targetNode_mask[valid_idx]])\n",
    "    valid_pearson = calc_pearson(valid_pred, valid_labels)\n",
    "\n",
    "    return test_pearson, test_pred, test_labels, train_pearson, train_pred, train_labels, \\\n",
    "        valid_pearson, valid_pred, valid_labels\n",
    "        \n",
    "\n",
    "def calc_pearson(scores, targets):\n",
    "    '''\n",
    "    Calculates Pearson correlation coefficient (PCC) between predicted \\\n",
    "        expression levels and true expression levels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scores [array]: Predicted expression levels\n",
    "    targets [array]: True expression levels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pcc [float]: Pearson correlation coefficient\n",
    "\n",
    "    '''\n",
    "    pcc, _ = pearsonr(scores, targets)\n",
    "            \n",
    "    return pcc\n",
    "    \n",
    "    \n",
    "def to_cpu_npy(x):\n",
    "    '''\n",
    "    Simple helper function to transfer GPU tensors to CPU numpy matrices\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x [tensor]: PyTorch tensor stored on GPU\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_x [array]: Numpy array stored on CPU\n",
    "\n",
    "    '''\n",
    "\n",
    "    new_x = x.cpu().detach().numpy()\n",
    "    \n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21f11274-c908-41ca-9908-e92a1cdcfcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSAGEConv(SAGEConv):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(AttentionSAGEConv, self).__init__(in_channels, out_channels)\n",
    "        \n",
    "        # Additional parameters for attention\n",
    "        self.att = nn.Parameter(torch.Tensor(1, out_channels))  # Attention vector\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "        # Initialize parameters\n",
    "        glorot(self.att)  # Glorot initialization\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Standard SAGEConv aggregation\n",
    "        out = super(AttentionSAGEConv, self).forward(x, edge_index)\n",
    "        \n",
    "        # Calculate attention coefficients\n",
    "        row, col = edge_index  # Split edge index\n",
    "        out_i = out[row]  # Messages from source nodes\n",
    "        out_j = out[col]  # Messages to target nodes\n",
    "        \n",
    "        # Compute attention scores with element-wise dot-product and leaky ReLU\n",
    "        alpha = (out_i * self.att).sum(dim=-1) + (out_j * self.att).sum(dim=-1)\n",
    "        alpha = self.leaky_relu(alpha)\n",
    "        \n",
    "        # Softmax to normalize the attention scores on neighbors\n",
    "        alpha = torch.exp(alpha)\n",
    "        alpha_sum = torch.zeros_like(alpha).scatter_add(0, row, alpha)\n",
    "        alpha = alpha / (alpha_sum[row] + 1e-16)\n",
    "        \n",
    "        # Apply attention scores to aggregated neighbors\n",
    "        out = out * alpha.view(-1, 1)  # Element-wise multiply with attention scores\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa050a5b-c3fa-4d25-8ac2-84f419f5556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN_regression(nn.Module):\n",
    "\n",
    "    def __init__(self, num_feat, num_graph_conv_layers, graph_conv_layer_sizes, num_lin_layers, lin_hidden_sizes, num_classes):\n",
    "        '''\n",
    "        Defines regression model class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_feat [int]: Feature dimension (int)\n",
    "        num_graph_conv_layers [int]: Number of graph convolutional layers (1, 2, or 3)\n",
    "        graph_conv_layer_sizes [int]: Embedding size of graph convolutional layers \n",
    "        num_lin_layers [int]: Number of linear layers (1, 2, or 3)\n",
    "        lin_hidden_sizes [int]: Embedding size of hidden linear layers\n",
    "        num_classes [int]: Size of predicted output tensor for batch size of N, \n",
    "            i.e. N x num_classes(=1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        '''\n",
    "        \n",
    "        super(GAN_regression, self).__init__()\n",
    "        \n",
    "        self.num_graph_conv_layers = num_graph_conv_layers\n",
    "        self.num_lin_layers = num_lin_layers\n",
    "        self.dropout = 0.5\n",
    "    \n",
    "        if self.num_graph_conv_layers == 1:\n",
    "            self.conv1 = AttentionSAGEConv(graph_conv_layer_sizes[0], graph_conv_layer_sizes[1])\n",
    "        elif self.num_graph_conv_layers == 2:\n",
    "            self.conv1 = AttentionSAGEConv(graph_conv_layer_sizes[0], graph_conv_layer_sizes[1])\n",
    "            self.conv2 = AttentionSAGEConv(graph_conv_layer_sizes[1], graph_conv_layer_sizes[2])\n",
    "        elif self.num_graph_conv_layers == 3:\n",
    "            self.conv1 = AttentionSAGEConv(graph_conv_layer_sizes[0], graph_conv_layer_sizes[1])\n",
    "            self.conv2 = AttentionSAGEConv(graph_conv_layer_sizes[1], graph_conv_layer_sizes[2])\n",
    "            self.conv3 = AttentionSAGEConv(graph_conv_layer_sizes[2], graph_conv_layer_sizes[3])\n",
    "        \n",
    "        if self.num_lin_layers == 1:\n",
    "            self.lin1 = nn.Linear(lin_hidden_sizes[0], lin_hidden_sizes[1])\n",
    "        elif self.num_lin_layers == 2:\n",
    "            self.lin1 = nn.Linear(lin_hidden_sizes[0], lin_hidden_sizes[1])\n",
    "            self.lin2 = nn.Linear(lin_hidden_sizes[1], lin_hidden_sizes[2])\n",
    "        elif self.num_lin_layers == 3:\n",
    "            self.lin1 = nn.Linear(lin_hidden_sizes[0], lin_hidden_sizes[1])\n",
    "            self.lin2 = nn.Linear(lin_hidden_sizes[1], lin_hidden_sizes[2])\n",
    "            self.lin3 = nn.Linear(lin_hidden_sizes[2], lin_hidden_sizes[3])\n",
    "        \n",
    "        self.loss_calc = nn.MSELoss()\n",
    "\n",
    "        \n",
    "    def forward(self, x, edge_index, train_status=False):\n",
    "        '''\n",
    "        Forward function\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x [tensor]: Node features\n",
    "        edge_index [tensor]: Subgraph mask\n",
    "        train_status [bool]: optional, set to True for dropout\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        scores [tensor]: Predicted expression levels\n",
    "\n",
    "        '''\n",
    "        ### Graph convolution module\n",
    "        if self.num_graph_conv_layers == 1:\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = torch.relu(h)\n",
    "        elif self.num_graph_conv_layers == 2:\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = torch.relu(h)\n",
    "            h = self.conv2(h, edge_index)\n",
    "            h = torch.relu(h)\n",
    "        elif self.num_graph_conv_layers == 3:\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = torch.relu(h)\n",
    "            h = self.conv2(h, edge_index)\n",
    "            h = torch.relu(h)\n",
    "            h = self.conv3(h, edge_index)\n",
    "            h = torch.relu(h)\n",
    "\n",
    "        h = F.dropout(h, p = self.dropout, training=train_status)\n",
    "\n",
    "        if self.num_lin_layers == 1:\n",
    "            scores = self.lin1(h)\n",
    "        elif self.num_lin_layers == 2:\n",
    "            scores = self.lin1(h)\n",
    "            scores = torch.relu(scores)\n",
    "            scores = self.lin2(scores)\n",
    "        elif self.num_lin_layers == 3:\n",
    "            scores = self.lin1(h)\n",
    "            scores = torch.relu(scores)\n",
    "            scores = self.lin2(scores)\n",
    "            scores = torch.relu(scores)\n",
    "            scores = self.lin3(scores)\n",
    "        \n",
    "        if len(scores.size()) > 1:\n",
    "            scores = scores.squeeze()\n",
    "            \n",
    "        return scores\n",
    "    \n",
    "    \n",
    "    def loss(self, scores, targets):\n",
    "        '''\n",
    "        Calculates mean squared error loss\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        scores [tensor]: Predicted scores from forward function\n",
    "        labels [tensor]: Target scores \n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mse [tensor]: Mean squared error loss\n",
    "\n",
    "        '''\n",
    "        \n",
    "        mse = self.loss_calc(scores, targets)\n",
    "\n",
    "        return mse\n",
    "    \n",
    "\n",
    "class GAN_classification(nn.Module):\n",
    "\n",
    "    def __init__(self, num_feat, num_graph_conv_layers, graph_conv_layer_sizes, num_lin_layers, lin_hidden_sizes, num_classes):\n",
    "        '''\n",
    "        Defines classification model class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_feat [int]: Feature dimension (int)\n",
    "        num_graph_conv_layers [int]: Number of graph convolutional layers (1, 2, or 3)\n",
    "        graph_conv_layer_sizes [int]: Embedding size of graph convolutional layers \n",
    "        num_lin_layers [int]: Number of linear layers (1, 2, or 3)\n",
    "        lin_hidden_sizes [int]: Embedding size of hidden linear layers\n",
    "        num_classes [int]: Number of classes to be predicted(=2)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        '''\n",
    "        \n",
    "        super(GAN_classification, self).__init__()\n",
    "\n",
    "        self.num_graph_conv_layers = num_graph_conv_layers\n",
    "        self.num_lin_layers = num_lin_layers\n",
    "        self.dropout_value = 0.5\n",
    "\n",
    "        if self.num_graph_conv_layers == 1:\n",
    "            self.conv1 = AttentionSAGEConv(graph_conv_layer_sizes[0], graph_conv_layer_sizes[1])\n",
    "        elif self.num_graph_conv_layers == 2:\n",
    "            self.conv1 = AttentionSAGEConv(graph_conv_layer_sizes[0], graph_conv_layer_sizes[1])\n",
    "            self.conv2 = AttentionSAGEConv(graph_conv_layer_sizes[1], graph_conv_layer_sizes[2])\n",
    "        elif self.num_graph_conv_layers == 3:\n",
    "            self.conv1 = AttentionSAGEConv(graph_conv_layer_sizes[0], graph_conv_layer_sizes[1])\n",
    "            self.conv2 = AttentionSAGEConv(graph_conv_layer_sizes[1], graph_conv_layer_sizes[2])\n",
    "            self.conv3 = AttentionSAGEConv(graph_conv_layer_sizes[2], graph_conv_layer_sizes[3])\n",
    "        \n",
    "        if self.num_lin_layers == 1:\n",
    "            self.lin1 = nn.Linear(lin_hidden_sizes[0], lin_hidden_sizes[1])\n",
    "        elif self.num_lin_layers == 2:\n",
    "            self.lin1 = nn.Linear(lin_hidden_sizes[0], lin_hidden_sizes[1])\n",
    "            self.lin2 = nn.Linear(lin_hidden_sizes[1], lin_hidden_sizes[2])\n",
    "        elif self.num_lin_layers == 3:\n",
    "            self.lin1 = nn.Linear(lin_hidden_sizes[0], lin_hidden_sizes[1])\n",
    "            self.lin2 = nn.Linear(lin_hidden_sizes[1], lin_hidden_sizes[2])\n",
    "            self.lin3 = nn.Linear(lin_hidden_sizes[2], lin_hidden_sizes[3])\n",
    "            \n",
    "        self.loss_calc = nn.CrossEntropyLoss()\n",
    "        self.torch_softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, edge_index, train_status=False):\n",
    "        '''\n",
    "        Forward function.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x [tensor]: Node features\n",
    "        edge_index [tensor]: Subgraph mask\n",
    "        train_status [bool]: optional, set to True for dropout\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        scores [tensor]: Pre-normalized class scores\n",
    "\n",
    "        '''\n",
    "\n",
    "        ### Graph convolution module\n",
    "        if self.num_graph_conv_layers == 1:\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = torch.relu(h)\n",
    "        elif self.num_graph_conv_layers == 2:\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = torch.relu(h)\n",
    "            h = self.conv2(h, edge_index)\n",
    "            h = torch.relu(h)\n",
    "        elif self.num_graph_conv_layers == 3:\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = torch.relu(h)\n",
    "            h = self.conv2(h, edge_index)\n",
    "            h = torch.relu(h)\n",
    "            h = self.conv3(h, edge_index)\n",
    "            h = torch.relu(h)\n",
    "            \n",
    "        h = F.dropout(h, p = self.dropout_value, training=train_status)\n",
    "\n",
    "        ### Linear module\n",
    "        if self.num_lin_layers == 1:\n",
    "            scores = self.lin1(h)\n",
    "        elif self.num_lin_layers == 2:\n",
    "            scores = self.lin1(h)\n",
    "            scores = torch.relu(scores)\n",
    "            scores = self.lin2(scores)\n",
    "        elif self.num_lin_layers == 3:\n",
    "            scores = self.lin1(h)\n",
    "            scores = torch.relu(scores)\n",
    "            scores = self.lin2(scores)\n",
    "            scores = torch.relu(scores)\n",
    "            scores = self.lin3(scores)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    \n",
    "    def loss(self, scores, labels):\n",
    "        '''\n",
    "        Calculates cross-entropy loss\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        scores [tensor]: Pre-normalized class scores from forward function\n",
    "        labels [tensor]: Class labels for nodes\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        xent_loss [tensor]: Cross-entropy loss\n",
    "\n",
    "        '''\n",
    "\n",
    "        xent_loss = self.loss_calc(scores, labels)\n",
    "\n",
    "        return xent_loss\n",
    "    \n",
    "    \n",
    "    def calc_softmax_pred(self, scores):\n",
    "        '''\n",
    "        Calculates softmax scores and predicted classes\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        scores [tensor]: Pre-normalized class scores\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        softmax [tensor]: Probability for each class\n",
    "        predicted [tensor]: Predicted class\n",
    "\n",
    "        '''\n",
    "        \n",
    "        softmax = self.torch_softmax(scores)\n",
    "        \n",
    "        predicted = torch.argmax(softmax, 1)\n",
    "        \n",
    "        return softmax, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9af55b56-2af5-4f1b-8f2b-15a10a2d3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter and model definitions\n",
    "cell_line = 'E116'\n",
    "max_epoch = 1000\n",
    "learning_rate = 1e-4\n",
    "num_graph_conv_layers = 2\n",
    "graph_conv_embed_size = 256\n",
    "num_lin_layers = 3\n",
    "lin_hidden_size = 256\n",
    "regression_flag = 0\n",
    "random_seed = 0\n",
    "\n",
    "chip_res = 10000\n",
    "hic_res = 10000\n",
    "num_hm = 6\n",
    "num_feat = int((hic_res/chip_res)*num_hm)\n",
    "num_classes = 2 if regression_flag == 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9e9b1f9-4e7f-4a9b-8240-ecfda2c47997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell line: E116\n",
      "ChIP-seq resolution: 10000\n",
      "\n",
      "\n",
      "Training set: 70%\n",
      "Validation set: 15%\n",
      "Testing set: 15%\n",
      "\n",
      "\n",
      "Model hyperparameters: \n",
      "Number of epochs: 1000\n",
      "Learning rate: 0.0001\n",
      "Number of graph convolutional layers: 2\n",
      "Graph convolutional embedding size: 256\n",
      "Number of linear layers: 3\n",
      "Linear hidden layer size: 256\n"
     ]
    }
   ],
   "source": [
    "print('Cell line:', cell_line)\n",
    "print('ChIP-seq resolution:', str(chip_res))\n",
    "print('\\n')\n",
    "print('Training set: 70%')\n",
    "print('Validation set: 15%')\n",
    "print('Testing set: 15%')\n",
    "print('\\n')\n",
    "print('Model hyperparameters: ')\n",
    "print('Number of epochs:', max_epoch)\n",
    "print('Learning rate:', learning_rate)\n",
    "print('Number of graph convolutional layers:', str(num_graph_conv_layers))\n",
    "print('Graph convolutional embedding size:', graph_conv_embed_size)\n",
    "print('Number of linear layers:', str(num_lin_layers))\n",
    "print('Linear hidden layer size:', lin_hidden_size)\n",
    "\n",
    "# random_seed = random.randint(0,10000)\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "###Test for GPU availability\n",
    "cuda_flag = torch.cuda.is_available()\n",
    "if cuda_flag:  \n",
    "  dev = \"cuda\" \n",
    "else:\n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cbed5fc-58e7-4c2f-92b4-be85d79eb8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Load input files\n",
    "def prepare_data(cell_line, regression_flag):\n",
    "    base_path = os.getcwd()\n",
    "    save_dir = os.path.join(base_path, 'data', cell_line, 'saved_runs')\n",
    "    hic_sparse_mat_file = os.path.join(base_path, 'data', cell_line, 'hic_sparse.npz')\n",
    "    np_nodes_lab_genes_file = os.path.join(base_path, 'data',  cell_line, \\\n",
    "        'np_nodes_lab_genes_reg' + str(regression_flag) + '.npy')\n",
    "    np_hmods_norm_all_file = os.path.join(base_path, 'data', cell_line, \\\n",
    "        'np_hmods_norm_chip_' + str(chip_res) + 'bp.npy')\n",
    "    df_genes_file = os.path.join(base_path, 'data', cell_line, 'df_genes_reg' + str(regression_flag) + '.pkl')\n",
    "    df_genes = pd.read_pickle(df_genes_file)\n",
    "    \n",
    "    mat = load_npz(hic_sparse_mat_file)\n",
    "    allNodes_hms = np.load(np_hmods_norm_all_file)\n",
    "    hms = allNodes_hms[:, 1:] #only includes features, not node ids\n",
    "    X = torch.tensor(hms).float().reshape(-1, num_feat) \n",
    "    allNodes = allNodes_hms[:, 0].astype(int)\n",
    "    geneNodes_labs = np.load(np_nodes_lab_genes_file)\n",
    "\n",
    "    geneNodes = geneNodes_labs[:, -2].astype(int)\n",
    "    allLabs = -1*np.ones(np.shape(allNodes))\n",
    "\n",
    "    targetNode_mask = torch.tensor(geneNodes).long()\n",
    "\n",
    "    if regression_flag == 0:\n",
    "        geneLabs = geneNodes_labs[:, -1].astype(int)\n",
    "        allLabs[geneNodes] = geneLabs\n",
    "        Y = torch.tensor(allLabs).long()\n",
    "    else:\n",
    "        geneLabs = geneNodes_labs[:, -1].astype(float)\n",
    "        allLabs[geneNodes] = geneLabs\n",
    "        Y = torch.tensor(allLabs).float()\n",
    "\n",
    "    extract = torch_geometric.utils.from_scipy_sparse_matrix(mat)\n",
    "    data = torch_geometric.data.Data(edge_index = extract[0], edge_attr = extract[1], x = X, y = Y)\n",
    "    G = data\n",
    "    \n",
    "    ###Randomize node order and split into 70%/15%/15% training/validation/test sets\n",
    "    pred_idx_shuff = torch.randperm(targetNode_mask.shape[0])\n",
    "    fin_train = np.floor(0.7*pred_idx_shuff.shape[0]).astype(int)\n",
    "    fin_valid = np.floor(0.85*pred_idx_shuff.shape[0]).astype(int)\n",
    "    train_idx = pred_idx_shuff[:fin_train]\n",
    "    valid_idx = pred_idx_shuff[fin_train:fin_valid]\n",
    "    test_idx = pred_idx_shuff[fin_valid:]\n",
    "    \n",
    "    return G, targetNode_mask, train_idx, valid_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e463b991-32c0-488f-a71a-af9e75d62783",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_conv_embed_size = 256\n",
    "num_graph_conv_layers = 2\n",
    "\n",
    "graph_conv_layer_sizes = [num_feat] + \\\n",
    "    [int(max(graph_conv_embed_size, lin_hidden_size)) \\\n",
    "          for i in np.arange(1, num_graph_conv_layers, 1)] + [lin_hidden_size]\n",
    "\n",
    "graph_lin_hidden_sizes = [graph_conv_layer_sizes[-1]] + \\\n",
    "    [int(max(lin_hidden_size, num_classes)) \\\n",
    "          for i in np.arange(1, num_lin_layers, 1)] + [num_classes]\n",
    "\n",
    "graph_lin_hidden_sizes_reg = [graph_conv_layer_sizes[-1]] + \\\n",
    "    [int(max(lin_hidden_size, num_classes)) \\\n",
    "          for i in np.arange(1, num_lin_layers, 1)] + [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb833077-660c-4299-8cab-6fc4bed7e93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 256, 256, 2]\n"
     ]
    }
   ],
   "source": [
    "num_classes_reg = 1\n",
    "lin_hidden_sizes = [num_feat] + [int(max(lin_hidden_size, num_classes)) for i in np.arange(1, num_lin_layers, 1)] + [num_classes]\n",
    "lin_hidden_sizes_reg = [num_feat] + [int(max(lin_hidden_size, num_classes_reg)) for i in np.arange(1, num_lin_layers, 1)] + [num_classes_reg]\n",
    "\n",
    "print(lin_hidden_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d7f2e66-2924-4f14-853d-29c936e5397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_lines = ['E116', 'E122', 'E123']\n",
    "classification_res = pd.DataFrame(columns=cell_lines)\n",
    "regression_res = pd.DataFrame(columns=cell_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b90b0d09-0291-4cb9-b5d1-afb5f76c8cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Cell Line E116...\n",
      "\n",
      "\n",
      "Epoch 0 out of 1000\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.73 GiB. GPU 0 has a total capacity of 10.91 GiB of which 2.01 GiB is free. Process 40256 has 4.62 GiB memory in use. Process 8912 has 4.28 GiB memory in use. Of the allocated memory 4.11 GiB is allocated by PyTorch, and 13.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m GAN_classification(num_feat, num_graph_conv_layers, graph_conv_layer_sizes, num_lin_layers, graph_lin_hidden_sizes, num_classes)\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p : p\u001b[38;5;241m.\u001b[39mrequires_grad, model\u001b[38;5;241m.\u001b[39mparameters()), lr \u001b[38;5;241m=\u001b[39m learning_rate)\n\u001b[1;32m      9\u001b[0m train_loss_vec, train_AUROC_vec, valid_loss_vec, valid_AUROC_vec \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain_model_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetNode_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m test_AUROC, test_AUPR, test_pred, test_labels, train_AUPR, train_pred, train_labels, \\\n\u001b[1;32m     13\u001b[0m     valid_AUPR, valid_pred, valid_labels \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m     14\u001b[0m         eval_model_classification(model, G, targetNode_mask, train_idx, valid_idx, test_idx)\n\u001b[1;32m     16\u001b[0m gcn_auroc\u001b[38;5;241m.\u001b[39mappend(test_AUROC)\n",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m, in \u001b[0;36mtrain_model_classification\u001b[0;34m(model, graph, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer)\u001b[0m\n\u001b[1;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m### Only trains on nodes with genes due to masking\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m forward_scores \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_status\u001b[49m\u001b[43m)\u001b[49m[targetNode_mask]\n\u001b[1;32m     57\u001b[0m train_scores \u001b[38;5;241m=\u001b[39m forward_scores[train_idx]\n\u001b[1;32m     59\u001b[0m train_loss  \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(train_scores, torch\u001b[38;5;241m.\u001b[39mLongTensor(train_labels)\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 197\u001b[0m, in \u001b[0;36mGAN_classification.forward\u001b[0;34m(self, x, edge_index, train_status)\u001b[0m\n\u001b[1;32m    195\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(h)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_graph_conv_layers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 197\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(h)\n\u001b[1;32m    199\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(h, edge_index)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m, in \u001b[0;36mAttentionSAGEConv.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     17\u001b[0m row, col \u001b[38;5;241m=\u001b[39m edge_index  \u001b[38;5;66;03m# Split edge index\u001b[39;00m\n\u001b[1;32m     18\u001b[0m out_i \u001b[38;5;241m=\u001b[39m out[row]  \u001b[38;5;66;03m# Messages from source nodes\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m out_j \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Messages to target nodes\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Compute attention scores with element-wise dot-product and leaky ReLU\u001b[39;00m\n\u001b[1;32m     22\u001b[0m alpha \u001b[38;5;241m=\u001b[39m (out_i \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m (out_j \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.73 GiB. GPU 0 has a total capacity of 10.91 GiB of which 2.01 GiB is free. Process 40256 has 4.62 GiB memory in use. Process 8912 has 4.28 GiB memory in use. Of the allocated memory 4.11 GiB is allocated by PyTorch, and 13.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "gcn_auroc = []\n",
    "for cell_line in cell_lines:\n",
    "    print(f'\\nTraining Cell Line {cell_line}...')\n",
    "    G, targetNode_mask, train_idx, valid_idx, test_idx = prepare_data(cell_line=cell_line, regression_flag = regression_flag)\n",
    "    \n",
    "    model = GAN_classification(num_feat, num_graph_conv_layers, graph_conv_layer_sizes, num_lin_layers, graph_lin_hidden_sizes, num_classes)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), lr = learning_rate)\n",
    "    \n",
    "    train_loss_vec, train_AUROC_vec, valid_loss_vec, valid_AUROC_vec = \\\n",
    "    train_model_classification(model, G, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer)\n",
    "\n",
    "    test_AUROC, test_AUPR, test_pred, test_labels, train_AUPR, train_pred, train_labels, \\\n",
    "        valid_AUPR, valid_pred, valid_labels = \\\n",
    "            eval_model_classification(model, G, targetNode_mask, train_idx, valid_idx, test_idx)\n",
    "    \n",
    "    gcn_auroc.append(test_AUROC)\n",
    "    \n",
    "classification_res.loc['GCN'] = gcn_auroc\n",
    "classification_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a2b45-a6c5-420a-a9af-b08c9431b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes_reg = 1\n",
    "\n",
    "gcn_pearson = []\n",
    "for cell_line in cell_lines:\n",
    "    print(f'\\nTraining Cell Line {cell_line}...')\n",
    "    G, targetNode_mask, train_idx, valid_idx, test_idx = prepare_data(cell_line=cell_line, regression_flag = 1)\n",
    "    \n",
    "    model = GAN_regression(num_feat, num_graph_conv_layers, graph_conv_layer_sizes, num_lin_layers, graph_lin_hidden_sizes_reg, num_classes_reg)\n",
    "    optimizer = torch.optim.Adam(filter(lambda p : p.requires_grad, model.parameters()), lr = learning_rate)\n",
    "    \n",
    "    train_loss_vec, train_pearson_vec, valid_loss_vec, valid_pearson_vec = \\\n",
    "        train_model_regression(model, G, max_epoch, learning_rate, targetNode_mask, train_idx, valid_idx, optimizer)\n",
    "    \n",
    "    test_pearson, test_pred, test_labels, train_pearson, train_pred, train_labels, \\\n",
    "        valid_pearson, valid_pred, valid_labels = \\\n",
    "            eval_model_regression(model, G, targetNode_mask, train_idx, valid_idx, test_idx)\n",
    "    \n",
    "    gcn_pearson.append(test_pearson)\n",
    "    \n",
    "regression_res.loc['GCN'] = gcn_pearson\n",
    "regression_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
