{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79cd282d",
   "metadata": {},
   "source": [
    "#### Question From Midterm Report:\n",
    "\n",
    "**3:** Letâ€™s say your training a model and the accuracy for the model plateaued after the first epoch. I.e. the accuracy remains the same for every epoch. List 5 factors that could cause this and how you would address each of them.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1. Check the learning rate. If the learning rate is too high, training could converge quickly and plateau after the first epoch. It also could have quickly found a local minimum, so the best solution is not found. A learning rate that is too low would not be a likely cause of this issue, as training would converge slower, not faster. If a low learning rate still causes the training to plateau, another issue is likely the cause.\n",
    "\n",
    "2. Check the distribution of classes. If the model is a classification model, the classes could be imbalanced. For example, 94% of data could be in one class, and the remaining 6% could be scattered across 5 other classes. This would cause the training to only be able to detect some patterns from the data, or cause it to only predict the class containing 94% of the data every time. Therefore, learning doesn't improve, and it plateaus after the first epoch. This can be fixed by oversampling the minority classes or undersampling the majority class to create a more even distribution for training. Another option is to weigh the minority classes much higher than the majority class in training.\n",
    "\n",
    "3. Check the batch size. If the batch size is really small, training accuracy could increase quickly and stop. If the batch size is too large, it could overfit. Finding the right batch size through hyperparameter tuning would ensure both issues don't occur.\n",
    "\n",
    "4. Check if the model is too large. If the model is too large (and/or the learning rate is too high), it could cause training to plateau early. Using regularization such as dropout, L1, or L2 could help reduce the complexity of the model to the point where training doesn't plateau after 1 epoch.\n",
    "\n",
    "5. Check the model parameters. Perhaps the choices of parameters such as number of layers, input size, output size, or hidden dimension size are not optimal for the model. Using grid search or other hyperparameter tuning methods to find optimal values for each of these could help the training stagnation issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57374c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
