{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime, date\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_line = 'E116'\n",
    "max_epoch = 1000\n",
    "learning_rate = 1e-4\n",
    "num_graph_conv_layers = 2\n",
    "graph_conv_embed_size = 256\n",
    "num_lin_layers = 3\n",
    "lin_hidden_size = 256\n",
    "regression_flag = 0\n",
    "random_seed = 0\n",
    "\n",
    "chip_res = 10000\n",
    "hic_res = 10000\n",
    "num_hm = 6\n",
    "num_feat = int((hic_res/chip_res)*num_hm)\n",
    "num_classes = 2 if regression_flag == 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "save_dir = os.path.join(base_path, 'data', cell_line, 'saved_runs')\n",
    "hic_sparse_mat_file = os.path.join(base_path, 'data', cell_line, 'hic_sparse.npz')\n",
    "np_nodes_lab_genes_file = os.path.join(base_path, 'data',  cell_line, \\\n",
    "    'np_nodes_lab_genes_reg' + str(regression_flag) + '.npy')\n",
    "np_hmods_norm_all_file = os.path.join(base_path, 'data', cell_line, \\\n",
    "    'np_hmods_norm_chip_' + str(chip_res) + 'bp.npy')\n",
    "df_genes_file = os.path.join(base_path, 'data', cell_line, 'df_genes_reg' + str(regression_flag) + '.pkl')\n",
    "df_genes = pd.read_pickle(df_genes_file)\n",
    "\n",
    "mat = load_npz(hic_sparse_mat_file)\n",
    "allNodes_hms = np.load(np_hmods_norm_all_file)\n",
    "hms = allNodes_hms[:, 1:] #only includes features, not node ids\n",
    "X = torch.tensor(hms).float().reshape(-1, num_feat) \n",
    "allNodes = allNodes_hms[:, 0].astype(int)\n",
    "geneNodes_labs = np.load(np_nodes_lab_genes_file)\n",
    "\n",
    "geneNodes = geneNodes_labs[:, -2].astype(int)\n",
    "allLabs = -1*np.ones(np.shape(allNodes))\n",
    "\n",
    "targetNode_mask = torch.tensor(geneNodes).long()\n",
    "\n",
    "if regression_flag == 0:\n",
    "    geneLabs = geneNodes_labs[:, -1].astype(int)\n",
    "    allLabs[geneNodes] = geneLabs\n",
    "    Y = torch.tensor(allLabs).long()\n",
    "else:\n",
    "    geneLabs = geneNodes_labs[:, -1].astype(float)\n",
    "    allLabs[geneNodes] = geneLabs\n",
    "    Y = torch.tensor(allLabs).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_idx_shuff = torch.randperm(targetNode_mask.shape[0])\n",
    "fin_train = np.floor(0.7*pred_idx_shuff.shape[0]).astype(int)\n",
    "fin_valid = np.floor(0.85*pred_idx_shuff.shape[0]).astype(int)\n",
    "train_idx = pred_idx_shuff[:fin_train]\n",
    "valid_idx = pred_idx_shuff[fin_train:fin_valid]\n",
    "test_idx = pred_idx_shuff[fin_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LENGTH = 6\n",
    "NUM_CLASSES = 2 \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_conv_layers, num_linear_layers, dropout_rate=0.2):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        in_channels = 1\n",
    "        out_channels = 16\n",
    "        current_length = INPUT_LENGTH\n",
    "\n",
    "        for i in range(num_conv_layers):\n",
    "            conv = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=2, stride=2) if current_length // 2 > 0 else nn.Identity(),\n",
    "            )\n",
    "            self.conv_layers.append(conv)\n",
    "            \n",
    "            in_channels = out_channels\n",
    "            out_channels *= 2\n",
    "            current_length = max(1, current_length // 2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        linear_input_size = in_channels * current_length\n",
    "\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "        if num_linear_layers > 1:\n",
    "            for i in range(num_linear_layers - 1):\n",
    "                next_size = int(linear_input_size // 2)\n",
    "                fc = nn.Sequential(\n",
    "                    nn.Linear(int(linear_input_size), next_size),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                self.linear_layers.append(fc)\n",
    "                linear_input_size = next_size\n",
    "\n",
    "        self.final_linear = nn.Linear(linear_input_size, NUM_CLASSES)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layers[0](x)\n",
    "        \n",
    "        for layer in self.conv_layers[1:]:\n",
    "            out = layer(out)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "        out = self.flatten(out)\n",
    "        for layer in self.linear_layers:\n",
    "            out = layer(out)\n",
    "        \n",
    "        out = self.final_linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def calculate_accuracy(self, dataset):\n",
    "        data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        num_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_inputs, batch_labels in data_loader:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                output = model(batch_inputs)\n",
    "                pred = torch.argmax(output, dim=1)\n",
    "                num_correct += torch.sum(pred == batch_labels)\n",
    "            \n",
    "        return float(num_correct / len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X[targetNode_mask][train_idx]\n",
    "train_labels = torch.tensor(geneNodes_labs[train_idx][:, 1]).long()\n",
    "\n",
    "valid_data = X[targetNode_mask][valid_idx]\n",
    "valid_labels = torch.tensor(geneNodes_labs[valid_idx][:, 1]).long()\n",
    "\n",
    "test_data = X[targetNode_mask][test_idx]\n",
    "test_labels = torch.tensor(geneNodes_labs[test_idx][:, 1]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.unsqueeze(1)\n",
    "test_data = test_data.unsqueeze(1)\n",
    "valid_data = valid_data.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(valid_data, valid_labels)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_data, valid_labels)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    num_epochs = 100\n",
    "    valid_accuracies = []\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_inputs, batch_labels in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(batch_inputs)\n",
    "            loss = criterion(output, batch_labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_data_loader)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            valid_accuracy = model.calculate_accuracy(valid_dataset)\n",
    "            valid_accuracies.append(valid_accuracy)\n",
    "            print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {avg_loss:.4f}, Validation Accuracy: {valid_accuracy:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if len(valid_accuracies) > 2:\n",
    "            if (valid_accuracies[-1] < valid_accuracies[-2]) and (valid_accuracies[-1] < valid_accuracies[-3]):\n",
    "                print(f'Training stopped due to early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "    return valid_accuracies[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.5253, Validation Accuracy: 0.8251\n",
      "Epoch 2/300, Loss: 0.4147\n",
      "Epoch 3/300, Loss: 0.4094\n",
      "Epoch 4/300, Loss: 0.4038\n",
      "Epoch 5/300, Loss: 0.3938\n",
      "Epoch 6/300, Loss: 0.3901\n",
      "Epoch 7/300, Loss: 0.3860\n",
      "Epoch 8/300, Loss: 0.3850\n",
      "Epoch 9/300, Loss: 0.3855\n",
      "Epoch 10/300, Loss: 0.3851\n",
      "Epoch 11/300, Loss: 0.3824, Validation Accuracy: 0.8371\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7g/_rr8_gwn7374lww7jr31vz080000gn/T/ipykernel_57563/1398785012.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/7g/_rr8_gwn7374lww7jr31vz080000gn/T/ipykernel_57563/2916450457.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, n_epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Capstone_MT_Q1_Proj-master/capstone_venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/7g/_rr8_gwn7374lww7jr31vz080000gn/T/ipykernel_57563/870868217.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Capstone_MT_Q1_Proj-master/capstone_venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Capstone_MT_Q1_Proj-master/capstone_venv/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Capstone_MT_Q1_Proj-master/capstone_venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Capstone_MT_Q1_Proj-master/capstone_venv/lib/python3.7/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m         return F.max_pool1d(input, self.kernel_size, self.stride,\n\u001b[1;32m     93\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                             return_indices=self.return_indices)\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Capstone_MT_Q1_Proj-master/capstone_venv/lib/python3.7/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Capstone_MT_Q1_Proj-master/capstone_venv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool1d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CNN(num_conv_layers=3, num_linear_layers=2, dropout_rate=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train(model, n_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0\n",
      "Epoch 1/200, Loss: 0.6515, Validation Accuracy: 0.7705\n",
      "Epoch 2/200, Loss: 0.5252\n",
      "Epoch 3/200, Loss: 0.4559\n",
      "Epoch 4/200, Loss: 0.4370\n",
      "Epoch 5/200, Loss: 0.4301\n",
      "Epoch 6/200, Loss: 0.4245\n",
      "Epoch 7/200, Loss: 0.4205\n",
      "Epoch 8/200, Loss: 0.4170\n",
      "Epoch 9/200, Loss: 0.4138\n",
      "Epoch 10/200, Loss: 0.4106\n",
      "Epoch 11/200, Loss: 0.4076, Validation Accuracy: 0.8323\n",
      "Epoch 12/200, Loss: 0.4047\n",
      "Epoch 13/200, Loss: 0.4027\n",
      "Epoch 14/200, Loss: 0.4007\n",
      "Epoch 15/200, Loss: 0.3993\n",
      "Epoch 16/200, Loss: 0.3969\n",
      "Epoch 17/200, Loss: 0.3952\n",
      "Epoch 18/200, Loss: 0.3945\n",
      "Epoch 19/200, Loss: 0.3933\n",
      "Epoch 20/200, Loss: 0.3917\n",
      "Epoch 21/200, Loss: 0.3910, Validation Accuracy: 0.8343\n",
      "Epoch 22/200, Loss: 0.3903\n",
      "Epoch 23/200, Loss: 0.3900\n",
      "Epoch 24/200, Loss: 0.3894\n",
      "Epoch 25/200, Loss: 0.3884\n",
      "Epoch 26/200, Loss: 0.3879\n",
      "Epoch 27/200, Loss: 0.3875\n",
      "Epoch 28/200, Loss: 0.3869\n",
      "Epoch 29/200, Loss: 0.3865\n",
      "Epoch 30/200, Loss: 0.3865\n",
      "Epoch 31/200, Loss: 0.3858, Validation Accuracy: 0.8383\n",
      "Epoch 32/200, Loss: 0.3856\n",
      "Epoch 33/200, Loss: 0.3850\n",
      "Epoch 34/200, Loss: 0.3851\n",
      "Epoch 35/200, Loss: 0.3850\n",
      "Epoch 36/200, Loss: 0.3845\n",
      "Epoch 37/200, Loss: 0.3842\n",
      "Epoch 38/200, Loss: 0.3846\n",
      "Epoch 39/200, Loss: 0.3837\n",
      "Epoch 40/200, Loss: 0.3835\n",
      "Epoch 41/200, Loss: 0.3834, Validation Accuracy: 0.8375\n",
      "Epoch 42/200, Loss: 0.3839\n",
      "Epoch 43/200, Loss: 0.3834\n",
      "Epoch 44/200, Loss: 0.3829\n",
      "Epoch 45/200, Loss: 0.3832\n",
      "Epoch 46/200, Loss: 0.3834\n",
      "Epoch 47/200, Loss: 0.3827\n",
      "Epoch 48/200, Loss: 0.3827\n",
      "Epoch 49/200, Loss: 0.3822\n",
      "Epoch 50/200, Loss: 0.3820\n",
      "Epoch 51/200, Loss: 0.3821, Validation Accuracy: 0.8383\n",
      "Epoch 52/200, Loss: 0.3821\n",
      "Epoch 53/200, Loss: 0.3823\n",
      "Epoch 54/200, Loss: 0.3814\n",
      "Epoch 55/200, Loss: 0.3818\n",
      "Epoch 56/200, Loss: 0.3813\n",
      "Epoch 57/200, Loss: 0.3813\n",
      "Epoch 58/200, Loss: 0.3810\n",
      "Epoch 59/200, Loss: 0.3815\n",
      "Epoch 60/200, Loss: 0.3806\n",
      "Epoch 61/200, Loss: 0.3818, Validation Accuracy: 0.8391\n",
      "Epoch 62/200, Loss: 0.3812\n",
      "Epoch 63/200, Loss: 0.3807\n",
      "Epoch 64/200, Loss: 0.3807\n",
      "Epoch 65/200, Loss: 0.3810\n",
      "Epoch 66/200, Loss: 0.3806\n",
      "Epoch 67/200, Loss: 0.3809\n",
      "Epoch 68/200, Loss: 0.3806\n",
      "Epoch 69/200, Loss: 0.3805\n",
      "Epoch 70/200, Loss: 0.3800\n",
      "Epoch 71/200, Loss: 0.3806, Validation Accuracy: 0.8395\n",
      "Epoch 72/200, Loss: 0.3800\n",
      "Epoch 73/200, Loss: 0.3797\n",
      "Epoch 74/200, Loss: 0.3799\n",
      "Epoch 75/200, Loss: 0.3799\n",
      "Epoch 76/200, Loss: 0.3801\n",
      "Epoch 77/200, Loss: 0.3794\n",
      "Epoch 78/200, Loss: 0.3796\n",
      "Epoch 79/200, Loss: 0.3799\n",
      "Epoch 80/200, Loss: 0.3795\n",
      "Epoch 81/200, Loss: 0.3792, Validation Accuracy: 0.8407\n",
      "Epoch 82/200, Loss: 0.3793\n",
      "Epoch 83/200, Loss: 0.3789\n",
      "Epoch 84/200, Loss: 0.3787\n",
      "Epoch 85/200, Loss: 0.3790\n",
      "Epoch 86/200, Loss: 0.3789\n",
      "Epoch 87/200, Loss: 0.3787\n",
      "Epoch 88/200, Loss: 0.3783\n",
      "Epoch 89/200, Loss: 0.3793\n",
      "Epoch 90/200, Loss: 0.3790\n",
      "Epoch 91/200, Loss: 0.3787, Validation Accuracy: 0.8379\n",
      "Training stopped due to early stopping at epoch 90\n",
      "1 1 0.1\n",
      "Epoch 1/200, Loss: 0.6647, Validation Accuracy: 0.7836\n",
      "Epoch 2/200, Loss: 0.5391\n",
      "Epoch 3/200, Loss: 0.4678\n",
      "Epoch 4/200, Loss: 0.4447\n",
      "Epoch 5/200, Loss: 0.4357\n",
      "Epoch 6/200, Loss: 0.4323\n",
      "Epoch 7/200, Loss: 0.4277\n",
      "Epoch 8/200, Loss: 0.4262\n",
      "Epoch 9/200, Loss: 0.4220\n",
      "Epoch 10/200, Loss: 0.4205\n",
      "Epoch 11/200, Loss: 0.4182, Validation Accuracy: 0.8240\n",
      "Epoch 12/200, Loss: 0.4182\n",
      "Epoch 13/200, Loss: 0.4163\n",
      "Epoch 14/200, Loss: 0.4120\n",
      "Epoch 15/200, Loss: 0.4127\n",
      "Epoch 16/200, Loss: 0.4119\n",
      "Epoch 17/200, Loss: 0.4105\n",
      "Epoch 18/200, Loss: 0.4079\n",
      "Epoch 19/200, Loss: 0.4058\n",
      "Epoch 20/200, Loss: 0.4042\n",
      "Epoch 21/200, Loss: 0.4050, Validation Accuracy: 0.8267\n",
      "Epoch 22/200, Loss: 0.4046\n",
      "Epoch 23/200, Loss: 0.4048\n",
      "Epoch 24/200, Loss: 0.4024\n",
      "Epoch 25/200, Loss: 0.4027\n",
      "Epoch 26/200, Loss: 0.4039\n",
      "Epoch 27/200, Loss: 0.4015\n",
      "Epoch 28/200, Loss: 0.4023\n",
      "Epoch 29/200, Loss: 0.4004\n",
      "Epoch 30/200, Loss: 0.4005\n",
      "Epoch 31/200, Loss: 0.4006, Validation Accuracy: 0.8355\n",
      "Epoch 32/200, Loss: 0.4008\n",
      "Epoch 33/200, Loss: 0.3988\n",
      "Epoch 34/200, Loss: 0.4014\n",
      "Epoch 35/200, Loss: 0.4005\n",
      "Epoch 36/200, Loss: 0.4009\n",
      "Epoch 37/200, Loss: 0.4032\n",
      "Epoch 38/200, Loss: 0.3997\n",
      "Epoch 39/200, Loss: 0.4019\n",
      "Epoch 40/200, Loss: 0.3992\n",
      "Epoch 41/200, Loss: 0.4003, Validation Accuracy: 0.8275\n",
      "Epoch 42/200, Loss: 0.4002\n",
      "Epoch 43/200, Loss: 0.3991\n",
      "Epoch 44/200, Loss: 0.3986\n",
      "Epoch 45/200, Loss: 0.3994\n",
      "Epoch 46/200, Loss: 0.3981\n",
      "Epoch 47/200, Loss: 0.3984\n",
      "Epoch 48/200, Loss: 0.4007\n",
      "Epoch 49/200, Loss: 0.4008\n",
      "Epoch 50/200, Loss: 0.3979\n",
      "Epoch 51/200, Loss: 0.3981, Validation Accuracy: 0.8331\n",
      "Epoch 52/200, Loss: 0.3992\n",
      "Epoch 53/200, Loss: 0.3963\n",
      "Epoch 54/200, Loss: 0.3975\n",
      "Epoch 55/200, Loss: 0.3983\n",
      "Epoch 56/200, Loss: 0.3971\n",
      "Epoch 57/200, Loss: 0.3974\n",
      "Epoch 58/200, Loss: 0.3991\n",
      "Epoch 59/200, Loss: 0.3990\n",
      "Epoch 60/200, Loss: 0.3987\n",
      "Epoch 61/200, Loss: 0.3975, Validation Accuracy: 0.8351\n",
      "Epoch 62/200, Loss: 0.3974\n",
      "Epoch 63/200, Loss: 0.3984\n",
      "Epoch 64/200, Loss: 0.3949\n",
      "Epoch 65/200, Loss: 0.3954\n",
      "Epoch 66/200, Loss: 0.3948\n",
      "Epoch 67/200, Loss: 0.3975\n",
      "Epoch 68/200, Loss: 0.3982\n",
      "Epoch 69/200, Loss: 0.3953\n",
      "Epoch 70/200, Loss: 0.3951\n",
      "Epoch 71/200, Loss: 0.3964, Validation Accuracy: 0.8323\n",
      "Training stopped due to early stopping at epoch 70\n",
      "1 1 0.2\n",
      "Epoch 1/200, Loss: 0.6387, Validation Accuracy: 0.7936\n",
      "Epoch 2/200, Loss: 0.5356\n",
      "Epoch 3/200, Loss: 0.4696\n",
      "Epoch 4/200, Loss: 0.4488\n",
      "Epoch 5/200, Loss: 0.4387\n",
      "Epoch 6/200, Loss: 0.4345\n",
      "Epoch 7/200, Loss: 0.4302\n",
      "Epoch 8/200, Loss: 0.4254\n",
      "Epoch 9/200, Loss: 0.4242\n",
      "Epoch 10/200, Loss: 0.4230\n",
      "Epoch 11/200, Loss: 0.4206, Validation Accuracy: 0.8212\n",
      "Epoch 12/200, Loss: 0.4165\n",
      "Epoch 13/200, Loss: 0.4167\n",
      "Epoch 14/200, Loss: 0.4154\n",
      "Epoch 15/200, Loss: 0.4139\n",
      "Epoch 16/200, Loss: 0.4135\n",
      "Epoch 17/200, Loss: 0.4133\n",
      "Epoch 18/200, Loss: 0.4098\n",
      "Epoch 19/200, Loss: 0.4108\n",
      "Epoch 20/200, Loss: 0.4090\n",
      "Epoch 21/200, Loss: 0.4087, Validation Accuracy: 0.8279\n",
      "Epoch 22/200, Loss: 0.4069\n",
      "Epoch 23/200, Loss: 0.4077\n",
      "Epoch 24/200, Loss: 0.4027\n",
      "Epoch 25/200, Loss: 0.4066\n",
      "Epoch 26/200, Loss: 0.4041\n",
      "Epoch 27/200, Loss: 0.4044\n",
      "Epoch 28/200, Loss: 0.4059\n",
      "Epoch 29/200, Loss: 0.4039\n",
      "Epoch 30/200, Loss: 0.4023\n",
      "Epoch 31/200, Loss: 0.4039, Validation Accuracy: 0.8347\n",
      "Epoch 32/200, Loss: 0.4051\n",
      "Epoch 33/200, Loss: 0.4024\n",
      "Epoch 34/200, Loss: 0.4009\n",
      "Epoch 35/200, Loss: 0.4028\n",
      "Epoch 36/200, Loss: 0.3997\n",
      "Epoch 37/200, Loss: 0.4008\n",
      "Epoch 38/200, Loss: 0.4017\n",
      "Epoch 39/200, Loss: 0.4022\n",
      "Epoch 40/200, Loss: 0.4012\n",
      "Epoch 41/200, Loss: 0.3993, Validation Accuracy: 0.8367\n",
      "Epoch 42/200, Loss: 0.4005\n",
      "Epoch 43/200, Loss: 0.3999\n",
      "Epoch 44/200, Loss: 0.4015\n",
      "Epoch 45/200, Loss: 0.4043\n",
      "Epoch 46/200, Loss: 0.4009\n",
      "Epoch 47/200, Loss: 0.4012\n",
      "Epoch 48/200, Loss: 0.3994\n",
      "Epoch 49/200, Loss: 0.4002\n",
      "Epoch 50/200, Loss: 0.4020\n",
      "Epoch 51/200, Loss: 0.3982, Validation Accuracy: 0.8351\n",
      "Epoch 52/200, Loss: 0.3987\n",
      "Epoch 53/200, Loss: 0.4014\n",
      "Epoch 54/200, Loss: 0.3982\n",
      "Epoch 55/200, Loss: 0.3967\n",
      "Epoch 56/200, Loss: 0.3957\n",
      "Epoch 57/200, Loss: 0.3973\n",
      "Epoch 58/200, Loss: 0.4007\n",
      "Epoch 59/200, Loss: 0.3981\n",
      "Epoch 60/200, Loss: 0.3985\n",
      "Epoch 61/200, Loss: 0.3961, Validation Accuracy: 0.8383\n",
      "Epoch 62/200, Loss: 0.3997\n",
      "Epoch 63/200, Loss: 0.3980\n",
      "Epoch 64/200, Loss: 0.3943\n",
      "Epoch 65/200, Loss: 0.3969\n",
      "Epoch 66/200, Loss: 0.3953\n",
      "Epoch 67/200, Loss: 0.3988\n",
      "Epoch 68/200, Loss: 0.3979\n",
      "Epoch 69/200, Loss: 0.3976\n",
      "Epoch 70/200, Loss: 0.3982\n",
      "Epoch 71/200, Loss: 0.3971, Validation Accuracy: 0.8319\n",
      "Training stopped due to early stopping at epoch 70\n",
      "1 1 0.3\n",
      "Epoch 1/200, Loss: 0.6672, Validation Accuracy: 0.7281\n",
      "Epoch 2/200, Loss: 0.5420\n",
      "Epoch 3/200, Loss: 0.4623\n",
      "Epoch 4/200, Loss: 0.4491\n",
      "Epoch 5/200, Loss: 0.4381\n",
      "Epoch 6/200, Loss: 0.4380\n",
      "Epoch 7/200, Loss: 0.4314\n",
      "Epoch 8/200, Loss: 0.4272\n",
      "Epoch 9/200, Loss: 0.4248\n",
      "Epoch 10/200, Loss: 0.4238\n",
      "Epoch 11/200, Loss: 0.4211, Validation Accuracy: 0.8228\n",
      "Epoch 12/200, Loss: 0.4193\n",
      "Epoch 13/200, Loss: 0.4211\n",
      "Epoch 14/200, Loss: 0.4191\n",
      "Epoch 15/200, Loss: 0.4170\n",
      "Epoch 16/200, Loss: 0.4164\n",
      "Epoch 17/200, Loss: 0.4130\n",
      "Epoch 18/200, Loss: 0.4123\n",
      "Epoch 19/200, Loss: 0.4135\n",
      "Epoch 20/200, Loss: 0.4120\n",
      "Epoch 21/200, Loss: 0.4135, Validation Accuracy: 0.8299\n",
      "Epoch 22/200, Loss: 0.4124\n",
      "Epoch 23/200, Loss: 0.4097\n",
      "Epoch 24/200, Loss: 0.4121\n",
      "Epoch 25/200, Loss: 0.4128\n",
      "Epoch 26/200, Loss: 0.4122\n",
      "Epoch 27/200, Loss: 0.4113\n",
      "Epoch 28/200, Loss: 0.4112\n",
      "Epoch 29/200, Loss: 0.4117\n",
      "Epoch 30/200, Loss: 0.4125\n",
      "Epoch 31/200, Loss: 0.4120, Validation Accuracy: 0.8291\n",
      "Epoch 32/200, Loss: 0.4101\n",
      "Epoch 33/200, Loss: 0.4081\n",
      "Epoch 34/200, Loss: 0.4099\n",
      "Epoch 35/200, Loss: 0.4091\n",
      "Epoch 36/200, Loss: 0.4090\n",
      "Epoch 37/200, Loss: 0.4095\n",
      "Epoch 38/200, Loss: 0.4080\n",
      "Epoch 39/200, Loss: 0.4096\n",
      "Epoch 40/200, Loss: 0.4080\n",
      "Epoch 41/200, Loss: 0.4066, Validation Accuracy: 0.8303\n",
      "Epoch 42/200, Loss: 0.4070\n",
      "Epoch 43/200, Loss: 0.4102\n",
      "Epoch 44/200, Loss: 0.4090\n",
      "Epoch 45/200, Loss: 0.4054\n",
      "Epoch 46/200, Loss: 0.4061\n",
      "Epoch 47/200, Loss: 0.4099\n",
      "Epoch 48/200, Loss: 0.4079\n",
      "Epoch 49/200, Loss: 0.4066\n",
      "Epoch 50/200, Loss: 0.4041\n",
      "Epoch 51/200, Loss: 0.4071, Validation Accuracy: 0.8323\n",
      "Epoch 52/200, Loss: 0.4096\n",
      "Epoch 53/200, Loss: 0.4075\n",
      "Epoch 54/200, Loss: 0.4055\n",
      "Epoch 55/200, Loss: 0.4068\n",
      "Epoch 56/200, Loss: 0.4067\n",
      "Epoch 57/200, Loss: 0.4055\n",
      "Epoch 58/200, Loss: 0.4033\n",
      "Epoch 59/200, Loss: 0.4066\n",
      "Epoch 60/200, Loss: 0.4068\n",
      "Epoch 61/200, Loss: 0.4044, Validation Accuracy: 0.8359\n",
      "Epoch 62/200, Loss: 0.4066\n",
      "Epoch 63/200, Loss: 0.4029\n",
      "Epoch 64/200, Loss: 0.4075\n",
      "Epoch 65/200, Loss: 0.4091\n",
      "Epoch 66/200, Loss: 0.4062\n",
      "Epoch 67/200, Loss: 0.4063\n",
      "Epoch 68/200, Loss: 0.4062\n",
      "Epoch 69/200, Loss: 0.4049\n",
      "Epoch 70/200, Loss: 0.4059\n",
      "Epoch 71/200, Loss: 0.4070, Validation Accuracy: 0.8295\n",
      "Training stopped due to early stopping at epoch 70\n",
      "1 2 0\n",
      "Epoch 1/200, Loss: 0.5940, Validation Accuracy: 0.8204\n",
      "Epoch 2/200, Loss: 0.4326\n",
      "Epoch 3/200, Loss: 0.4197\n",
      "Epoch 4/200, Loss: 0.4120\n",
      "Epoch 5/200, Loss: 0.4072\n",
      "Epoch 6/200, Loss: 0.4023\n",
      "Epoch 7/200, Loss: 0.3984\n",
      "Epoch 8/200, Loss: 0.3945\n",
      "Epoch 9/200, Loss: 0.3921\n",
      "Epoch 10/200, Loss: 0.3901\n",
      "Epoch 11/200, Loss: 0.3898, Validation Accuracy: 0.8367\n",
      "Epoch 12/200, Loss: 0.3879\n",
      "Epoch 13/200, Loss: 0.3860\n",
      "Epoch 14/200, Loss: 0.3873\n",
      "Epoch 15/200, Loss: 0.3853\n",
      "Epoch 16/200, Loss: 0.3846\n",
      "Epoch 17/200, Loss: 0.3841\n",
      "Epoch 18/200, Loss: 0.3835\n",
      "Epoch 19/200, Loss: 0.3829\n",
      "Epoch 20/200, Loss: 0.3835\n",
      "Epoch 21/200, Loss: 0.3832, Validation Accuracy: 0.8419\n",
      "Epoch 22/200, Loss: 0.3820\n",
      "Epoch 23/200, Loss: 0.3812\n",
      "Epoch 24/200, Loss: 0.3809\n",
      "Epoch 25/200, Loss: 0.3821\n",
      "Epoch 26/200, Loss: 0.3807\n",
      "Epoch 27/200, Loss: 0.3806\n",
      "Epoch 28/200, Loss: 0.3822\n",
      "Epoch 29/200, Loss: 0.3801\n",
      "Epoch 30/200, Loss: 0.3807\n",
      "Epoch 31/200, Loss: 0.3804, Validation Accuracy: 0.8415\n",
      "Epoch 32/200, Loss: 0.3800\n",
      "Epoch 33/200, Loss: 0.3800\n",
      "Epoch 34/200, Loss: 0.3789\n",
      "Epoch 35/200, Loss: 0.3801\n",
      "Epoch 36/200, Loss: 0.3791\n",
      "Epoch 37/200, Loss: 0.3788\n",
      "Epoch 38/200, Loss: 0.3791\n",
      "Epoch 39/200, Loss: 0.3785\n",
      "Epoch 40/200, Loss: 0.3783\n",
      "Epoch 41/200, Loss: 0.3783, Validation Accuracy: 0.8415\n",
      "Epoch 42/200, Loss: 0.3784\n",
      "Epoch 43/200, Loss: 0.3782\n",
      "Epoch 44/200, Loss: 0.3788\n",
      "Epoch 45/200, Loss: 0.3773\n",
      "Epoch 46/200, Loss: 0.3780\n",
      "Epoch 47/200, Loss: 0.3774\n",
      "Epoch 48/200, Loss: 0.3771\n",
      "Epoch 49/200, Loss: 0.3773\n",
      "Epoch 50/200, Loss: 0.3762\n",
      "Epoch 51/200, Loss: 0.3768, Validation Accuracy: 0.8391\n",
      "Training stopped due to early stopping at epoch 50\n",
      "1 2 0.1\n",
      "Epoch 1/200, Loss: 0.6224, Validation Accuracy: 0.7880\n",
      "Epoch 2/200, Loss: 0.4497\n",
      "Epoch 3/200, Loss: 0.4242\n",
      "Epoch 4/200, Loss: 0.4175\n",
      "Epoch 5/200, Loss: 0.4142\n",
      "Epoch 6/200, Loss: 0.4090\n",
      "Epoch 7/200, Loss: 0.4075\n",
      "Epoch 8/200, Loss: 0.4064\n",
      "Epoch 9/200, Loss: 0.4040\n",
      "Epoch 10/200, Loss: 0.4020\n",
      "Epoch 11/200, Loss: 0.4020, Validation Accuracy: 0.8319\n",
      "Epoch 12/200, Loss: 0.4003\n",
      "Epoch 13/200, Loss: 0.4001\n",
      "Epoch 14/200, Loss: 0.4001\n",
      "Epoch 15/200, Loss: 0.3976\n",
      "Epoch 16/200, Loss: 0.3961\n",
      "Epoch 17/200, Loss: 0.3952\n",
      "Epoch 18/200, Loss: 0.3965\n",
      "Epoch 19/200, Loss: 0.3944\n",
      "Epoch 20/200, Loss: 0.3953\n",
      "Epoch 21/200, Loss: 0.3952, Validation Accuracy: 0.8303\n",
      "Epoch 22/200, Loss: 0.3949\n",
      "Epoch 23/200, Loss: 0.3923\n",
      "Epoch 24/200, Loss: 0.3924\n",
      "Epoch 25/200, Loss: 0.3931\n",
      "Epoch 26/200, Loss: 0.3902\n",
      "Epoch 27/200, Loss: 0.3915\n",
      "Epoch 28/200, Loss: 0.3908\n",
      "Epoch 29/200, Loss: 0.3903\n",
      "Epoch 30/200, Loss: 0.3889\n",
      "Epoch 31/200, Loss: 0.3902, Validation Accuracy: 0.8343\n",
      "Epoch 32/200, Loss: 0.3894\n",
      "Epoch 33/200, Loss: 0.3879\n",
      "Epoch 34/200, Loss: 0.3868\n",
      "Epoch 35/200, Loss: 0.3860\n",
      "Epoch 36/200, Loss: 0.3859\n",
      "Epoch 37/200, Loss: 0.3856\n",
      "Epoch 38/200, Loss: 0.3870\n",
      "Epoch 39/200, Loss: 0.3857\n",
      "Epoch 40/200, Loss: 0.3867\n",
      "Epoch 41/200, Loss: 0.3869, Validation Accuracy: 0.8371\n",
      "Epoch 42/200, Loss: 0.3849\n",
      "Epoch 43/200, Loss: 0.3853\n",
      "Epoch 44/200, Loss: 0.3846\n",
      "Epoch 45/200, Loss: 0.3851\n",
      "Epoch 46/200, Loss: 0.3856\n",
      "Epoch 47/200, Loss: 0.3824\n",
      "Epoch 48/200, Loss: 0.3841\n",
      "Epoch 49/200, Loss: 0.3821\n",
      "Epoch 50/200, Loss: 0.3835\n",
      "Epoch 51/200, Loss: 0.3826, Validation Accuracy: 0.8363\n",
      "Epoch 52/200, Loss: 0.3832\n",
      "Epoch 53/200, Loss: 0.3835\n",
      "Epoch 54/200, Loss: 0.3815\n",
      "Epoch 55/200, Loss: 0.3819\n",
      "Epoch 56/200, Loss: 0.3826\n",
      "Epoch 57/200, Loss: 0.3825\n",
      "Epoch 58/200, Loss: 0.3807\n",
      "Epoch 59/200, Loss: 0.3820\n",
      "Epoch 60/200, Loss: 0.3831\n",
      "Epoch 61/200, Loss: 0.3835, Validation Accuracy: 0.8415\n",
      "Epoch 62/200, Loss: 0.3826\n",
      "Epoch 63/200, Loss: 0.3803\n",
      "Epoch 64/200, Loss: 0.3815\n",
      "Epoch 65/200, Loss: 0.3824\n",
      "Epoch 66/200, Loss: 0.3800\n",
      "Epoch 67/200, Loss: 0.3800\n",
      "Epoch 68/200, Loss: 0.3811\n",
      "Epoch 69/200, Loss: 0.3819\n",
      "Epoch 70/200, Loss: 0.3816\n",
      "Epoch 71/200, Loss: 0.3807, Validation Accuracy: 0.8419\n",
      "Epoch 72/200, Loss: 0.3821\n",
      "Epoch 73/200, Loss: 0.3807\n",
      "Epoch 74/200, Loss: 0.3796\n",
      "Epoch 75/200, Loss: 0.3799\n",
      "Epoch 76/200, Loss: 0.3787\n",
      "Epoch 77/200, Loss: 0.3819\n",
      "Epoch 78/200, Loss: 0.3814\n",
      "Epoch 79/200, Loss: 0.3802\n",
      "Epoch 80/200, Loss: 0.3787\n",
      "Epoch 81/200, Loss: 0.3786, Validation Accuracy: 0.8395\n",
      "Training stopped due to early stopping at epoch 80\n",
      "1 2 0.2\n",
      "Epoch 1/200, Loss: 0.6263, Validation Accuracy: 0.7944\n",
      "Epoch 2/200, Loss: 0.4566\n",
      "Epoch 3/200, Loss: 0.4324\n",
      "Epoch 4/200, Loss: 0.4274\n",
      "Epoch 5/200, Loss: 0.4222\n",
      "Epoch 6/200, Loss: 0.4216\n",
      "Epoch 7/200, Loss: 0.4168\n",
      "Epoch 8/200, Loss: 0.4162\n",
      "Epoch 9/200, Loss: 0.4117\n",
      "Epoch 10/200, Loss: 0.4104\n",
      "Epoch 11/200, Loss: 0.4069, Validation Accuracy: 0.8248\n",
      "Epoch 12/200, Loss: 0.4076\n",
      "Epoch 13/200, Loss: 0.4070\n",
      "Epoch 14/200, Loss: 0.4048\n",
      "Epoch 15/200, Loss: 0.4026\n",
      "Epoch 16/200, Loss: 0.4004\n",
      "Epoch 17/200, Loss: 0.4023\n",
      "Epoch 18/200, Loss: 0.4012\n",
      "Epoch 19/200, Loss: 0.3985\n",
      "Epoch 20/200, Loss: 0.3980\n",
      "Epoch 21/200, Loss: 0.3977, Validation Accuracy: 0.8343\n",
      "Epoch 22/200, Loss: 0.3946\n",
      "Epoch 23/200, Loss: 0.3958\n",
      "Epoch 24/200, Loss: 0.3942\n",
      "Epoch 25/200, Loss: 0.3938\n",
      "Epoch 26/200, Loss: 0.3922\n",
      "Epoch 27/200, Loss: 0.3937\n",
      "Epoch 28/200, Loss: 0.3916\n",
      "Epoch 29/200, Loss: 0.3924\n",
      "Epoch 30/200, Loss: 0.3924\n",
      "Epoch 31/200, Loss: 0.3890, Validation Accuracy: 0.8327\n",
      "Epoch 32/200, Loss: 0.3911\n",
      "Epoch 33/200, Loss: 0.3920\n",
      "Epoch 34/200, Loss: 0.3935\n",
      "Epoch 35/200, Loss: 0.3878\n",
      "Epoch 36/200, Loss: 0.3880\n",
      "Epoch 37/200, Loss: 0.3906\n",
      "Epoch 38/200, Loss: 0.3865\n",
      "Epoch 39/200, Loss: 0.3875\n",
      "Epoch 40/200, Loss: 0.3902\n",
      "Epoch 41/200, Loss: 0.3881, Validation Accuracy: 0.8359\n",
      "Epoch 42/200, Loss: 0.3876\n",
      "Epoch 43/200, Loss: 0.3883\n",
      "Epoch 44/200, Loss: 0.3882\n",
      "Epoch 45/200, Loss: 0.3878\n",
      "Epoch 46/200, Loss: 0.3866\n",
      "Epoch 47/200, Loss: 0.3856\n",
      "Epoch 48/200, Loss: 0.3868\n",
      "Epoch 49/200, Loss: 0.3878\n",
      "Epoch 50/200, Loss: 0.3856\n",
      "Epoch 51/200, Loss: 0.3851, Validation Accuracy: 0.8379\n",
      "Epoch 52/200, Loss: 0.3877\n",
      "Epoch 53/200, Loss: 0.3870\n",
      "Epoch 54/200, Loss: 0.3856\n",
      "Epoch 55/200, Loss: 0.3868\n",
      "Epoch 56/200, Loss: 0.3827\n",
      "Epoch 57/200, Loss: 0.3854\n",
      "Epoch 58/200, Loss: 0.3862\n",
      "Epoch 59/200, Loss: 0.3843\n",
      "Epoch 60/200, Loss: 0.3856\n",
      "Epoch 61/200, Loss: 0.3842, Validation Accuracy: 0.8343\n",
      "Training stopped due to early stopping at epoch 60\n",
      "1 2 0.3\n",
      "Epoch 1/200, Loss: 0.6226, Validation Accuracy: 0.7733\n",
      "Epoch 2/200, Loss: 0.4703\n",
      "Epoch 3/200, Loss: 0.4470\n",
      "Epoch 4/200, Loss: 0.4360\n",
      "Epoch 5/200, Loss: 0.4314\n",
      "Epoch 6/200, Loss: 0.4257\n",
      "Epoch 7/200, Loss: 0.4221\n",
      "Epoch 8/200, Loss: 0.4224\n",
      "Epoch 9/200, Loss: 0.4205\n",
      "Epoch 10/200, Loss: 0.4154\n",
      "Epoch 11/200, Loss: 0.4135, Validation Accuracy: 0.8244\n",
      "Epoch 12/200, Loss: 0.4096\n",
      "Epoch 13/200, Loss: 0.4115\n",
      "Epoch 14/200, Loss: 0.4085\n",
      "Epoch 15/200, Loss: 0.4087\n",
      "Epoch 16/200, Loss: 0.4085\n",
      "Epoch 17/200, Loss: 0.4074\n",
      "Epoch 18/200, Loss: 0.4071\n",
      "Epoch 19/200, Loss: 0.4074\n",
      "Epoch 20/200, Loss: 0.4043\n",
      "Epoch 21/200, Loss: 0.4074, Validation Accuracy: 0.8323\n",
      "Epoch 22/200, Loss: 0.4057\n",
      "Epoch 23/200, Loss: 0.4061\n",
      "Epoch 24/200, Loss: 0.4029\n",
      "Epoch 25/200, Loss: 0.4012\n",
      "Epoch 26/200, Loss: 0.4001\n",
      "Epoch 27/200, Loss: 0.4022\n",
      "Epoch 28/200, Loss: 0.4004\n",
      "Epoch 29/200, Loss: 0.4023\n",
      "Epoch 30/200, Loss: 0.4003\n",
      "Epoch 31/200, Loss: 0.4000, Validation Accuracy: 0.8387\n",
      "Epoch 32/200, Loss: 0.3983\n",
      "Epoch 33/200, Loss: 0.3994\n",
      "Epoch 34/200, Loss: 0.3966\n",
      "Epoch 35/200, Loss: 0.3957\n",
      "Epoch 36/200, Loss: 0.3992\n",
      "Epoch 37/200, Loss: 0.4001\n",
      "Epoch 38/200, Loss: 0.3981\n",
      "Epoch 39/200, Loss: 0.3958\n",
      "Epoch 40/200, Loss: 0.3964\n",
      "Epoch 41/200, Loss: 0.3953, Validation Accuracy: 0.8359\n",
      "Epoch 42/200, Loss: 0.3996\n",
      "Epoch 43/200, Loss: 0.3985\n",
      "Epoch 44/200, Loss: 0.3948\n",
      "Epoch 45/200, Loss: 0.3939\n",
      "Epoch 46/200, Loss: 0.3939\n",
      "Epoch 47/200, Loss: 0.3959\n",
      "Epoch 48/200, Loss: 0.3962\n",
      "Epoch 49/200, Loss: 0.3957\n",
      "Epoch 50/200, Loss: 0.3947\n",
      "Epoch 51/200, Loss: 0.3957, Validation Accuracy: 0.8311\n",
      "Training stopped due to early stopping at epoch 50\n",
      "1 3 0\n",
      "Epoch 1/200, Loss: 0.5985, Validation Accuracy: 0.8196\n",
      "Epoch 2/200, Loss: 0.4194\n",
      "Epoch 3/200, Loss: 0.4095\n",
      "Epoch 4/200, Loss: 0.4019\n",
      "Epoch 5/200, Loss: 0.3964\n",
      "Epoch 6/200, Loss: 0.3889\n",
      "Epoch 7/200, Loss: 0.3892\n",
      "Epoch 8/200, Loss: 0.3850\n",
      "Epoch 9/200, Loss: 0.3837\n",
      "Epoch 10/200, Loss: 0.3831\n",
      "Epoch 11/200, Loss: 0.3831, Validation Accuracy: 0.8355\n",
      "Epoch 12/200, Loss: 0.3802\n",
      "Epoch 13/200, Loss: 0.3803\n",
      "Epoch 14/200, Loss: 0.3806\n",
      "Epoch 15/200, Loss: 0.3796\n",
      "Epoch 16/200, Loss: 0.3802\n",
      "Epoch 17/200, Loss: 0.3807\n",
      "Epoch 18/200, Loss: 0.3812\n",
      "Epoch 19/200, Loss: 0.3790\n",
      "Epoch 20/200, Loss: 0.3788\n",
      "Epoch 21/200, Loss: 0.3782, Validation Accuracy: 0.8387\n",
      "Epoch 22/200, Loss: 0.3798\n",
      "Epoch 23/200, Loss: 0.3785\n",
      "Epoch 24/200, Loss: 0.3784\n",
      "Epoch 25/200, Loss: 0.3791\n",
      "Epoch 26/200, Loss: 0.3776\n",
      "Epoch 27/200, Loss: 0.3770\n",
      "Epoch 28/200, Loss: 0.3779\n",
      "Epoch 29/200, Loss: 0.3763\n",
      "Epoch 30/200, Loss: 0.3764\n",
      "Epoch 31/200, Loss: 0.3773, Validation Accuracy: 0.8411\n",
      "Epoch 32/200, Loss: 0.3768\n",
      "Epoch 33/200, Loss: 0.3761\n",
      "Epoch 34/200, Loss: 0.3768\n",
      "Epoch 35/200, Loss: 0.3763\n",
      "Epoch 36/200, Loss: 0.3769\n",
      "Epoch 37/200, Loss: 0.3755\n",
      "Epoch 38/200, Loss: 0.3769\n",
      "Epoch 39/200, Loss: 0.3756\n",
      "Epoch 40/200, Loss: 0.3760\n",
      "Epoch 41/200, Loss: 0.3757, Validation Accuracy: 0.8411\n",
      "Epoch 42/200, Loss: 0.3749\n",
      "Epoch 43/200, Loss: 0.3765\n",
      "Epoch 44/200, Loss: 0.3749\n",
      "Epoch 45/200, Loss: 0.3755\n",
      "Epoch 46/200, Loss: 0.3782\n",
      "Epoch 47/200, Loss: 0.3764\n",
      "Epoch 48/200, Loss: 0.3746\n",
      "Epoch 49/200, Loss: 0.3751\n",
      "Epoch 50/200, Loss: 0.3750\n",
      "Epoch 51/200, Loss: 0.3749, Validation Accuracy: 0.8435\n",
      "Epoch 52/200, Loss: 0.3738\n",
      "Epoch 53/200, Loss: 0.3744\n",
      "Epoch 54/200, Loss: 0.3748\n",
      "Epoch 55/200, Loss: 0.3734\n",
      "Epoch 56/200, Loss: 0.3727\n",
      "Epoch 57/200, Loss: 0.3742\n",
      "Epoch 58/200, Loss: 0.3730\n",
      "Epoch 59/200, Loss: 0.3738\n",
      "Epoch 60/200, Loss: 0.3727\n",
      "Epoch 61/200, Loss: 0.3742, Validation Accuracy: 0.8439\n",
      "Epoch 62/200, Loss: 0.3732\n",
      "Epoch 63/200, Loss: 0.3725\n",
      "Epoch 64/200, Loss: 0.3725\n",
      "Epoch 65/200, Loss: 0.3722\n",
      "Epoch 66/200, Loss: 0.3730\n",
      "Epoch 67/200, Loss: 0.3728\n",
      "Epoch 68/200, Loss: 0.3724\n",
      "Epoch 69/200, Loss: 0.3723\n",
      "Epoch 70/200, Loss: 0.3719\n",
      "Epoch 71/200, Loss: 0.3716, Validation Accuracy: 0.8443\n",
      "Epoch 72/200, Loss: 0.3713\n",
      "Epoch 73/200, Loss: 0.3717\n",
      "Epoch 74/200, Loss: 0.3717\n",
      "Epoch 75/200, Loss: 0.3710\n",
      "Epoch 76/200, Loss: 0.3719\n",
      "Epoch 77/200, Loss: 0.3710\n",
      "Epoch 78/200, Loss: 0.3730\n",
      "Epoch 79/200, Loss: 0.3710\n",
      "Epoch 80/200, Loss: 0.3716\n",
      "Epoch 81/200, Loss: 0.3712, Validation Accuracy: 0.8475\n",
      "Epoch 82/200, Loss: 0.3721\n",
      "Epoch 83/200, Loss: 0.3709\n",
      "Epoch 84/200, Loss: 0.3708\n",
      "Epoch 85/200, Loss: 0.3704\n",
      "Epoch 86/200, Loss: 0.3705\n",
      "Epoch 87/200, Loss: 0.3710\n",
      "Epoch 88/200, Loss: 0.3706\n",
      "Epoch 89/200, Loss: 0.3703\n",
      "Epoch 90/200, Loss: 0.3700\n",
      "Epoch 91/200, Loss: 0.3698, Validation Accuracy: 0.8343\n",
      "Training stopped due to early stopping at epoch 90\n",
      "1 3 0.1\n",
      "Epoch 1/200, Loss: 0.6353, Validation Accuracy: 0.8048\n",
      "Epoch 2/200, Loss: 0.4474\n",
      "Epoch 3/200, Loss: 0.4285\n",
      "Epoch 4/200, Loss: 0.4206\n",
      "Epoch 5/200, Loss: 0.4161\n",
      "Epoch 6/200, Loss: 0.4116\n",
      "Epoch 7/200, Loss: 0.4061\n",
      "Epoch 8/200, Loss: 0.4068\n",
      "Epoch 9/200, Loss: 0.4063\n",
      "Epoch 10/200, Loss: 0.4002\n",
      "Epoch 11/200, Loss: 0.4016, Validation Accuracy: 0.8395\n",
      "Epoch 12/200, Loss: 0.3975\n",
      "Epoch 13/200, Loss: 0.3968\n",
      "Epoch 14/200, Loss: 0.3951\n",
      "Epoch 15/200, Loss: 0.3954\n",
      "Epoch 16/200, Loss: 0.3955\n",
      "Epoch 17/200, Loss: 0.3952\n",
      "Epoch 18/200, Loss: 0.3946\n",
      "Epoch 19/200, Loss: 0.3943\n",
      "Epoch 20/200, Loss: 0.3930\n",
      "Epoch 21/200, Loss: 0.3919, Validation Accuracy: 0.8307\n",
      "Epoch 22/200, Loss: 0.3909\n",
      "Epoch 23/200, Loss: 0.3898\n",
      "Epoch 24/200, Loss: 0.3904\n",
      "Epoch 25/200, Loss: 0.3908\n",
      "Epoch 26/200, Loss: 0.3898\n",
      "Epoch 27/200, Loss: 0.3874\n",
      "Epoch 28/200, Loss: 0.3898\n",
      "Epoch 29/200, Loss: 0.3862\n",
      "Epoch 30/200, Loss: 0.3888\n",
      "Epoch 31/200, Loss: 0.3870, Validation Accuracy: 0.8391\n",
      "Epoch 32/200, Loss: 0.3878\n",
      "Epoch 33/200, Loss: 0.3871\n",
      "Epoch 34/200, Loss: 0.3845\n",
      "Epoch 35/200, Loss: 0.3849\n",
      "Epoch 36/200, Loss: 0.3849\n",
      "Epoch 37/200, Loss: 0.3851\n",
      "Epoch 38/200, Loss: 0.3837\n",
      "Epoch 39/200, Loss: 0.3839\n",
      "Epoch 40/200, Loss: 0.3823\n",
      "Epoch 41/200, Loss: 0.3827, Validation Accuracy: 0.8423\n",
      "Epoch 42/200, Loss: 0.3826\n",
      "Epoch 43/200, Loss: 0.3820\n",
      "Epoch 44/200, Loss: 0.3815\n",
      "Epoch 45/200, Loss: 0.3799\n",
      "Epoch 46/200, Loss: 0.3831\n",
      "Epoch 47/200, Loss: 0.3828\n",
      "Epoch 48/200, Loss: 0.3805\n",
      "Epoch 49/200, Loss: 0.3821\n",
      "Epoch 50/200, Loss: 0.3819\n",
      "Epoch 51/200, Loss: 0.3820, Validation Accuracy: 0.8435\n",
      "Epoch 52/200, Loss: 0.3813\n",
      "Epoch 53/200, Loss: 0.3814\n",
      "Epoch 54/200, Loss: 0.3807\n",
      "Epoch 55/200, Loss: 0.3812\n",
      "Epoch 56/200, Loss: 0.3803\n",
      "Epoch 57/200, Loss: 0.3803\n",
      "Epoch 58/200, Loss: 0.3820\n",
      "Epoch 59/200, Loss: 0.3811\n",
      "Epoch 60/200, Loss: 0.3826\n",
      "Epoch 61/200, Loss: 0.3794, Validation Accuracy: 0.8427\n",
      "Epoch 62/200, Loss: 0.3807\n",
      "Epoch 63/200, Loss: 0.3788\n",
      "Epoch 64/200, Loss: 0.3810\n",
      "Epoch 65/200, Loss: 0.3800\n",
      "Epoch 66/200, Loss: 0.3820\n",
      "Epoch 67/200, Loss: 0.3806\n",
      "Epoch 68/200, Loss: 0.3786\n",
      "Epoch 69/200, Loss: 0.3785\n",
      "Epoch 70/200, Loss: 0.3802\n",
      "Epoch 71/200, Loss: 0.3804, Validation Accuracy: 0.8435\n",
      "Epoch 72/200, Loss: 0.3784\n",
      "Epoch 73/200, Loss: 0.3802\n",
      "Epoch 74/200, Loss: 0.3803\n",
      "Epoch 75/200, Loss: 0.3807\n",
      "Epoch 76/200, Loss: 0.3808\n",
      "Epoch 77/200, Loss: 0.3802\n",
      "Epoch 78/200, Loss: 0.3799\n",
      "Epoch 79/200, Loss: 0.3805\n",
      "Epoch 80/200, Loss: 0.3764\n",
      "Epoch 81/200, Loss: 0.3793, Validation Accuracy: 0.8463\n",
      "Epoch 82/200, Loss: 0.3784\n",
      "Epoch 83/200, Loss: 0.3806\n",
      "Epoch 84/200, Loss: 0.3776\n",
      "Epoch 85/200, Loss: 0.3773\n",
      "Epoch 86/200, Loss: 0.3786\n",
      "Epoch 87/200, Loss: 0.3796\n",
      "Epoch 88/200, Loss: 0.3790\n",
      "Epoch 89/200, Loss: 0.3784\n",
      "Epoch 90/200, Loss: 0.3781\n",
      "Epoch 91/200, Loss: 0.3799, Validation Accuracy: 0.8411\n",
      "Training stopped due to early stopping at epoch 90\n",
      "1 3 0.2\n",
      "Epoch 1/200, Loss: 0.5881, Validation Accuracy: 0.8104\n",
      "Epoch 2/200, Loss: 0.4342\n",
      "Epoch 3/200, Loss: 0.4231\n",
      "Epoch 4/200, Loss: 0.4154\n",
      "Epoch 5/200, Loss: 0.4160\n",
      "Epoch 6/200, Loss: 0.4092\n",
      "Epoch 7/200, Loss: 0.4090\n",
      "Epoch 8/200, Loss: 0.4041\n",
      "Epoch 9/200, Loss: 0.4017\n",
      "Epoch 10/200, Loss: 0.3999\n",
      "Epoch 11/200, Loss: 0.3984, Validation Accuracy: 0.8351\n",
      "Epoch 12/200, Loss: 0.3982\n",
      "Epoch 13/200, Loss: 0.3985\n",
      "Epoch 14/200, Loss: 0.3940\n",
      "Epoch 15/200, Loss: 0.3981\n",
      "Epoch 16/200, Loss: 0.3957\n",
      "Epoch 17/200, Loss: 0.3956\n",
      "Epoch 18/200, Loss: 0.3940\n",
      "Epoch 19/200, Loss: 0.3924\n",
      "Epoch 20/200, Loss: 0.3929\n",
      "Epoch 21/200, Loss: 0.3892, Validation Accuracy: 0.8367\n",
      "Epoch 22/200, Loss: 0.3936\n",
      "Epoch 23/200, Loss: 0.3919\n",
      "Epoch 24/200, Loss: 0.3901\n",
      "Epoch 25/200, Loss: 0.3898\n",
      "Epoch 26/200, Loss: 0.3878\n",
      "Epoch 27/200, Loss: 0.3913\n",
      "Epoch 28/200, Loss: 0.3880\n",
      "Epoch 29/200, Loss: 0.3880\n",
      "Epoch 30/200, Loss: 0.3882\n",
      "Epoch 31/200, Loss: 0.3858, Validation Accuracy: 0.8347\n",
      "Training stopped due to early stopping at epoch 30\n",
      "1 3 0.3\n",
      "Epoch 1/200, Loss: 0.6151, Validation Accuracy: 0.7764\n",
      "Epoch 2/200, Loss: 0.4473\n",
      "Epoch 3/200, Loss: 0.4310\n",
      "Epoch 4/200, Loss: 0.4270\n",
      "Epoch 5/200, Loss: 0.4245\n",
      "Epoch 6/200, Loss: 0.4208\n",
      "Epoch 7/200, Loss: 0.4168\n",
      "Epoch 8/200, Loss: 0.4144\n",
      "Epoch 9/200, Loss: 0.4119\n",
      "Epoch 10/200, Loss: 0.4080\n",
      "Epoch 11/200, Loss: 0.4107, Validation Accuracy: 0.8259\n",
      "Epoch 12/200, Loss: 0.4063\n",
      "Epoch 13/200, Loss: 0.4042\n",
      "Epoch 14/200, Loss: 0.4062\n",
      "Epoch 15/200, Loss: 0.4079\n",
      "Epoch 16/200, Loss: 0.4038\n",
      "Epoch 17/200, Loss: 0.4025\n",
      "Epoch 18/200, Loss: 0.3998\n",
      "Epoch 19/200, Loss: 0.4018\n",
      "Epoch 20/200, Loss: 0.4023\n",
      "Epoch 21/200, Loss: 0.4017, Validation Accuracy: 0.8259\n",
      "Epoch 22/200, Loss: 0.3977\n",
      "Epoch 23/200, Loss: 0.4002\n",
      "Epoch 24/200, Loss: 0.3973\n",
      "Epoch 25/200, Loss: 0.3955\n",
      "Epoch 26/200, Loss: 0.3935\n",
      "Epoch 27/200, Loss: 0.3956\n",
      "Epoch 28/200, Loss: 0.3945\n",
      "Epoch 29/200, Loss: 0.3953\n",
      "Epoch 30/200, Loss: 0.3916\n",
      "Epoch 31/200, Loss: 0.3917, Validation Accuracy: 0.8367\n",
      "Epoch 32/200, Loss: 0.3889\n",
      "Epoch 33/200, Loss: 0.3893\n",
      "Epoch 34/200, Loss: 0.3874\n",
      "Epoch 35/200, Loss: 0.3918\n",
      "Epoch 36/200, Loss: 0.3897\n",
      "Epoch 37/200, Loss: 0.3866\n",
      "Epoch 38/200, Loss: 0.3862\n",
      "Epoch 39/200, Loss: 0.3878\n",
      "Epoch 40/200, Loss: 0.3862\n",
      "Epoch 41/200, Loss: 0.3861, Validation Accuracy: 0.8319\n",
      "Epoch 42/200, Loss: 0.3886\n",
      "Epoch 43/200, Loss: 0.3875\n",
      "Epoch 44/200, Loss: 0.3880\n",
      "Epoch 45/200, Loss: 0.3859\n",
      "Epoch 46/200, Loss: 0.3879\n",
      "Epoch 47/200, Loss: 0.3854\n",
      "Epoch 48/200, Loss: 0.3862\n",
      "Epoch 49/200, Loss: 0.3876\n",
      "Epoch 50/200, Loss: 0.3866\n",
      "Epoch 51/200, Loss: 0.3861, Validation Accuracy: 0.8399\n",
      "Epoch 52/200, Loss: 0.3829\n",
      "Epoch 53/200, Loss: 0.3847\n",
      "Epoch 54/200, Loss: 0.3850\n",
      "Epoch 55/200, Loss: 0.3860\n",
      "Epoch 56/200, Loss: 0.3855\n",
      "Epoch 57/200, Loss: 0.3857\n",
      "Epoch 58/200, Loss: 0.3820\n",
      "Epoch 59/200, Loss: 0.3866\n",
      "Epoch 60/200, Loss: 0.3836\n",
      "Epoch 61/200, Loss: 0.3834, Validation Accuracy: 0.8375\n",
      "Epoch 62/200, Loss: 0.3840\n",
      "Epoch 63/200, Loss: 0.3845\n",
      "Epoch 64/200, Loss: 0.3850\n",
      "Epoch 65/200, Loss: 0.3834\n",
      "Epoch 66/200, Loss: 0.3841\n",
      "Epoch 67/200, Loss: 0.3844\n",
      "Epoch 68/200, Loss: 0.3848\n",
      "Epoch 69/200, Loss: 0.3840\n",
      "Epoch 70/200, Loss: 0.3840\n",
      "Epoch 71/200, Loss: 0.3820, Validation Accuracy: 0.8351\n",
      "Training stopped due to early stopping at epoch 70\n",
      "2 1 0\n",
      "Epoch 1/200, Loss: 0.6185, Validation Accuracy: 0.8132\n",
      "Epoch 2/200, Loss: 0.4359\n",
      "Epoch 3/200, Loss: 0.4153\n",
      "Epoch 4/200, Loss: 0.4052\n",
      "Epoch 5/200, Loss: 0.3978\n",
      "Epoch 6/200, Loss: 0.3926\n",
      "Epoch 7/200, Loss: 0.3898\n",
      "Epoch 8/200, Loss: 0.3869\n",
      "Epoch 9/200, Loss: 0.3855\n",
      "Epoch 10/200, Loss: 0.3844\n",
      "Epoch 11/200, Loss: 0.3832, Validation Accuracy: 0.8363\n",
      "Epoch 12/200, Loss: 0.3824\n",
      "Epoch 13/200, Loss: 0.3819\n",
      "Epoch 14/200, Loss: 0.3826\n",
      "Epoch 15/200, Loss: 0.3814\n",
      "Epoch 16/200, Loss: 0.3806\n",
      "Epoch 17/200, Loss: 0.3808\n",
      "Epoch 18/200, Loss: 0.3814\n",
      "Epoch 19/200, Loss: 0.3807\n",
      "Epoch 20/200, Loss: 0.3798\n",
      "Epoch 21/200, Loss: 0.3793, Validation Accuracy: 0.8387\n",
      "Epoch 22/200, Loss: 0.3790\n",
      "Epoch 23/200, Loss: 0.3790\n",
      "Epoch 24/200, Loss: 0.3802\n",
      "Epoch 25/200, Loss: 0.3783\n",
      "Epoch 26/200, Loss: 0.3776\n",
      "Epoch 27/200, Loss: 0.3793\n",
      "Epoch 28/200, Loss: 0.3780\n",
      "Epoch 29/200, Loss: 0.3777\n",
      "Epoch 30/200, Loss: 0.3784\n",
      "Epoch 31/200, Loss: 0.3767, Validation Accuracy: 0.8383\n",
      "Epoch 32/200, Loss: 0.3773\n",
      "Epoch 33/200, Loss: 0.3779\n",
      "Epoch 34/200, Loss: 0.3770\n",
      "Epoch 35/200, Loss: 0.3763\n",
      "Epoch 36/200, Loss: 0.3761\n",
      "Epoch 37/200, Loss: 0.3763\n",
      "Epoch 38/200, Loss: 0.3755\n",
      "Epoch 39/200, Loss: 0.3764\n",
      "Epoch 40/200, Loss: 0.3763\n",
      "Epoch 41/200, Loss: 0.3772, Validation Accuracy: 0.8415\n",
      "Epoch 42/200, Loss: 0.3759\n",
      "Epoch 43/200, Loss: 0.3752\n",
      "Epoch 44/200, Loss: 0.3756\n",
      "Epoch 45/200, Loss: 0.3747\n",
      "Epoch 46/200, Loss: 0.3754\n",
      "Epoch 47/200, Loss: 0.3765\n",
      "Epoch 48/200, Loss: 0.3762\n",
      "Epoch 49/200, Loss: 0.3743\n",
      "Epoch 50/200, Loss: 0.3757\n",
      "Epoch 51/200, Loss: 0.3749, Validation Accuracy: 0.8403\n",
      "Epoch 52/200, Loss: 0.3748\n",
      "Epoch 53/200, Loss: 0.3745\n",
      "Epoch 54/200, Loss: 0.3747\n",
      "Epoch 55/200, Loss: 0.3740\n",
      "Epoch 56/200, Loss: 0.3741\n",
      "Epoch 57/200, Loss: 0.3747\n",
      "Epoch 58/200, Loss: 0.3739\n",
      "Epoch 59/200, Loss: 0.3749\n",
      "Epoch 60/200, Loss: 0.3736\n",
      "Epoch 61/200, Loss: 0.3736, Validation Accuracy: 0.8399\n",
      "Training stopped due to early stopping at epoch 60\n",
      "2 1 0.1\n",
      "Epoch 1/200, Loss: 0.5697, Validation Accuracy: 0.8204\n",
      "Epoch 2/200, Loss: 0.4197\n",
      "Epoch 3/200, Loss: 0.4097\n",
      "Epoch 4/200, Loss: 0.4037\n",
      "Epoch 5/200, Loss: 0.4001\n",
      "Epoch 6/200, Loss: 0.3984\n",
      "Epoch 7/200, Loss: 0.3946\n",
      "Epoch 8/200, Loss: 0.3952\n",
      "Epoch 9/200, Loss: 0.3922\n",
      "Epoch 10/200, Loss: 0.3941\n",
      "Epoch 11/200, Loss: 0.3912, Validation Accuracy: 0.8347\n",
      "Epoch 12/200, Loss: 0.3897\n",
      "Epoch 13/200, Loss: 0.3900\n",
      "Epoch 14/200, Loss: 0.3886\n",
      "Epoch 15/200, Loss: 0.3886\n",
      "Epoch 16/200, Loss: 0.3881\n",
      "Epoch 17/200, Loss: 0.3855\n",
      "Epoch 18/200, Loss: 0.3881\n",
      "Epoch 19/200, Loss: 0.3854\n",
      "Epoch 20/200, Loss: 0.3866\n",
      "Epoch 21/200, Loss: 0.3848, Validation Accuracy: 0.8367\n",
      "Epoch 22/200, Loss: 0.3835\n",
      "Epoch 23/200, Loss: 0.3849\n",
      "Epoch 24/200, Loss: 0.3845\n",
      "Epoch 25/200, Loss: 0.3825\n",
      "Epoch 26/200, Loss: 0.3818\n",
      "Epoch 27/200, Loss: 0.3824\n",
      "Epoch 28/200, Loss: 0.3826\n",
      "Epoch 29/200, Loss: 0.3826\n",
      "Epoch 30/200, Loss: 0.3823\n",
      "Epoch 31/200, Loss: 0.3806, Validation Accuracy: 0.8355\n",
      "Epoch 32/200, Loss: 0.3809\n",
      "Epoch 33/200, Loss: 0.3793\n",
      "Epoch 34/200, Loss: 0.3809\n",
      "Epoch 35/200, Loss: 0.3797\n",
      "Epoch 36/200, Loss: 0.3789\n",
      "Epoch 37/200, Loss: 0.3799\n",
      "Epoch 38/200, Loss: 0.3796\n",
      "Epoch 39/200, Loss: 0.3777\n",
      "Epoch 40/200, Loss: 0.3789\n",
      "Epoch 41/200, Loss: 0.3781, Validation Accuracy: 0.8379\n",
      "Epoch 42/200, Loss: 0.3791\n",
      "Epoch 43/200, Loss: 0.3787\n",
      "Epoch 44/200, Loss: 0.3784\n",
      "Epoch 45/200, Loss: 0.3777\n",
      "Epoch 46/200, Loss: 0.3782\n",
      "Epoch 47/200, Loss: 0.3770\n",
      "Epoch 48/200, Loss: 0.3782\n",
      "Epoch 49/200, Loss: 0.3780\n",
      "Epoch 50/200, Loss: 0.3767\n",
      "Epoch 51/200, Loss: 0.3767, Validation Accuracy: 0.8427\n",
      "Epoch 52/200, Loss: 0.3770\n",
      "Epoch 53/200, Loss: 0.3771\n",
      "Epoch 54/200, Loss: 0.3776\n",
      "Epoch 55/200, Loss: 0.3756\n",
      "Epoch 56/200, Loss: 0.3763\n",
      "Epoch 57/200, Loss: 0.3766\n",
      "Epoch 58/200, Loss: 0.3768\n",
      "Epoch 59/200, Loss: 0.3766\n",
      "Epoch 60/200, Loss: 0.3774\n",
      "Epoch 61/200, Loss: 0.3763, Validation Accuracy: 0.8395\n",
      "Epoch 62/200, Loss: 0.3751\n",
      "Epoch 63/200, Loss: 0.3779\n",
      "Epoch 64/200, Loss: 0.3757\n",
      "Epoch 65/200, Loss: 0.3776\n",
      "Epoch 66/200, Loss: 0.3760\n",
      "Epoch 67/200, Loss: 0.3756\n",
      "Epoch 68/200, Loss: 0.3762\n",
      "Epoch 69/200, Loss: 0.3778\n",
      "Epoch 70/200, Loss: 0.3755\n",
      "Epoch 71/200, Loss: 0.3742, Validation Accuracy: 0.8459\n",
      "Epoch 72/200, Loss: 0.3757\n",
      "Epoch 73/200, Loss: 0.3760\n",
      "Epoch 74/200, Loss: 0.3757\n",
      "Epoch 75/200, Loss: 0.3744\n",
      "Epoch 76/200, Loss: 0.3751\n",
      "Epoch 77/200, Loss: 0.3746\n",
      "Epoch 78/200, Loss: 0.3754\n",
      "Epoch 79/200, Loss: 0.3753\n",
      "Epoch 80/200, Loss: 0.3741\n",
      "Epoch 81/200, Loss: 0.3754, Validation Accuracy: 0.8467\n",
      "Epoch 82/200, Loss: 0.3749\n",
      "Epoch 83/200, Loss: 0.3725\n",
      "Epoch 84/200, Loss: 0.3744\n",
      "Epoch 85/200, Loss: 0.3753\n",
      "Epoch 86/200, Loss: 0.3726\n",
      "Epoch 87/200, Loss: 0.3750\n",
      "Epoch 88/200, Loss: 0.3736\n",
      "Epoch 89/200, Loss: 0.3737\n",
      "Epoch 90/200, Loss: 0.3737\n",
      "Epoch 91/200, Loss: 0.3751, Validation Accuracy: 0.8463\n",
      "Epoch 92/200, Loss: 0.3738\n",
      "Epoch 93/200, Loss: 0.3739\n",
      "Epoch 94/200, Loss: 0.3742\n",
      "Epoch 95/200, Loss: 0.3712\n",
      "Epoch 96/200, Loss: 0.3733\n",
      "Epoch 97/200, Loss: 0.3728\n",
      "Epoch 98/200, Loss: 0.3737\n",
      "Epoch 99/200, Loss: 0.3735\n",
      "Epoch 100/200, Loss: 0.3740\n",
      "Epoch 101/200, Loss: 0.3714, Validation Accuracy: 0.8447\n",
      "Training stopped due to early stopping at epoch 100\n",
      "2 1 0.2\n",
      "Epoch 1/200, Loss: 0.6282, Validation Accuracy: 0.8012\n",
      "Epoch 2/200, Loss: 0.4457\n",
      "Epoch 3/200, Loss: 0.4227\n",
      "Epoch 4/200, Loss: 0.4175\n",
      "Epoch 5/200, Loss: 0.4148\n",
      "Epoch 6/200, Loss: 0.4091\n",
      "Epoch 7/200, Loss: 0.4077\n",
      "Epoch 8/200, Loss: 0.4046\n",
      "Epoch 9/200, Loss: 0.4031\n",
      "Epoch 10/200, Loss: 0.4012\n",
      "Epoch 11/200, Loss: 0.4009, Validation Accuracy: 0.8311\n",
      "Epoch 12/200, Loss: 0.3986\n",
      "Epoch 13/200, Loss: 0.3981\n",
      "Epoch 14/200, Loss: 0.3964\n",
      "Epoch 15/200, Loss: 0.3977\n",
      "Epoch 16/200, Loss: 0.3963\n",
      "Epoch 17/200, Loss: 0.3961\n",
      "Epoch 18/200, Loss: 0.3945\n",
      "Epoch 19/200, Loss: 0.3954\n",
      "Epoch 20/200, Loss: 0.3933\n",
      "Epoch 21/200, Loss: 0.3923, Validation Accuracy: 0.8319\n",
      "Epoch 22/200, Loss: 0.3910\n",
      "Epoch 23/200, Loss: 0.3916\n",
      "Epoch 24/200, Loss: 0.3886\n",
      "Epoch 25/200, Loss: 0.3891\n",
      "Epoch 26/200, Loss: 0.3886\n",
      "Epoch 27/200, Loss: 0.3906\n",
      "Epoch 28/200, Loss: 0.3882\n",
      "Epoch 29/200, Loss: 0.3891\n",
      "Epoch 30/200, Loss: 0.3884\n",
      "Epoch 31/200, Loss: 0.3876, Validation Accuracy: 0.8323\n",
      "Epoch 32/200, Loss: 0.3888\n",
      "Epoch 33/200, Loss: 0.3872\n",
      "Epoch 34/200, Loss: 0.3887\n",
      "Epoch 35/200, Loss: 0.3876\n",
      "Epoch 36/200, Loss: 0.3872\n",
      "Epoch 37/200, Loss: 0.3856\n",
      "Epoch 38/200, Loss: 0.3868\n",
      "Epoch 39/200, Loss: 0.3862\n",
      "Epoch 40/200, Loss: 0.3861\n",
      "Epoch 41/200, Loss: 0.3860, Validation Accuracy: 0.8339\n",
      "Epoch 42/200, Loss: 0.3859\n",
      "Epoch 43/200, Loss: 0.3846\n",
      "Epoch 44/200, Loss: 0.3841\n",
      "Epoch 45/200, Loss: 0.3823\n",
      "Epoch 46/200, Loss: 0.3853\n",
      "Epoch 47/200, Loss: 0.3841\n",
      "Epoch 48/200, Loss: 0.3843\n",
      "Epoch 49/200, Loss: 0.3837\n",
      "Epoch 50/200, Loss: 0.3842\n",
      "Epoch 51/200, Loss: 0.3834, Validation Accuracy: 0.8375\n",
      "Epoch 52/200, Loss: 0.3841\n",
      "Epoch 53/200, Loss: 0.3835\n",
      "Epoch 54/200, Loss: 0.3816\n",
      "Epoch 55/200, Loss: 0.3807\n",
      "Epoch 56/200, Loss: 0.3855\n",
      "Epoch 57/200, Loss: 0.3827\n",
      "Epoch 58/200, Loss: 0.3824\n",
      "Epoch 59/200, Loss: 0.3818\n",
      "Epoch 60/200, Loss: 0.3820\n",
      "Epoch 61/200, Loss: 0.3827, Validation Accuracy: 0.8383\n",
      "Epoch 62/200, Loss: 0.3829\n",
      "Epoch 63/200, Loss: 0.3826\n",
      "Epoch 64/200, Loss: 0.3810\n",
      "Epoch 65/200, Loss: 0.3828\n",
      "Epoch 66/200, Loss: 0.3839\n",
      "Epoch 67/200, Loss: 0.3812\n",
      "Epoch 68/200, Loss: 0.3810\n",
      "Epoch 69/200, Loss: 0.3838\n",
      "Epoch 70/200, Loss: 0.3819\n",
      "Epoch 71/200, Loss: 0.3811, Validation Accuracy: 0.8395\n",
      "Epoch 72/200, Loss: 0.3809\n",
      "Epoch 73/200, Loss: 0.3824\n",
      "Epoch 74/200, Loss: 0.3820\n",
      "Epoch 75/200, Loss: 0.3806\n",
      "Epoch 76/200, Loss: 0.3808\n",
      "Epoch 77/200, Loss: 0.3820\n",
      "Epoch 78/200, Loss: 0.3830\n",
      "Epoch 79/200, Loss: 0.3805\n",
      "Epoch 80/200, Loss: 0.3822\n",
      "Epoch 81/200, Loss: 0.3810, Validation Accuracy: 0.8375\n",
      "Training stopped due to early stopping at epoch 80\n",
      "2 1 0.3\n",
      "Epoch 1/200, Loss: 0.5904, Validation Accuracy: 0.8000\n",
      "Epoch 2/200, Loss: 0.4349\n",
      "Epoch 3/200, Loss: 0.4210\n",
      "Epoch 4/200, Loss: 0.4176\n",
      "Epoch 5/200, Loss: 0.4138\n",
      "Epoch 6/200, Loss: 0.4101\n",
      "Epoch 7/200, Loss: 0.4087\n",
      "Epoch 8/200, Loss: 0.4073\n",
      "Epoch 9/200, Loss: 0.4055\n",
      "Epoch 10/200, Loss: 0.4041\n",
      "Epoch 11/200, Loss: 0.4024, Validation Accuracy: 0.8363\n",
      "Epoch 12/200, Loss: 0.4017\n",
      "Epoch 13/200, Loss: 0.4007\n",
      "Epoch 14/200, Loss: 0.3987\n",
      "Epoch 15/200, Loss: 0.4009\n",
      "Epoch 16/200, Loss: 0.3970\n",
      "Epoch 17/200, Loss: 0.3952\n",
      "Epoch 18/200, Loss: 0.3960\n",
      "Epoch 19/200, Loss: 0.3952\n",
      "Epoch 20/200, Loss: 0.3927\n",
      "Epoch 21/200, Loss: 0.3957, Validation Accuracy: 0.8343\n",
      "Epoch 22/200, Loss: 0.3953\n",
      "Epoch 23/200, Loss: 0.3947\n",
      "Epoch 24/200, Loss: 0.3920\n",
      "Epoch 25/200, Loss: 0.3917\n",
      "Epoch 26/200, Loss: 0.3928\n",
      "Epoch 27/200, Loss: 0.3913\n",
      "Epoch 28/200, Loss: 0.3931\n",
      "Epoch 29/200, Loss: 0.3898\n",
      "Epoch 30/200, Loss: 0.3932\n",
      "Epoch 31/200, Loss: 0.3905, Validation Accuracy: 0.8379\n",
      "Epoch 32/200, Loss: 0.3888\n",
      "Epoch 33/200, Loss: 0.3898\n",
      "Epoch 34/200, Loss: 0.3904\n",
      "Epoch 35/200, Loss: 0.3888\n",
      "Epoch 36/200, Loss: 0.3901\n",
      "Epoch 37/200, Loss: 0.3906\n",
      "Epoch 38/200, Loss: 0.3915\n",
      "Epoch 39/200, Loss: 0.3881\n",
      "Epoch 40/200, Loss: 0.3880\n",
      "Epoch 41/200, Loss: 0.3861, Validation Accuracy: 0.8411\n",
      "Epoch 42/200, Loss: 0.3901\n",
      "Epoch 43/200, Loss: 0.3865\n",
      "Epoch 44/200, Loss: 0.3891\n",
      "Epoch 45/200, Loss: 0.3867\n",
      "Epoch 46/200, Loss: 0.3883\n",
      "Epoch 47/200, Loss: 0.3885\n",
      "Epoch 48/200, Loss: 0.3878\n",
      "Epoch 49/200, Loss: 0.3854\n",
      "Epoch 50/200, Loss: 0.3857\n",
      "Epoch 51/200, Loss: 0.3878, Validation Accuracy: 0.8403\n",
      "Epoch 52/200, Loss: 0.3867\n",
      "Epoch 53/200, Loss: 0.3860\n",
      "Epoch 54/200, Loss: 0.3857\n",
      "Epoch 55/200, Loss: 0.3864\n",
      "Epoch 56/200, Loss: 0.3880\n",
      "Epoch 57/200, Loss: 0.3848\n",
      "Epoch 58/200, Loss: 0.3894\n",
      "Epoch 59/200, Loss: 0.3866\n",
      "Epoch 60/200, Loss: 0.3860\n",
      "Epoch 61/200, Loss: 0.3866, Validation Accuracy: 0.8355\n",
      "Training stopped due to early stopping at epoch 60\n",
      "2 2 0\n",
      "Epoch 1/200, Loss: 0.6318, Validation Accuracy: 0.8116\n",
      "Epoch 2/200, Loss: 0.4347\n",
      "Epoch 3/200, Loss: 0.4232\n",
      "Epoch 4/200, Loss: 0.4192\n",
      "Epoch 5/200, Loss: 0.4145\n",
      "Epoch 6/200, Loss: 0.4120\n",
      "Epoch 7/200, Loss: 0.4082\n",
      "Epoch 8/200, Loss: 0.4079\n",
      "Epoch 9/200, Loss: 0.4020\n",
      "Epoch 10/200, Loss: 0.4010\n",
      "Epoch 11/200, Loss: 0.3966, Validation Accuracy: 0.8287\n",
      "Epoch 12/200, Loss: 0.3956\n",
      "Epoch 13/200, Loss: 0.3931\n",
      "Epoch 14/200, Loss: 0.3928\n",
      "Epoch 15/200, Loss: 0.3898\n",
      "Epoch 16/200, Loss: 0.3896\n",
      "Epoch 17/200, Loss: 0.3873\n",
      "Epoch 18/200, Loss: 0.3863\n",
      "Epoch 19/200, Loss: 0.3866\n",
      "Epoch 20/200, Loss: 0.3865\n",
      "Epoch 21/200, Loss: 0.3853, Validation Accuracy: 0.8367\n",
      "Epoch 22/200, Loss: 0.3841\n",
      "Epoch 23/200, Loss: 0.3842\n",
      "Epoch 24/200, Loss: 0.3841\n",
      "Epoch 25/200, Loss: 0.3852\n",
      "Epoch 26/200, Loss: 0.3831\n",
      "Epoch 27/200, Loss: 0.3813\n",
      "Epoch 28/200, Loss: 0.3826\n",
      "Epoch 29/200, Loss: 0.3820\n",
      "Epoch 30/200, Loss: 0.3800\n",
      "Epoch 31/200, Loss: 0.3805, Validation Accuracy: 0.8379\n",
      "Epoch 32/200, Loss: 0.3798\n",
      "Epoch 33/200, Loss: 0.3800\n",
      "Epoch 34/200, Loss: 0.3793\n",
      "Epoch 35/200, Loss: 0.3787\n",
      "Epoch 36/200, Loss: 0.3788\n",
      "Epoch 37/200, Loss: 0.3781\n",
      "Epoch 38/200, Loss: 0.3781\n",
      "Epoch 39/200, Loss: 0.3773\n",
      "Epoch 40/200, Loss: 0.3786\n",
      "Epoch 41/200, Loss: 0.3766, Validation Accuracy: 0.8407\n",
      "Epoch 42/200, Loss: 0.3767\n",
      "Epoch 43/200, Loss: 0.3760\n",
      "Epoch 44/200, Loss: 0.3755\n",
      "Epoch 45/200, Loss: 0.3779\n",
      "Epoch 46/200, Loss: 0.3755\n",
      "Epoch 47/200, Loss: 0.3747\n",
      "Epoch 48/200, Loss: 0.3757\n",
      "Epoch 49/200, Loss: 0.3748\n",
      "Epoch 50/200, Loss: 0.3769\n",
      "Epoch 51/200, Loss: 0.3740, Validation Accuracy: 0.8419\n",
      "Epoch 52/200, Loss: 0.3739\n",
      "Epoch 53/200, Loss: 0.3752\n",
      "Epoch 54/200, Loss: 0.3739\n",
      "Epoch 55/200, Loss: 0.3752\n",
      "Epoch 56/200, Loss: 0.3744\n",
      "Epoch 57/200, Loss: 0.3726\n",
      "Epoch 58/200, Loss: 0.3744\n",
      "Epoch 59/200, Loss: 0.3730\n",
      "Epoch 60/200, Loss: 0.3743\n",
      "Epoch 61/200, Loss: 0.3744, Validation Accuracy: 0.8451\n",
      "Epoch 62/200, Loss: 0.3732\n",
      "Epoch 63/200, Loss: 0.3731\n",
      "Epoch 64/200, Loss: 0.3725\n",
      "Epoch 65/200, Loss: 0.3718\n",
      "Epoch 66/200, Loss: 0.3738\n",
      "Epoch 67/200, Loss: 0.3726\n",
      "Epoch 68/200, Loss: 0.3718\n",
      "Epoch 69/200, Loss: 0.3725\n",
      "Epoch 70/200, Loss: 0.3704\n",
      "Epoch 71/200, Loss: 0.3706, Validation Accuracy: 0.8455\n",
      "Epoch 72/200, Loss: 0.3724\n",
      "Epoch 73/200, Loss: 0.3708\n",
      "Epoch 74/200, Loss: 0.3725\n",
      "Epoch 75/200, Loss: 0.3715\n",
      "Epoch 76/200, Loss: 0.3724\n",
      "Epoch 77/200, Loss: 0.3726\n",
      "Epoch 78/200, Loss: 0.3709\n",
      "Epoch 79/200, Loss: 0.3706\n",
      "Epoch 80/200, Loss: 0.3710\n",
      "Epoch 81/200, Loss: 0.3703, Validation Accuracy: 0.8423\n",
      "Training stopped due to early stopping at epoch 80\n",
      "2 2 0.1\n",
      "Epoch 1/200, Loss: 0.5824, Validation Accuracy: 0.8184\n",
      "Epoch 2/200, Loss: 0.4226\n",
      "Epoch 3/200, Loss: 0.4145\n",
      "Epoch 4/200, Loss: 0.4065\n",
      "Epoch 5/200, Loss: 0.4005\n",
      "Epoch 6/200, Loss: 0.3962\n",
      "Epoch 7/200, Loss: 0.3926\n",
      "Epoch 8/200, Loss: 0.3913\n",
      "Epoch 9/200, Loss: 0.3874\n",
      "Epoch 10/200, Loss: 0.3888\n",
      "Epoch 11/200, Loss: 0.3888, Validation Accuracy: 0.8359\n",
      "Epoch 12/200, Loss: 0.3854\n",
      "Epoch 13/200, Loss: 0.3845\n",
      "Epoch 14/200, Loss: 0.3851\n",
      "Epoch 15/200, Loss: 0.3841\n",
      "Epoch 16/200, Loss: 0.3813\n",
      "Epoch 17/200, Loss: 0.3838\n",
      "Epoch 18/200, Loss: 0.3817\n",
      "Epoch 19/200, Loss: 0.3835\n",
      "Epoch 20/200, Loss: 0.3814\n",
      "Epoch 21/200, Loss: 0.3808, Validation Accuracy: 0.8371\n",
      "Epoch 22/200, Loss: 0.3788\n",
      "Epoch 23/200, Loss: 0.3816\n",
      "Epoch 24/200, Loss: 0.3789\n",
      "Epoch 25/200, Loss: 0.3799\n",
      "Epoch 26/200, Loss: 0.3794\n",
      "Epoch 27/200, Loss: 0.3786\n",
      "Epoch 28/200, Loss: 0.3800\n",
      "Epoch 29/200, Loss: 0.3783\n",
      "Epoch 30/200, Loss: 0.3776\n",
      "Epoch 31/200, Loss: 0.3792, Validation Accuracy: 0.8423\n",
      "Epoch 32/200, Loss: 0.3783\n",
      "Epoch 33/200, Loss: 0.3777\n",
      "Epoch 34/200, Loss: 0.3786\n",
      "Epoch 35/200, Loss: 0.3777\n",
      "Epoch 36/200, Loss: 0.3776\n",
      "Epoch 37/200, Loss: 0.3780\n",
      "Epoch 38/200, Loss: 0.3782\n",
      "Epoch 39/200, Loss: 0.3766\n",
      "Epoch 40/200, Loss: 0.3775\n",
      "Epoch 41/200, Loss: 0.3751, Validation Accuracy: 0.8431\n",
      "Epoch 42/200, Loss: 0.3766\n",
      "Epoch 43/200, Loss: 0.3761\n",
      "Epoch 44/200, Loss: 0.3744\n",
      "Epoch 45/200, Loss: 0.3777\n",
      "Epoch 46/200, Loss: 0.3780\n",
      "Epoch 47/200, Loss: 0.3765\n",
      "Epoch 48/200, Loss: 0.3766\n",
      "Epoch 49/200, Loss: 0.3765\n",
      "Epoch 50/200, Loss: 0.3767\n",
      "Epoch 51/200, Loss: 0.3750, Validation Accuracy: 0.8423\n",
      "Epoch 52/200, Loss: 0.3757\n",
      "Epoch 53/200, Loss: 0.3747\n",
      "Epoch 54/200, Loss: 0.3733\n",
      "Epoch 55/200, Loss: 0.3758\n",
      "Epoch 56/200, Loss: 0.3758\n",
      "Epoch 57/200, Loss: 0.3749\n",
      "Epoch 58/200, Loss: 0.3744\n",
      "Epoch 59/200, Loss: 0.3748\n",
      "Epoch 60/200, Loss: 0.3744\n",
      "Epoch 61/200, Loss: 0.3742, Validation Accuracy: 0.8383\n",
      "Training stopped due to early stopping at epoch 60\n",
      "2 2 0.2\n",
      "Epoch 1/200, Loss: 0.5996, Validation Accuracy: 0.8088\n",
      "Epoch 2/200, Loss: 0.4300\n",
      "Epoch 3/200, Loss: 0.4151\n",
      "Epoch 4/200, Loss: 0.4082\n",
      "Epoch 5/200, Loss: 0.4041\n",
      "Epoch 6/200, Loss: 0.4044\n",
      "Epoch 7/200, Loss: 0.3961\n",
      "Epoch 8/200, Loss: 0.3937\n",
      "Epoch 9/200, Loss: 0.3929\n",
      "Epoch 10/200, Loss: 0.3924\n",
      "Epoch 11/200, Loss: 0.3922, Validation Accuracy: 0.8371\n",
      "Epoch 12/200, Loss: 0.3920\n",
      "Epoch 13/200, Loss: 0.3913\n",
      "Epoch 14/200, Loss: 0.3901\n",
      "Epoch 15/200, Loss: 0.3889\n",
      "Epoch 16/200, Loss: 0.3875\n",
      "Epoch 17/200, Loss: 0.3873\n",
      "Epoch 18/200, Loss: 0.3884\n",
      "Epoch 19/200, Loss: 0.3854\n",
      "Epoch 20/200, Loss: 0.3878\n",
      "Epoch 21/200, Loss: 0.3862, Validation Accuracy: 0.8391\n",
      "Epoch 22/200, Loss: 0.3848\n",
      "Epoch 23/200, Loss: 0.3842\n",
      "Epoch 24/200, Loss: 0.3845\n",
      "Epoch 25/200, Loss: 0.3827\n",
      "Epoch 26/200, Loss: 0.3822\n",
      "Epoch 27/200, Loss: 0.3829\n",
      "Epoch 28/200, Loss: 0.3803\n",
      "Epoch 29/200, Loss: 0.3825\n",
      "Epoch 30/200, Loss: 0.3819\n",
      "Epoch 31/200, Loss: 0.3818, Validation Accuracy: 0.8423\n",
      "Epoch 32/200, Loss: 0.3797\n",
      "Epoch 33/200, Loss: 0.3817\n",
      "Epoch 34/200, Loss: 0.3788\n",
      "Epoch 35/200, Loss: 0.3822\n",
      "Epoch 36/200, Loss: 0.3809\n",
      "Epoch 37/200, Loss: 0.3796\n",
      "Epoch 38/200, Loss: 0.3801\n",
      "Epoch 39/200, Loss: 0.3792\n",
      "Epoch 40/200, Loss: 0.3799\n",
      "Epoch 41/200, Loss: 0.3794, Validation Accuracy: 0.8411\n",
      "Epoch 42/200, Loss: 0.3781\n",
      "Epoch 43/200, Loss: 0.3777\n",
      "Epoch 44/200, Loss: 0.3790\n",
      "Epoch 45/200, Loss: 0.3784\n",
      "Epoch 46/200, Loss: 0.3793\n",
      "Epoch 47/200, Loss: 0.3806\n",
      "Epoch 48/200, Loss: 0.3792\n",
      "Epoch 49/200, Loss: 0.3774\n",
      "Epoch 50/200, Loss: 0.3781\n",
      "Epoch 51/200, Loss: 0.3766, Validation Accuracy: 0.8371\n",
      "Training stopped due to early stopping at epoch 50\n",
      "2 2 0.3\n",
      "Epoch 1/200, Loss: 0.5961, Validation Accuracy: 0.8032\n",
      "Epoch 2/200, Loss: 0.4289\n",
      "Epoch 3/200, Loss: 0.4198\n",
      "Epoch 4/200, Loss: 0.4083\n",
      "Epoch 5/200, Loss: 0.4073\n",
      "Epoch 6/200, Loss: 0.4042\n",
      "Epoch 7/200, Loss: 0.4003\n",
      "Epoch 8/200, Loss: 0.3991\n",
      "Epoch 9/200, Loss: 0.3952\n",
      "Epoch 10/200, Loss: 0.3936\n",
      "Epoch 11/200, Loss: 0.3939, Validation Accuracy: 0.8311\n",
      "Epoch 12/200, Loss: 0.3916\n",
      "Epoch 13/200, Loss: 0.3894\n",
      "Epoch 14/200, Loss: 0.3882\n",
      "Epoch 15/200, Loss: 0.3881\n",
      "Epoch 16/200, Loss: 0.3882\n",
      "Epoch 17/200, Loss: 0.3874\n",
      "Epoch 18/200, Loss: 0.3873\n",
      "Epoch 19/200, Loss: 0.3882\n",
      "Epoch 20/200, Loss: 0.3846\n",
      "Epoch 21/200, Loss: 0.3865, Validation Accuracy: 0.8395\n",
      "Epoch 22/200, Loss: 0.3847\n",
      "Epoch 23/200, Loss: 0.3837\n",
      "Epoch 24/200, Loss: 0.3835\n",
      "Epoch 25/200, Loss: 0.3836\n",
      "Epoch 26/200, Loss: 0.3817\n",
      "Epoch 27/200, Loss: 0.3826\n",
      "Epoch 28/200, Loss: 0.3826\n",
      "Epoch 29/200, Loss: 0.3812\n",
      "Epoch 30/200, Loss: 0.3819\n",
      "Epoch 31/200, Loss: 0.3820, Validation Accuracy: 0.8423\n",
      "Epoch 32/200, Loss: 0.3809\n",
      "Epoch 33/200, Loss: 0.3816\n",
      "Epoch 34/200, Loss: 0.3803\n",
      "Epoch 35/200, Loss: 0.3813\n",
      "Epoch 36/200, Loss: 0.3816\n",
      "Epoch 37/200, Loss: 0.3793\n",
      "Epoch 38/200, Loss: 0.3806\n",
      "Epoch 39/200, Loss: 0.3806\n",
      "Epoch 40/200, Loss: 0.3797\n",
      "Epoch 41/200, Loss: 0.3791, Validation Accuracy: 0.8423\n",
      "Epoch 42/200, Loss: 0.3798\n",
      "Epoch 43/200, Loss: 0.3794\n",
      "Epoch 44/200, Loss: 0.3804\n",
      "Epoch 45/200, Loss: 0.3787\n",
      "Epoch 46/200, Loss: 0.3795\n",
      "Epoch 47/200, Loss: 0.3780\n",
      "Epoch 48/200, Loss: 0.3791\n",
      "Epoch 49/200, Loss: 0.3794\n",
      "Epoch 50/200, Loss: 0.3799\n",
      "Epoch 51/200, Loss: 0.3785, Validation Accuracy: 0.8347\n",
      "Training stopped due to early stopping at epoch 50\n",
      "2 3 0\n",
      "Epoch 1/200, Loss: 0.5923, Validation Accuracy: 0.8160\n",
      "Epoch 2/200, Loss: 0.4210\n",
      "Epoch 3/200, Loss: 0.4078\n",
      "Epoch 4/200, Loss: 0.4015\n",
      "Epoch 5/200, Loss: 0.3973\n",
      "Epoch 6/200, Loss: 0.3972\n",
      "Epoch 7/200, Loss: 0.3956\n",
      "Epoch 8/200, Loss: 0.3921\n",
      "Epoch 9/200, Loss: 0.3913\n",
      "Epoch 10/200, Loss: 0.3906\n",
      "Epoch 11/200, Loss: 0.3884, Validation Accuracy: 0.8383\n",
      "Epoch 12/200, Loss: 0.3882\n",
      "Epoch 13/200, Loss: 0.3869\n",
      "Epoch 14/200, Loss: 0.3861\n",
      "Epoch 15/200, Loss: 0.3847\n",
      "Epoch 16/200, Loss: 0.3851\n",
      "Epoch 17/200, Loss: 0.3845\n",
      "Epoch 18/200, Loss: 0.3846\n",
      "Epoch 19/200, Loss: 0.3826\n",
      "Epoch 20/200, Loss: 0.3846\n",
      "Epoch 21/200, Loss: 0.3821, Validation Accuracy: 0.8379\n",
      "Epoch 22/200, Loss: 0.3818\n",
      "Epoch 23/200, Loss: 0.3820\n",
      "Epoch 24/200, Loss: 0.3803\n",
      "Epoch 25/200, Loss: 0.3801\n",
      "Epoch 26/200, Loss: 0.3790\n",
      "Epoch 27/200, Loss: 0.3792\n",
      "Epoch 28/200, Loss: 0.3791\n",
      "Epoch 29/200, Loss: 0.3786\n",
      "Epoch 30/200, Loss: 0.3776\n",
      "Epoch 31/200, Loss: 0.3774, Validation Accuracy: 0.8367\n",
      "Training stopped due to early stopping at epoch 30\n",
      "2 3 0.1\n",
      "Epoch 1/200, Loss: 0.5799, Validation Accuracy: 0.8140\n",
      "Epoch 2/200, Loss: 0.4266\n",
      "Epoch 3/200, Loss: 0.4157\n",
      "Epoch 4/200, Loss: 0.4082\n",
      "Epoch 5/200, Loss: 0.4036\n",
      "Epoch 6/200, Loss: 0.3985\n",
      "Epoch 7/200, Loss: 0.3953\n",
      "Epoch 8/200, Loss: 0.3950\n",
      "Epoch 9/200, Loss: 0.3932\n",
      "Epoch 10/200, Loss: 0.3912\n",
      "Epoch 11/200, Loss: 0.3874, Validation Accuracy: 0.8327\n",
      "Epoch 12/200, Loss: 0.3870\n",
      "Epoch 13/200, Loss: 0.3872\n",
      "Epoch 14/200, Loss: 0.3850\n",
      "Epoch 15/200, Loss: 0.3861\n",
      "Epoch 16/200, Loss: 0.3841\n",
      "Epoch 17/200, Loss: 0.3838\n",
      "Epoch 18/200, Loss: 0.3804\n",
      "Epoch 19/200, Loss: 0.3805\n",
      "Epoch 20/200, Loss: 0.3799\n",
      "Epoch 21/200, Loss: 0.3808, Validation Accuracy: 0.8403\n",
      "Epoch 22/200, Loss: 0.3808\n",
      "Epoch 23/200, Loss: 0.3799\n",
      "Epoch 24/200, Loss: 0.3811\n",
      "Epoch 25/200, Loss: 0.3803\n",
      "Epoch 26/200, Loss: 0.3785\n",
      "Epoch 27/200, Loss: 0.3807\n",
      "Epoch 28/200, Loss: 0.3782\n",
      "Epoch 29/200, Loss: 0.3782\n",
      "Epoch 30/200, Loss: 0.3776\n",
      "Epoch 31/200, Loss: 0.3789, Validation Accuracy: 0.8375\n",
      "Epoch 32/200, Loss: 0.3776\n",
      "Epoch 33/200, Loss: 0.3766\n",
      "Epoch 34/200, Loss: 0.3767\n",
      "Epoch 35/200, Loss: 0.3777\n",
      "Epoch 36/200, Loss: 0.3768\n",
      "Epoch 37/200, Loss: 0.3767\n",
      "Epoch 38/200, Loss: 0.3797\n",
      "Epoch 39/200, Loss: 0.3775\n",
      "Epoch 40/200, Loss: 0.3767\n",
      "Epoch 41/200, Loss: 0.3782, Validation Accuracy: 0.8419\n",
      "Epoch 42/200, Loss: 0.3764\n",
      "Epoch 43/200, Loss: 0.3762\n",
      "Epoch 44/200, Loss: 0.3763\n",
      "Epoch 45/200, Loss: 0.3775\n",
      "Epoch 46/200, Loss: 0.3770\n",
      "Epoch 47/200, Loss: 0.3757\n",
      "Epoch 48/200, Loss: 0.3753\n",
      "Epoch 49/200, Loss: 0.3745\n",
      "Epoch 50/200, Loss: 0.3776\n",
      "Epoch 51/200, Loss: 0.3759, Validation Accuracy: 0.8411\n",
      "Epoch 52/200, Loss: 0.3758\n",
      "Epoch 53/200, Loss: 0.3765\n",
      "Epoch 54/200, Loss: 0.3760\n",
      "Epoch 55/200, Loss: 0.3752\n",
      "Epoch 56/200, Loss: 0.3758\n",
      "Epoch 57/200, Loss: 0.3758\n",
      "Epoch 58/200, Loss: 0.3747\n",
      "Epoch 59/200, Loss: 0.3751\n",
      "Epoch 60/200, Loss: 0.3742\n",
      "Epoch 61/200, Loss: 0.3755, Validation Accuracy: 0.8395\n",
      "Training stopped due to early stopping at epoch 60\n",
      "2 3 0.2\n",
      "Epoch 1/200, Loss: 0.5962, Validation Accuracy: 0.8052\n",
      "Epoch 2/200, Loss: 0.4282\n",
      "Epoch 3/200, Loss: 0.4176\n",
      "Epoch 4/200, Loss: 0.4125\n",
      "Epoch 5/200, Loss: 0.4062\n",
      "Epoch 6/200, Loss: 0.4011\n",
      "Epoch 7/200, Loss: 0.3986\n",
      "Epoch 8/200, Loss: 0.3964\n",
      "Epoch 9/200, Loss: 0.3940\n",
      "Epoch 10/200, Loss: 0.3918\n",
      "Epoch 11/200, Loss: 0.3926, Validation Accuracy: 0.8299\n",
      "Epoch 12/200, Loss: 0.3896\n",
      "Epoch 13/200, Loss: 0.3885\n",
      "Epoch 14/200, Loss: 0.3879\n",
      "Epoch 15/200, Loss: 0.3863\n",
      "Epoch 16/200, Loss: 0.3859\n",
      "Epoch 17/200, Loss: 0.3867\n",
      "Epoch 18/200, Loss: 0.3839\n",
      "Epoch 19/200, Loss: 0.3836\n",
      "Epoch 20/200, Loss: 0.3848\n",
      "Epoch 21/200, Loss: 0.3844, Validation Accuracy: 0.8379\n",
      "Epoch 22/200, Loss: 0.3836\n",
      "Epoch 23/200, Loss: 0.3840\n",
      "Epoch 24/200, Loss: 0.3839\n",
      "Epoch 25/200, Loss: 0.3816\n",
      "Epoch 26/200, Loss: 0.3831\n",
      "Epoch 27/200, Loss: 0.3822\n",
      "Epoch 28/200, Loss: 0.3815\n",
      "Epoch 29/200, Loss: 0.3825\n",
      "Epoch 30/200, Loss: 0.3798\n",
      "Epoch 31/200, Loss: 0.3808, Validation Accuracy: 0.8359\n",
      "Epoch 32/200, Loss: 0.3776\n",
      "Epoch 33/200, Loss: 0.3791\n",
      "Epoch 34/200, Loss: 0.3789\n",
      "Epoch 35/200, Loss: 0.3794\n",
      "Epoch 36/200, Loss: 0.3785\n",
      "Epoch 37/200, Loss: 0.3788\n",
      "Epoch 38/200, Loss: 0.3784\n",
      "Epoch 39/200, Loss: 0.3782\n",
      "Epoch 40/200, Loss: 0.3782\n",
      "Epoch 41/200, Loss: 0.3767, Validation Accuracy: 0.8367\n",
      "Epoch 42/200, Loss: 0.3779\n",
      "Epoch 43/200, Loss: 0.3772\n",
      "Epoch 44/200, Loss: 0.3767\n",
      "Epoch 45/200, Loss: 0.3755\n",
      "Epoch 46/200, Loss: 0.3756\n",
      "Epoch 47/200, Loss: 0.3775\n",
      "Epoch 48/200, Loss: 0.3773\n",
      "Epoch 49/200, Loss: 0.3771\n",
      "Epoch 50/200, Loss: 0.3770\n",
      "Epoch 51/200, Loss: 0.3760, Validation Accuracy: 0.8403\n",
      "Epoch 52/200, Loss: 0.3757\n",
      "Epoch 53/200, Loss: 0.3743\n",
      "Epoch 54/200, Loss: 0.3767\n",
      "Epoch 55/200, Loss: 0.3739\n",
      "Epoch 56/200, Loss: 0.3750\n",
      "Epoch 57/200, Loss: 0.3754\n",
      "Epoch 58/200, Loss: 0.3756\n",
      "Epoch 59/200, Loss: 0.3745\n",
      "Epoch 60/200, Loss: 0.3740\n",
      "Epoch 61/200, Loss: 0.3731, Validation Accuracy: 0.8403\n",
      "Epoch 62/200, Loss: 0.3744\n",
      "Epoch 63/200, Loss: 0.3742\n",
      "Epoch 64/200, Loss: 0.3745\n",
      "Epoch 65/200, Loss: 0.3740\n",
      "Epoch 66/200, Loss: 0.3740\n",
      "Epoch 67/200, Loss: 0.3742\n",
      "Epoch 68/200, Loss: 0.3746\n",
      "Epoch 69/200, Loss: 0.3755\n",
      "Epoch 70/200, Loss: 0.3754\n",
      "Epoch 71/200, Loss: 0.3751, Validation Accuracy: 0.8439\n",
      "Epoch 72/200, Loss: 0.3735\n",
      "Epoch 73/200, Loss: 0.3739\n",
      "Epoch 74/200, Loss: 0.3746\n",
      "Epoch 75/200, Loss: 0.3741\n",
      "Epoch 76/200, Loss: 0.3735\n",
      "Epoch 77/200, Loss: 0.3728\n",
      "Epoch 78/200, Loss: 0.3729\n",
      "Epoch 79/200, Loss: 0.3723\n",
      "Epoch 80/200, Loss: 0.3724\n",
      "Epoch 81/200, Loss: 0.3733, Validation Accuracy: 0.8359\n",
      "Training stopped due to early stopping at epoch 80\n",
      "2 3 0.3\n",
      "Epoch 1/200, Loss: 0.6078, Validation Accuracy: 0.8120\n",
      "Epoch 2/200, Loss: 0.4312\n",
      "Epoch 3/200, Loss: 0.4200\n",
      "Epoch 4/200, Loss: 0.4149\n",
      "Epoch 5/200, Loss: 0.4133\n",
      "Epoch 6/200, Loss: 0.4068\n",
      "Epoch 7/200, Loss: 0.4076\n",
      "Epoch 8/200, Loss: 0.4060\n",
      "Epoch 9/200, Loss: 0.4057\n",
      "Epoch 10/200, Loss: 0.4030\n",
      "Epoch 11/200, Loss: 0.4014, Validation Accuracy: 0.8307\n",
      "Epoch 12/200, Loss: 0.3999\n",
      "Epoch 13/200, Loss: 0.3989\n",
      "Epoch 14/200, Loss: 0.3962\n",
      "Epoch 15/200, Loss: 0.3966\n",
      "Epoch 16/200, Loss: 0.3960\n",
      "Epoch 17/200, Loss: 0.3933\n",
      "Epoch 18/200, Loss: 0.3914\n",
      "Epoch 19/200, Loss: 0.3915\n",
      "Epoch 20/200, Loss: 0.3923\n",
      "Epoch 21/200, Loss: 0.3914, Validation Accuracy: 0.8343\n",
      "Epoch 22/200, Loss: 0.3924\n",
      "Epoch 23/200, Loss: 0.3895\n",
      "Epoch 24/200, Loss: 0.3878\n",
      "Epoch 25/200, Loss: 0.3907\n",
      "Epoch 26/200, Loss: 0.3877\n",
      "Epoch 27/200, Loss: 0.3864\n",
      "Epoch 28/200, Loss: 0.3853\n",
      "Epoch 29/200, Loss: 0.3845\n",
      "Epoch 30/200, Loss: 0.3864\n",
      "Epoch 31/200, Loss: 0.3835, Validation Accuracy: 0.8323\n",
      "Epoch 32/200, Loss: 0.3847\n",
      "Epoch 33/200, Loss: 0.3854\n",
      "Epoch 34/200, Loss: 0.3837\n",
      "Epoch 35/200, Loss: 0.3841\n",
      "Epoch 36/200, Loss: 0.3835\n",
      "Epoch 37/200, Loss: 0.3844\n",
      "Epoch 38/200, Loss: 0.3824\n",
      "Epoch 39/200, Loss: 0.3834\n",
      "Epoch 40/200, Loss: 0.3826\n",
      "Epoch 41/200, Loss: 0.3832, Validation Accuracy: 0.8407\n",
      "Epoch 42/200, Loss: 0.3812\n",
      "Epoch 43/200, Loss: 0.3830\n",
      "Epoch 44/200, Loss: 0.3818\n",
      "Epoch 45/200, Loss: 0.3822\n",
      "Epoch 46/200, Loss: 0.3803\n",
      "Epoch 47/200, Loss: 0.3803\n",
      "Epoch 48/200, Loss: 0.3821\n",
      "Epoch 49/200, Loss: 0.3807\n",
      "Epoch 50/200, Loss: 0.3817\n",
      "Epoch 51/200, Loss: 0.3805, Validation Accuracy: 0.8379\n",
      "Epoch 52/200, Loss: 0.3801\n",
      "Epoch 53/200, Loss: 0.3815\n",
      "Epoch 54/200, Loss: 0.3815\n",
      "Epoch 55/200, Loss: 0.3804\n",
      "Epoch 56/200, Loss: 0.3792\n",
      "Epoch 57/200, Loss: 0.3802\n",
      "Epoch 58/200, Loss: 0.3819\n",
      "Epoch 59/200, Loss: 0.3794\n",
      "Epoch 60/200, Loss: 0.3802\n",
      "Epoch 61/200, Loss: 0.3815, Validation Accuracy: 0.8347\n",
      "Training stopped due to early stopping at epoch 60\n",
      "3 1 0\n",
      "Epoch 1/200, Loss: 0.5281, Validation Accuracy: 0.8224\n",
      "Epoch 2/200, Loss: 0.4121\n",
      "Epoch 3/200, Loss: 0.4031\n",
      "Epoch 4/200, Loss: 0.3985\n",
      "Epoch 5/200, Loss: 0.3943\n",
      "Epoch 6/200, Loss: 0.3915\n",
      "Epoch 7/200, Loss: 0.3881\n",
      "Epoch 8/200, Loss: 0.3879\n",
      "Epoch 9/200, Loss: 0.3855\n",
      "Epoch 10/200, Loss: 0.3855\n",
      "Epoch 11/200, Loss: 0.3838, Validation Accuracy: 0.8347\n",
      "Epoch 12/200, Loss: 0.3815\n",
      "Epoch 13/200, Loss: 0.3824\n",
      "Epoch 14/200, Loss: 0.3822\n",
      "Epoch 15/200, Loss: 0.3805\n",
      "Epoch 16/200, Loss: 0.3797\n",
      "Epoch 17/200, Loss: 0.3785\n",
      "Epoch 18/200, Loss: 0.3773\n",
      "Epoch 19/200, Loss: 0.3788\n",
      "Epoch 20/200, Loss: 0.3772\n",
      "Epoch 21/200, Loss: 0.3766, Validation Accuracy: 0.8443\n",
      "Epoch 22/200, Loss: 0.3799\n",
      "Epoch 23/200, Loss: 0.3789\n",
      "Epoch 24/200, Loss: 0.3770\n",
      "Epoch 25/200, Loss: 0.3772\n",
      "Epoch 26/200, Loss: 0.3752\n",
      "Epoch 27/200, Loss: 0.3752\n",
      "Epoch 28/200, Loss: 0.3751\n",
      "Epoch 29/200, Loss: 0.3754\n",
      "Epoch 30/200, Loss: 0.3764\n",
      "Epoch 31/200, Loss: 0.3727, Validation Accuracy: 0.8419\n",
      "Epoch 32/200, Loss: 0.3743\n",
      "Epoch 33/200, Loss: 0.3743\n",
      "Epoch 34/200, Loss: 0.3757\n",
      "Epoch 35/200, Loss: 0.3737\n",
      "Epoch 36/200, Loss: 0.3731\n",
      "Epoch 37/200, Loss: 0.3736\n",
      "Epoch 38/200, Loss: 0.3740\n",
      "Epoch 39/200, Loss: 0.3733\n",
      "Epoch 40/200, Loss: 0.3735\n",
      "Epoch 41/200, Loss: 0.3734, Validation Accuracy: 0.8427\n",
      "Epoch 42/200, Loss: 0.3740\n",
      "Epoch 43/200, Loss: 0.3733\n",
      "Epoch 44/200, Loss: 0.3729\n",
      "Epoch 45/200, Loss: 0.3732\n",
      "Epoch 46/200, Loss: 0.3750\n",
      "Epoch 47/200, Loss: 0.3722\n",
      "Epoch 48/200, Loss: 0.3721\n",
      "Epoch 49/200, Loss: 0.3725\n",
      "Epoch 50/200, Loss: 0.3717\n",
      "Epoch 51/200, Loss: 0.3710, Validation Accuracy: 0.8435\n",
      "Epoch 52/200, Loss: 0.3720\n",
      "Epoch 53/200, Loss: 0.3707\n",
      "Epoch 54/200, Loss: 0.3736\n",
      "Epoch 55/200, Loss: 0.3729\n",
      "Epoch 56/200, Loss: 0.3711\n",
      "Epoch 57/200, Loss: 0.3700\n",
      "Epoch 58/200, Loss: 0.3713\n",
      "Epoch 59/200, Loss: 0.3707\n",
      "Epoch 60/200, Loss: 0.3699\n",
      "Epoch 61/200, Loss: 0.3704, Validation Accuracy: 0.8483\n",
      "Epoch 62/200, Loss: 0.3712\n",
      "Epoch 63/200, Loss: 0.3719\n",
      "Epoch 64/200, Loss: 0.3708\n",
      "Epoch 65/200, Loss: 0.3704\n",
      "Epoch 66/200, Loss: 0.3709\n",
      "Epoch 67/200, Loss: 0.3700\n",
      "Epoch 68/200, Loss: 0.3698\n",
      "Epoch 69/200, Loss: 0.3703\n",
      "Epoch 70/200, Loss: 0.3715\n",
      "Epoch 71/200, Loss: 0.3701, Validation Accuracy: 0.8427\n",
      "Training stopped due to early stopping at epoch 70\n",
      "3 1 0.1\n",
      "Epoch 1/200, Loss: 0.5320, Validation Accuracy: 0.8188\n",
      "Epoch 2/200, Loss: 0.4103\n",
      "Epoch 3/200, Loss: 0.3999\n",
      "Epoch 4/200, Loss: 0.3948\n",
      "Epoch 5/200, Loss: 0.3912\n",
      "Epoch 6/200, Loss: 0.3900\n",
      "Epoch 7/200, Loss: 0.3855\n",
      "Epoch 8/200, Loss: 0.3856\n",
      "Epoch 9/200, Loss: 0.3835\n",
      "Epoch 10/200, Loss: 0.3819\n",
      "Epoch 11/200, Loss: 0.3798, Validation Accuracy: 0.8375\n",
      "Epoch 12/200, Loss: 0.3786\n",
      "Epoch 13/200, Loss: 0.3795\n",
      "Epoch 14/200, Loss: 0.3790\n",
      "Epoch 15/200, Loss: 0.3800\n",
      "Epoch 16/200, Loss: 0.3780\n",
      "Epoch 17/200, Loss: 0.3784\n",
      "Epoch 18/200, Loss: 0.3778\n",
      "Epoch 19/200, Loss: 0.3768\n",
      "Epoch 20/200, Loss: 0.3768\n",
      "Epoch 21/200, Loss: 0.3768, Validation Accuracy: 0.8443\n",
      "Epoch 22/200, Loss: 0.3768\n",
      "Epoch 23/200, Loss: 0.3760\n",
      "Epoch 24/200, Loss: 0.3746\n",
      "Epoch 25/200, Loss: 0.3765\n",
      "Epoch 26/200, Loss: 0.3745\n",
      "Epoch 27/200, Loss: 0.3744\n",
      "Epoch 28/200, Loss: 0.3736\n",
      "Epoch 29/200, Loss: 0.3758\n",
      "Epoch 30/200, Loss: 0.3734\n",
      "Epoch 31/200, Loss: 0.3779, Validation Accuracy: 0.8439\n",
      "Epoch 32/200, Loss: 0.3730\n",
      "Epoch 33/200, Loss: 0.3734\n",
      "Epoch 34/200, Loss: 0.3735\n",
      "Epoch 35/200, Loss: 0.3749\n",
      "Epoch 36/200, Loss: 0.3727\n",
      "Epoch 37/200, Loss: 0.3732\n",
      "Epoch 38/200, Loss: 0.3734\n",
      "Epoch 39/200, Loss: 0.3726\n",
      "Epoch 40/200, Loss: 0.3728\n",
      "Epoch 41/200, Loss: 0.3740, Validation Accuracy: 0.8427\n",
      "Training stopped due to early stopping at epoch 40\n",
      "3 1 0.2\n",
      "Epoch 1/200, Loss: 0.5335, Validation Accuracy: 0.8172\n",
      "Epoch 2/200, Loss: 0.4137\n",
      "Epoch 3/200, Loss: 0.4057\n",
      "Epoch 4/200, Loss: 0.3994\n",
      "Epoch 5/200, Loss: 0.3974\n",
      "Epoch 6/200, Loss: 0.3958\n",
      "Epoch 7/200, Loss: 0.3950\n",
      "Epoch 8/200, Loss: 0.3919\n",
      "Epoch 9/200, Loss: 0.3908\n",
      "Epoch 10/200, Loss: 0.3894\n",
      "Epoch 11/200, Loss: 0.3877, Validation Accuracy: 0.8375\n",
      "Epoch 12/200, Loss: 0.3861\n",
      "Epoch 13/200, Loss: 0.3861\n",
      "Epoch 14/200, Loss: 0.3848\n",
      "Epoch 15/200, Loss: 0.3836\n",
      "Epoch 16/200, Loss: 0.3854\n",
      "Epoch 17/200, Loss: 0.3837\n",
      "Epoch 18/200, Loss: 0.3843\n",
      "Epoch 19/200, Loss: 0.3821\n",
      "Epoch 20/200, Loss: 0.3825\n",
      "Epoch 21/200, Loss: 0.3808, Validation Accuracy: 0.8351\n",
      "Epoch 22/200, Loss: 0.3828\n",
      "Epoch 23/200, Loss: 0.3830\n",
      "Epoch 24/200, Loss: 0.3825\n",
      "Epoch 25/200, Loss: 0.3801\n",
      "Epoch 26/200, Loss: 0.3804\n",
      "Epoch 27/200, Loss: 0.3805\n",
      "Epoch 28/200, Loss: 0.3811\n",
      "Epoch 29/200, Loss: 0.3801\n",
      "Epoch 30/200, Loss: 0.3792\n",
      "Epoch 31/200, Loss: 0.3790, Validation Accuracy: 0.8419\n",
      "Epoch 32/200, Loss: 0.3800\n",
      "Epoch 33/200, Loss: 0.3782\n",
      "Epoch 34/200, Loss: 0.3795\n",
      "Epoch 35/200, Loss: 0.3795\n",
      "Epoch 36/200, Loss: 0.3758\n",
      "Epoch 37/200, Loss: 0.3784\n",
      "Epoch 38/200, Loss: 0.3749\n",
      "Epoch 39/200, Loss: 0.3763\n",
      "Epoch 40/200, Loss: 0.3781\n",
      "Epoch 41/200, Loss: 0.3767, Validation Accuracy: 0.8431\n",
      "Epoch 42/200, Loss: 0.3782\n",
      "Epoch 43/200, Loss: 0.3762\n",
      "Epoch 44/200, Loss: 0.3767\n",
      "Epoch 45/200, Loss: 0.3776\n",
      "Epoch 46/200, Loss: 0.3764\n",
      "Epoch 47/200, Loss: 0.3750\n",
      "Epoch 48/200, Loss: 0.3748\n",
      "Epoch 49/200, Loss: 0.3759\n",
      "Epoch 50/200, Loss: 0.3747\n",
      "Epoch 51/200, Loss: 0.3755, Validation Accuracy: 0.8415\n",
      "Training stopped due to early stopping at epoch 50\n",
      "3 1 0.3\n",
      "Epoch 1/200, Loss: 0.5378, Validation Accuracy: 0.8216\n",
      "Epoch 2/200, Loss: 0.4128\n",
      "Epoch 3/200, Loss: 0.4083\n",
      "Epoch 4/200, Loss: 0.4031\n",
      "Epoch 5/200, Loss: 0.3991\n",
      "Epoch 6/200, Loss: 0.3976\n",
      "Epoch 7/200, Loss: 0.3955\n",
      "Epoch 8/200, Loss: 0.3944\n",
      "Epoch 9/200, Loss: 0.3944\n",
      "Epoch 10/200, Loss: 0.3941\n",
      "Epoch 11/200, Loss: 0.3917, Validation Accuracy: 0.8375\n",
      "Epoch 12/200, Loss: 0.3895\n",
      "Epoch 13/200, Loss: 0.3899\n",
      "Epoch 14/200, Loss: 0.3875\n",
      "Epoch 15/200, Loss: 0.3901\n",
      "Epoch 16/200, Loss: 0.3883\n",
      "Epoch 17/200, Loss: 0.3880\n",
      "Epoch 18/200, Loss: 0.3872\n",
      "Epoch 19/200, Loss: 0.3836\n",
      "Epoch 20/200, Loss: 0.3850\n",
      "Epoch 21/200, Loss: 0.3853, Validation Accuracy: 0.8399\n",
      "Epoch 22/200, Loss: 0.3858\n",
      "Epoch 23/200, Loss: 0.3828\n",
      "Epoch 24/200, Loss: 0.3846\n",
      "Epoch 25/200, Loss: 0.3830\n",
      "Epoch 26/200, Loss: 0.3815\n",
      "Epoch 27/200, Loss: 0.3837\n",
      "Epoch 28/200, Loss: 0.3810\n",
      "Epoch 29/200, Loss: 0.3808\n",
      "Epoch 30/200, Loss: 0.3809\n",
      "Epoch 31/200, Loss: 0.3821, Validation Accuracy: 0.8391\n",
      "Epoch 32/200, Loss: 0.3820\n",
      "Epoch 33/200, Loss: 0.3820\n",
      "Epoch 34/200, Loss: 0.3789\n",
      "Epoch 35/200, Loss: 0.3807\n",
      "Epoch 36/200, Loss: 0.3801\n",
      "Epoch 37/200, Loss: 0.3786\n",
      "Epoch 38/200, Loss: 0.3793\n",
      "Epoch 39/200, Loss: 0.3797\n",
      "Epoch 40/200, Loss: 0.3787\n",
      "Epoch 41/200, Loss: 0.3776, Validation Accuracy: 0.8439\n",
      "Epoch 42/200, Loss: 0.3798\n",
      "Epoch 43/200, Loss: 0.3787\n",
      "Epoch 44/200, Loss: 0.3782\n",
      "Epoch 45/200, Loss: 0.3781\n",
      "Epoch 46/200, Loss: 0.3773\n",
      "Epoch 47/200, Loss: 0.3780\n",
      "Epoch 48/200, Loss: 0.3779\n",
      "Epoch 49/200, Loss: 0.3787\n",
      "Epoch 50/200, Loss: 0.3772\n",
      "Epoch 51/200, Loss: 0.3771, Validation Accuracy: 0.8419\n",
      "Epoch 52/200, Loss: 0.3777\n",
      "Epoch 53/200, Loss: 0.3768\n",
      "Epoch 54/200, Loss: 0.3780\n",
      "Epoch 55/200, Loss: 0.3760\n",
      "Epoch 56/200, Loss: 0.3764\n",
      "Epoch 57/200, Loss: 0.3760\n",
      "Epoch 58/200, Loss: 0.3768\n",
      "Epoch 59/200, Loss: 0.3761\n",
      "Epoch 60/200, Loss: 0.3766\n",
      "Epoch 61/200, Loss: 0.3762, Validation Accuracy: 0.8431\n",
      "Epoch 62/200, Loss: 0.3761\n",
      "Epoch 63/200, Loss: 0.3759\n",
      "Epoch 64/200, Loss: 0.3740\n",
      "Epoch 65/200, Loss: 0.3759\n",
      "Epoch 66/200, Loss: 0.3762\n",
      "Epoch 67/200, Loss: 0.3745\n",
      "Epoch 68/200, Loss: 0.3764\n",
      "Epoch 69/200, Loss: 0.3739\n",
      "Epoch 70/200, Loss: 0.3792\n",
      "Epoch 71/200, Loss: 0.3757, Validation Accuracy: 0.8467\n",
      "Epoch 72/200, Loss: 0.3743\n",
      "Epoch 73/200, Loss: 0.3737\n",
      "Epoch 74/200, Loss: 0.3740\n",
      "Epoch 75/200, Loss: 0.3738\n",
      "Epoch 76/200, Loss: 0.3730\n",
      "Epoch 77/200, Loss: 0.3752\n",
      "Epoch 78/200, Loss: 0.3752\n",
      "Epoch 79/200, Loss: 0.3748\n",
      "Epoch 80/200, Loss: 0.3742\n",
      "Epoch 81/200, Loss: 0.3739, Validation Accuracy: 0.8459\n",
      "Epoch 82/200, Loss: 0.3755\n",
      "Epoch 83/200, Loss: 0.3732\n",
      "Epoch 84/200, Loss: 0.3735\n",
      "Epoch 85/200, Loss: 0.3724\n",
      "Epoch 86/200, Loss: 0.3729\n",
      "Epoch 87/200, Loss: 0.3746\n",
      "Epoch 88/200, Loss: 0.3720\n",
      "Epoch 89/200, Loss: 0.3743\n",
      "Epoch 90/200, Loss: 0.3734\n",
      "Epoch 91/200, Loss: 0.3734, Validation Accuracy: 0.8459\n",
      "Epoch 92/200, Loss: 0.3727\n",
      "Epoch 93/200, Loss: 0.3735\n",
      "Epoch 94/200, Loss: 0.3720\n",
      "Epoch 95/200, Loss: 0.3717\n",
      "Epoch 96/200, Loss: 0.3728\n",
      "Epoch 97/200, Loss: 0.3737\n",
      "Epoch 98/200, Loss: 0.3740\n",
      "Epoch 99/200, Loss: 0.3726\n",
      "Epoch 100/200, Loss: 0.3734\n",
      "Epoch 101/200, Loss: 0.3727, Validation Accuracy: 0.8479\n",
      "Epoch 102/200, Loss: 0.3717\n",
      "Epoch 103/200, Loss: 0.3724\n",
      "Epoch 104/200, Loss: 0.3717\n",
      "Epoch 105/200, Loss: 0.3726\n",
      "Epoch 106/200, Loss: 0.3755\n",
      "Epoch 107/200, Loss: 0.3739\n",
      "Epoch 108/200, Loss: 0.3721\n",
      "Epoch 109/200, Loss: 0.3709\n",
      "Epoch 110/200, Loss: 0.3713\n",
      "Epoch 111/200, Loss: 0.3719, Validation Accuracy: 0.8463\n",
      "Epoch 112/200, Loss: 0.3718\n",
      "Epoch 113/200, Loss: 0.3710\n",
      "Epoch 114/200, Loss: 0.3717\n",
      "Epoch 115/200, Loss: 0.3709\n",
      "Epoch 116/200, Loss: 0.3710\n",
      "Epoch 117/200, Loss: 0.3713\n",
      "Epoch 118/200, Loss: 0.3707\n",
      "Epoch 119/200, Loss: 0.3709\n",
      "Epoch 120/200, Loss: 0.3706\n",
      "Epoch 121/200, Loss: 0.3709, Validation Accuracy: 0.8463\n",
      "Epoch 122/200, Loss: 0.3718\n",
      "Epoch 123/200, Loss: 0.3714\n",
      "Epoch 124/200, Loss: 0.3705\n",
      "Epoch 125/200, Loss: 0.3721\n",
      "Epoch 126/200, Loss: 0.3702\n",
      "Epoch 127/200, Loss: 0.3720\n",
      "Epoch 128/200, Loss: 0.3727\n",
      "Epoch 129/200, Loss: 0.3713\n",
      "Epoch 130/200, Loss: 0.3690\n",
      "Epoch 131/200, Loss: 0.3686, Validation Accuracy: 0.8491\n",
      "Epoch 132/200, Loss: 0.3715\n",
      "Epoch 133/200, Loss: 0.3706\n",
      "Epoch 134/200, Loss: 0.3727\n",
      "Epoch 135/200, Loss: 0.3714\n",
      "Epoch 136/200, Loss: 0.3707\n",
      "Epoch 137/200, Loss: 0.3706\n",
      "Epoch 138/200, Loss: 0.3713\n",
      "Epoch 139/200, Loss: 0.3706\n",
      "Epoch 140/200, Loss: 0.3703\n",
      "Epoch 141/200, Loss: 0.3692, Validation Accuracy: 0.8479\n",
      "Epoch 142/200, Loss: 0.3706\n",
      "Epoch 143/200, Loss: 0.3686\n",
      "Epoch 144/200, Loss: 0.3718\n",
      "Epoch 145/200, Loss: 0.3719\n",
      "Epoch 146/200, Loss: 0.3693\n",
      "Epoch 147/200, Loss: 0.3691\n",
      "Epoch 148/200, Loss: 0.3698\n",
      "Epoch 149/200, Loss: 0.3705\n",
      "Epoch 150/200, Loss: 0.3695\n",
      "Epoch 151/200, Loss: 0.3697, Validation Accuracy: 0.8483\n",
      "Epoch 152/200, Loss: 0.3698\n",
      "Epoch 153/200, Loss: 0.3700\n",
      "Epoch 154/200, Loss: 0.3689\n",
      "Epoch 155/200, Loss: 0.3706\n",
      "Epoch 156/200, Loss: 0.3719\n",
      "Epoch 157/200, Loss: 0.3693\n",
      "Epoch 158/200, Loss: 0.3679\n",
      "Epoch 159/200, Loss: 0.3679\n",
      "Epoch 160/200, Loss: 0.3692\n",
      "Epoch 161/200, Loss: 0.3707, Validation Accuracy: 0.8467\n",
      "Training stopped due to early stopping at epoch 160\n",
      "3 2 0\n",
      "Epoch 1/200, Loss: 0.5247, Validation Accuracy: 0.8216\n",
      "Epoch 2/200, Loss: 0.4099\n",
      "Epoch 3/200, Loss: 0.4025\n",
      "Epoch 4/200, Loss: 0.3945\n",
      "Epoch 5/200, Loss: 0.3928\n",
      "Epoch 6/200, Loss: 0.3908\n",
      "Epoch 7/200, Loss: 0.3875\n",
      "Epoch 8/200, Loss: 0.3880\n",
      "Epoch 9/200, Loss: 0.3892\n",
      "Epoch 10/200, Loss: 0.3852\n",
      "Epoch 11/200, Loss: 0.3844, Validation Accuracy: 0.8339\n",
      "Epoch 12/200, Loss: 0.3850\n",
      "Epoch 13/200, Loss: 0.3873\n",
      "Epoch 14/200, Loss: 0.3851\n",
      "Epoch 15/200, Loss: 0.3822\n",
      "Epoch 16/200, Loss: 0.3823\n",
      "Epoch 17/200, Loss: 0.3822\n",
      "Epoch 18/200, Loss: 0.3822\n",
      "Epoch 19/200, Loss: 0.3817\n",
      "Epoch 20/200, Loss: 0.3807\n",
      "Epoch 21/200, Loss: 0.3801, Validation Accuracy: 0.8387\n",
      "Epoch 22/200, Loss: 0.3797\n",
      "Epoch 23/200, Loss: 0.3790\n",
      "Epoch 24/200, Loss: 0.3797\n",
      "Epoch 25/200, Loss: 0.3789\n",
      "Epoch 26/200, Loss: 0.3796\n",
      "Epoch 27/200, Loss: 0.3781\n",
      "Epoch 28/200, Loss: 0.3795\n",
      "Epoch 29/200, Loss: 0.3784\n",
      "Epoch 30/200, Loss: 0.3782\n",
      "Epoch 31/200, Loss: 0.3784, Validation Accuracy: 0.8427\n",
      "Epoch 32/200, Loss: 0.3780\n",
      "Epoch 33/200, Loss: 0.3780\n",
      "Epoch 34/200, Loss: 0.3812\n",
      "Epoch 35/200, Loss: 0.3790\n",
      "Epoch 36/200, Loss: 0.3778\n",
      "Epoch 37/200, Loss: 0.3778\n",
      "Epoch 38/200, Loss: 0.3786\n",
      "Epoch 39/200, Loss: 0.3765\n",
      "Epoch 40/200, Loss: 0.3764\n",
      "Epoch 41/200, Loss: 0.3759, Validation Accuracy: 0.8447\n",
      "Epoch 42/200, Loss: 0.3755\n",
      "Epoch 43/200, Loss: 0.3764\n",
      "Epoch 44/200, Loss: 0.3749\n",
      "Epoch 45/200, Loss: 0.3769\n",
      "Epoch 46/200, Loss: 0.3756\n",
      "Epoch 47/200, Loss: 0.3743\n",
      "Epoch 48/200, Loss: 0.3764\n",
      "Epoch 49/200, Loss: 0.3747\n",
      "Epoch 50/200, Loss: 0.3778\n",
      "Epoch 51/200, Loss: 0.3760, Validation Accuracy: 0.8391\n",
      "Training stopped due to early stopping at epoch 50\n",
      "3 2 0.1\n",
      "Epoch 1/200, Loss: 0.5313, Validation Accuracy: 0.8220\n",
      "Epoch 2/200, Loss: 0.4192\n",
      "Epoch 3/200, Loss: 0.4109\n",
      "Epoch 4/200, Loss: 0.4063\n",
      "Epoch 5/200, Loss: 0.3993\n",
      "Epoch 6/200, Loss: 0.3960\n",
      "Epoch 7/200, Loss: 0.3951\n",
      "Epoch 8/200, Loss: 0.3894\n",
      "Epoch 9/200, Loss: 0.3867\n",
      "Epoch 10/200, Loss: 0.3881\n",
      "Epoch 11/200, Loss: 0.3888, Validation Accuracy: 0.8311\n",
      "Epoch 12/200, Loss: 0.3834\n",
      "Epoch 13/200, Loss: 0.3843\n",
      "Epoch 14/200, Loss: 0.3820\n",
      "Epoch 15/200, Loss: 0.3818\n",
      "Epoch 16/200, Loss: 0.3812\n",
      "Epoch 17/200, Loss: 0.3813\n",
      "Epoch 18/200, Loss: 0.3809\n",
      "Epoch 19/200, Loss: 0.3795\n",
      "Epoch 20/200, Loss: 0.3794\n",
      "Epoch 21/200, Loss: 0.3785, Validation Accuracy: 0.8435\n",
      "Epoch 22/200, Loss: 0.3805\n",
      "Epoch 23/200, Loss: 0.3783\n",
      "Epoch 24/200, Loss: 0.3766\n",
      "Epoch 25/200, Loss: 0.3793\n",
      "Epoch 26/200, Loss: 0.3759\n",
      "Epoch 27/200, Loss: 0.3765\n",
      "Epoch 28/200, Loss: 0.3751\n",
      "Epoch 29/200, Loss: 0.3752\n",
      "Epoch 30/200, Loss: 0.3766\n",
      "Epoch 31/200, Loss: 0.3750, Validation Accuracy: 0.8371\n",
      "Epoch 32/200, Loss: 0.3766\n",
      "Epoch 33/200, Loss: 0.3760\n",
      "Epoch 34/200, Loss: 0.3759\n",
      "Epoch 35/200, Loss: 0.3745\n",
      "Epoch 36/200, Loss: 0.3746\n",
      "Epoch 37/200, Loss: 0.3734\n",
      "Epoch 38/200, Loss: 0.3737\n",
      "Epoch 39/200, Loss: 0.3766\n",
      "Epoch 40/200, Loss: 0.3727\n",
      "Epoch 41/200, Loss: 0.3755, Validation Accuracy: 0.8439\n",
      "Epoch 42/200, Loss: 0.3736\n",
      "Epoch 43/200, Loss: 0.3739\n",
      "Epoch 44/200, Loss: 0.3726\n",
      "Epoch 45/200, Loss: 0.3723\n",
      "Epoch 46/200, Loss: 0.3744\n",
      "Epoch 47/200, Loss: 0.3735\n",
      "Epoch 48/200, Loss: 0.3710\n",
      "Epoch 49/200, Loss: 0.3719\n",
      "Epoch 50/200, Loss: 0.3711\n",
      "Epoch 51/200, Loss: 0.3717, Validation Accuracy: 0.8427\n",
      "Epoch 52/200, Loss: 0.3711\n",
      "Epoch 53/200, Loss: 0.3730\n",
      "Epoch 54/200, Loss: 0.3708\n",
      "Epoch 55/200, Loss: 0.3712\n",
      "Epoch 56/200, Loss: 0.3719\n",
      "Epoch 57/200, Loss: 0.3716\n",
      "Epoch 58/200, Loss: 0.3711\n",
      "Epoch 59/200, Loss: 0.3711\n",
      "Epoch 60/200, Loss: 0.3708\n",
      "Epoch 61/200, Loss: 0.3694, Validation Accuracy: 0.8435\n",
      "Epoch 62/200, Loss: 0.3719\n",
      "Epoch 63/200, Loss: 0.3706\n",
      "Epoch 64/200, Loss: 0.3689\n",
      "Epoch 65/200, Loss: 0.3697\n",
      "Epoch 66/200, Loss: 0.3705\n",
      "Epoch 67/200, Loss: 0.3691\n",
      "Epoch 68/200, Loss: 0.3689\n",
      "Epoch 69/200, Loss: 0.3700\n",
      "Epoch 70/200, Loss: 0.3683\n",
      "Epoch 71/200, Loss: 0.3707, Validation Accuracy: 0.8431\n",
      "Epoch 72/200, Loss: 0.3702\n",
      "Epoch 73/200, Loss: 0.3699\n",
      "Epoch 74/200, Loss: 0.3708\n",
      "Epoch 75/200, Loss: 0.3685\n",
      "Epoch 76/200, Loss: 0.3682\n",
      "Epoch 77/200, Loss: 0.3690\n",
      "Epoch 78/200, Loss: 0.3688\n",
      "Epoch 79/200, Loss: 0.3686\n",
      "Epoch 80/200, Loss: 0.3698\n",
      "Epoch 81/200, Loss: 0.3690, Validation Accuracy: 0.8455\n",
      "Epoch 82/200, Loss: 0.3681\n",
      "Epoch 83/200, Loss: 0.3691\n",
      "Epoch 84/200, Loss: 0.3690\n",
      "Epoch 85/200, Loss: 0.3693\n",
      "Epoch 86/200, Loss: 0.3672\n",
      "Epoch 87/200, Loss: 0.3677\n",
      "Epoch 88/200, Loss: 0.3672\n",
      "Epoch 89/200, Loss: 0.3684\n",
      "Epoch 90/200, Loss: 0.3692\n",
      "Epoch 91/200, Loss: 0.3692, Validation Accuracy: 0.8459\n",
      "Epoch 92/200, Loss: 0.3691\n",
      "Epoch 93/200, Loss: 0.3676\n",
      "Epoch 94/200, Loss: 0.3676\n",
      "Epoch 95/200, Loss: 0.3669\n",
      "Epoch 96/200, Loss: 0.3680\n",
      "Epoch 97/200, Loss: 0.3706\n",
      "Epoch 98/200, Loss: 0.3661\n",
      "Epoch 99/200, Loss: 0.3676\n",
      "Epoch 100/200, Loss: 0.3654\n",
      "Epoch 101/200, Loss: 0.3658, Validation Accuracy: 0.8447\n",
      "Training stopped due to early stopping at epoch 100\n",
      "3 2 0.2\n",
      "Epoch 1/200, Loss: 0.5194, Validation Accuracy: 0.8263\n",
      "Epoch 2/200, Loss: 0.4066\n",
      "Epoch 3/200, Loss: 0.3989\n",
      "Epoch 4/200, Loss: 0.3976\n",
      "Epoch 5/200, Loss: 0.3916\n",
      "Epoch 6/200, Loss: 0.3897\n",
      "Epoch 7/200, Loss: 0.3880\n",
      "Epoch 8/200, Loss: 0.3870\n",
      "Epoch 9/200, Loss: 0.3877\n",
      "Epoch 10/200, Loss: 0.3863\n",
      "Epoch 11/200, Loss: 0.3870, Validation Accuracy: 0.8399\n",
      "Epoch 12/200, Loss: 0.3825\n",
      "Epoch 13/200, Loss: 0.3809\n",
      "Epoch 14/200, Loss: 0.3807\n",
      "Epoch 15/200, Loss: 0.3797\n",
      "Epoch 16/200, Loss: 0.3833\n",
      "Epoch 17/200, Loss: 0.3806\n",
      "Epoch 18/200, Loss: 0.3792\n",
      "Epoch 19/200, Loss: 0.3792\n",
      "Epoch 20/200, Loss: 0.3802\n",
      "Epoch 21/200, Loss: 0.3787, Validation Accuracy: 0.8363\n",
      "Epoch 22/200, Loss: 0.3783\n",
      "Epoch 23/200, Loss: 0.3771\n",
      "Epoch 24/200, Loss: 0.3777\n",
      "Epoch 25/200, Loss: 0.3780\n",
      "Epoch 26/200, Loss: 0.3757\n",
      "Epoch 27/200, Loss: 0.3758\n",
      "Epoch 28/200, Loss: 0.3768\n",
      "Epoch 29/200, Loss: 0.3772\n",
      "Epoch 30/200, Loss: 0.3767\n",
      "Epoch 31/200, Loss: 0.3757, Validation Accuracy: 0.8343\n",
      "Training stopped due to early stopping at epoch 30\n",
      "3 2 0.3\n",
      "Epoch 1/200, Loss: 0.5527, Validation Accuracy: 0.8172\n",
      "Epoch 2/200, Loss: 0.4206\n",
      "Epoch 3/200, Loss: 0.4108\n",
      "Epoch 4/200, Loss: 0.3996\n",
      "Epoch 5/200, Loss: 0.3984\n",
      "Epoch 6/200, Loss: 0.3952\n",
      "Epoch 7/200, Loss: 0.3894\n",
      "Epoch 8/200, Loss: 0.3890\n",
      "Epoch 9/200, Loss: 0.3885\n",
      "Epoch 10/200, Loss: 0.3903\n",
      "Epoch 11/200, Loss: 0.3867, Validation Accuracy: 0.8387\n",
      "Epoch 12/200, Loss: 0.3840\n",
      "Epoch 13/200, Loss: 0.3841\n",
      "Epoch 14/200, Loss: 0.3830\n",
      "Epoch 15/200, Loss: 0.3831\n",
      "Epoch 16/200, Loss: 0.3820\n",
      "Epoch 17/200, Loss: 0.3843\n",
      "Epoch 18/200, Loss: 0.3817\n",
      "Epoch 19/200, Loss: 0.3824\n",
      "Epoch 20/200, Loss: 0.3820\n",
      "Epoch 21/200, Loss: 0.3822, Validation Accuracy: 0.8375\n",
      "Epoch 22/200, Loss: 0.3810\n",
      "Epoch 23/200, Loss: 0.3817\n",
      "Epoch 24/200, Loss: 0.3799\n",
      "Epoch 25/200, Loss: 0.3786\n",
      "Epoch 26/200, Loss: 0.3810\n",
      "Epoch 27/200, Loss: 0.3776\n",
      "Epoch 28/200, Loss: 0.3788\n",
      "Epoch 29/200, Loss: 0.3793\n",
      "Epoch 30/200, Loss: 0.3774\n",
      "Epoch 31/200, Loss: 0.3786, Validation Accuracy: 0.8343\n",
      "Training stopped due to early stopping at epoch 30\n",
      "3 3 0\n",
      "Epoch 1/200, Loss: 0.5545, Validation Accuracy: 0.8208\n",
      "Epoch 2/200, Loss: 0.4100\n",
      "Epoch 3/200, Loss: 0.4020\n",
      "Epoch 4/200, Loss: 0.4008\n",
      "Epoch 5/200, Loss: 0.3989\n",
      "Epoch 6/200, Loss: 0.3957\n",
      "Epoch 7/200, Loss: 0.3934\n",
      "Epoch 8/200, Loss: 0.3933\n",
      "Epoch 9/200, Loss: 0.3938\n",
      "Epoch 10/200, Loss: 0.3889\n",
      "Epoch 11/200, Loss: 0.3905, Validation Accuracy: 0.8355\n",
      "Epoch 12/200, Loss: 0.3917\n",
      "Epoch 13/200, Loss: 0.3874\n",
      "Epoch 14/200, Loss: 0.3875\n",
      "Epoch 15/200, Loss: 0.3856\n",
      "Epoch 16/200, Loss: 0.3877\n",
      "Epoch 17/200, Loss: 0.3886\n",
      "Epoch 18/200, Loss: 0.3858\n",
      "Epoch 19/200, Loss: 0.3867\n",
      "Epoch 20/200, Loss: 0.3852\n",
      "Epoch 21/200, Loss: 0.3853, Validation Accuracy: 0.8367\n",
      "Epoch 22/200, Loss: 0.3847\n",
      "Epoch 23/200, Loss: 0.3819\n",
      "Epoch 24/200, Loss: 0.3871\n",
      "Epoch 25/200, Loss: 0.3844\n",
      "Epoch 26/200, Loss: 0.3816\n",
      "Epoch 27/200, Loss: 0.3836\n",
      "Epoch 28/200, Loss: 0.3829\n",
      "Epoch 29/200, Loss: 0.3825\n",
      "Epoch 30/200, Loss: 0.3804\n",
      "Epoch 31/200, Loss: 0.3818, Validation Accuracy: 0.8403\n",
      "Epoch 32/200, Loss: 0.3787\n",
      "Epoch 33/200, Loss: 0.3776\n",
      "Epoch 34/200, Loss: 0.3802\n",
      "Epoch 35/200, Loss: 0.3812\n",
      "Epoch 36/200, Loss: 0.3773\n",
      "Epoch 37/200, Loss: 0.3780\n",
      "Epoch 38/200, Loss: 0.3775\n",
      "Epoch 39/200, Loss: 0.3793\n",
      "Epoch 40/200, Loss: 0.3772\n",
      "Epoch 41/200, Loss: 0.3780, Validation Accuracy: 0.8403\n",
      "Epoch 42/200, Loss: 0.3781\n",
      "Epoch 43/200, Loss: 0.3755\n",
      "Epoch 44/200, Loss: 0.3772\n",
      "Epoch 45/200, Loss: 0.3763\n",
      "Epoch 46/200, Loss: 0.3758\n",
      "Epoch 47/200, Loss: 0.3765\n",
      "Epoch 48/200, Loss: 0.3765\n",
      "Epoch 49/200, Loss: 0.3768\n",
      "Epoch 50/200, Loss: 0.3772\n",
      "Epoch 51/200, Loss: 0.3748, Validation Accuracy: 0.8411\n",
      "Epoch 52/200, Loss: 0.3743\n",
      "Epoch 53/200, Loss: 0.3752\n",
      "Epoch 54/200, Loss: 0.3737\n",
      "Epoch 55/200, Loss: 0.3743\n",
      "Epoch 56/200, Loss: 0.3744\n",
      "Epoch 57/200, Loss: 0.3748\n",
      "Epoch 58/200, Loss: 0.3725\n",
      "Epoch 59/200, Loss: 0.3728\n",
      "Epoch 60/200, Loss: 0.3726\n",
      "Epoch 61/200, Loss: 0.3788, Validation Accuracy: 0.8371\n",
      "Training stopped due to early stopping at epoch 60\n",
      "3 3 0.1\n",
      "Epoch 1/200, Loss: 0.5741, Validation Accuracy: 0.8112\n",
      "Epoch 2/200, Loss: 0.4185\n",
      "Epoch 3/200, Loss: 0.4083\n",
      "Epoch 4/200, Loss: 0.4017\n",
      "Epoch 5/200, Loss: 0.4001\n",
      "Epoch 6/200, Loss: 0.3968\n",
      "Epoch 7/200, Loss: 0.3908\n",
      "Epoch 8/200, Loss: 0.3909\n",
      "Epoch 9/200, Loss: 0.3926\n",
      "Epoch 10/200, Loss: 0.3892\n",
      "Epoch 11/200, Loss: 0.3862, Validation Accuracy: 0.8367\n",
      "Epoch 12/200, Loss: 0.3894\n",
      "Epoch 13/200, Loss: 0.3853\n",
      "Epoch 14/200, Loss: 0.3841\n",
      "Epoch 15/200, Loss: 0.3848\n",
      "Epoch 16/200, Loss: 0.3830\n",
      "Epoch 17/200, Loss: 0.3803\n",
      "Epoch 18/200, Loss: 0.3824\n",
      "Epoch 19/200, Loss: 0.3812\n",
      "Epoch 20/200, Loss: 0.3798\n",
      "Epoch 21/200, Loss: 0.3800, Validation Accuracy: 0.8347\n",
      "Epoch 22/200, Loss: 0.3804\n",
      "Epoch 23/200, Loss: 0.3789\n",
      "Epoch 24/200, Loss: 0.3778\n",
      "Epoch 25/200, Loss: 0.3769\n",
      "Epoch 26/200, Loss: 0.3784\n",
      "Epoch 27/200, Loss: 0.3774\n",
      "Epoch 28/200, Loss: 0.3781\n",
      "Epoch 29/200, Loss: 0.3781\n",
      "Epoch 30/200, Loss: 0.3782\n",
      "Epoch 31/200, Loss: 0.3767, Validation Accuracy: 0.8443\n",
      "Epoch 32/200, Loss: 0.3751\n",
      "Epoch 33/200, Loss: 0.3747\n",
      "Epoch 34/200, Loss: 0.3768\n",
      "Epoch 35/200, Loss: 0.3748\n",
      "Epoch 36/200, Loss: 0.3768\n",
      "Epoch 37/200, Loss: 0.3746\n",
      "Epoch 38/200, Loss: 0.3752\n",
      "Epoch 39/200, Loss: 0.3726\n",
      "Epoch 40/200, Loss: 0.3738\n",
      "Epoch 41/200, Loss: 0.3733, Validation Accuracy: 0.8459\n",
      "Epoch 42/200, Loss: 0.3747\n",
      "Epoch 43/200, Loss: 0.3732\n",
      "Epoch 44/200, Loss: 0.3716\n",
      "Epoch 45/200, Loss: 0.3720\n",
      "Epoch 46/200, Loss: 0.3738\n",
      "Epoch 47/200, Loss: 0.3726\n",
      "Epoch 48/200, Loss: 0.3719\n",
      "Epoch 49/200, Loss: 0.3711\n",
      "Epoch 50/200, Loss: 0.3736\n",
      "Epoch 51/200, Loss: 0.3709, Validation Accuracy: 0.8443\n",
      "Epoch 52/200, Loss: 0.3727\n",
      "Epoch 53/200, Loss: 0.3727\n",
      "Epoch 54/200, Loss: 0.3718\n",
      "Epoch 55/200, Loss: 0.3721\n",
      "Epoch 56/200, Loss: 0.3724\n",
      "Epoch 57/200, Loss: 0.3710\n",
      "Epoch 58/200, Loss: 0.3715\n",
      "Epoch 59/200, Loss: 0.3710\n",
      "Epoch 60/200, Loss: 0.3727\n",
      "Epoch 61/200, Loss: 0.3721, Validation Accuracy: 0.8435\n",
      "Training stopped due to early stopping at epoch 60\n",
      "3 3 0.2\n",
      "Epoch 1/200, Loss: 0.5421, Validation Accuracy: 0.8080\n",
      "Epoch 2/200, Loss: 0.4167\n",
      "Epoch 3/200, Loss: 0.4063\n",
      "Epoch 4/200, Loss: 0.4003\n",
      "Epoch 5/200, Loss: 0.3978\n",
      "Epoch 6/200, Loss: 0.3935\n",
      "Epoch 7/200, Loss: 0.3903\n",
      "Epoch 8/200, Loss: 0.3904\n",
      "Epoch 9/200, Loss: 0.3871\n",
      "Epoch 10/200, Loss: 0.3872\n",
      "Epoch 11/200, Loss: 0.3859, Validation Accuracy: 0.8383\n",
      "Epoch 12/200, Loss: 0.3861\n",
      "Epoch 13/200, Loss: 0.3852\n",
      "Epoch 14/200, Loss: 0.3866\n",
      "Epoch 15/200, Loss: 0.3842\n",
      "Epoch 16/200, Loss: 0.3829\n",
      "Epoch 17/200, Loss: 0.3817\n",
      "Epoch 18/200, Loss: 0.3830\n",
      "Epoch 19/200, Loss: 0.3818\n",
      "Epoch 20/200, Loss: 0.3815\n",
      "Epoch 21/200, Loss: 0.3804, Validation Accuracy: 0.8343\n",
      "Epoch 22/200, Loss: 0.3818\n",
      "Epoch 23/200, Loss: 0.3780\n",
      "Epoch 24/200, Loss: 0.3810\n",
      "Epoch 25/200, Loss: 0.3803\n",
      "Epoch 26/200, Loss: 0.3816\n",
      "Epoch 27/200, Loss: 0.3789\n",
      "Epoch 28/200, Loss: 0.3795\n",
      "Epoch 29/200, Loss: 0.3788\n",
      "Epoch 30/200, Loss: 0.3774\n",
      "Epoch 31/200, Loss: 0.3782, Validation Accuracy: 0.8375\n",
      "Epoch 32/200, Loss: 0.3779\n",
      "Epoch 33/200, Loss: 0.3789\n",
      "Epoch 34/200, Loss: 0.3774\n",
      "Epoch 35/200, Loss: 0.3777\n",
      "Epoch 36/200, Loss: 0.3778\n",
      "Epoch 37/200, Loss: 0.3780\n",
      "Epoch 38/200, Loss: 0.3773\n",
      "Epoch 39/200, Loss: 0.3766\n",
      "Epoch 40/200, Loss: 0.3772\n",
      "Epoch 41/200, Loss: 0.3769, Validation Accuracy: 0.8435\n",
      "Epoch 42/200, Loss: 0.3775\n",
      "Epoch 43/200, Loss: 0.3759\n",
      "Epoch 44/200, Loss: 0.3763\n",
      "Epoch 45/200, Loss: 0.3761\n",
      "Epoch 46/200, Loss: 0.3766\n",
      "Epoch 47/200, Loss: 0.3758\n",
      "Epoch 48/200, Loss: 0.3778\n",
      "Epoch 49/200, Loss: 0.3745\n",
      "Epoch 50/200, Loss: 0.3761\n",
      "Epoch 51/200, Loss: 0.3756, Validation Accuracy: 0.8459\n",
      "Epoch 52/200, Loss: 0.3749\n",
      "Epoch 53/200, Loss: 0.3747\n",
      "Epoch 54/200, Loss: 0.3739\n",
      "Epoch 55/200, Loss: 0.3755\n",
      "Epoch 56/200, Loss: 0.3737\n",
      "Epoch 57/200, Loss: 0.3748\n",
      "Epoch 58/200, Loss: 0.3746\n",
      "Epoch 59/200, Loss: 0.3745\n",
      "Epoch 60/200, Loss: 0.3746\n",
      "Epoch 61/200, Loss: 0.3740, Validation Accuracy: 0.8395\n",
      "Training stopped due to early stopping at epoch 60\n",
      "3 3 0.3\n",
      "Epoch 1/200, Loss: 0.5442, Validation Accuracy: 0.8196\n",
      "Epoch 2/200, Loss: 0.4183\n",
      "Epoch 3/200, Loss: 0.4086\n",
      "Epoch 4/200, Loss: 0.4064\n",
      "Epoch 5/200, Loss: 0.4023\n",
      "Epoch 6/200, Loss: 0.3989\n",
      "Epoch 7/200, Loss: 0.3940\n",
      "Epoch 8/200, Loss: 0.3906\n",
      "Epoch 9/200, Loss: 0.3917\n",
      "Epoch 10/200, Loss: 0.3886\n",
      "Epoch 11/200, Loss: 0.3891, Validation Accuracy: 0.8335\n",
      "Epoch 12/200, Loss: 0.3880\n",
      "Epoch 13/200, Loss: 0.3863\n",
      "Epoch 14/200, Loss: 0.3858\n",
      "Epoch 15/200, Loss: 0.3889\n",
      "Epoch 16/200, Loss: 0.3853\n",
      "Epoch 17/200, Loss: 0.3857\n",
      "Epoch 18/200, Loss: 0.3834\n",
      "Epoch 19/200, Loss: 0.3818\n",
      "Epoch 20/200, Loss: 0.3813\n",
      "Epoch 21/200, Loss: 0.3811, Validation Accuracy: 0.8375\n",
      "Epoch 22/200, Loss: 0.3814\n",
      "Epoch 23/200, Loss: 0.3797\n",
      "Epoch 24/200, Loss: 0.3808\n",
      "Epoch 25/200, Loss: 0.3805\n",
      "Epoch 26/200, Loss: 0.3814\n",
      "Epoch 27/200, Loss: 0.3796\n",
      "Epoch 28/200, Loss: 0.3781\n",
      "Epoch 29/200, Loss: 0.3798\n",
      "Epoch 30/200, Loss: 0.3763\n",
      "Epoch 31/200, Loss: 0.3773, Validation Accuracy: 0.8431\n",
      "Epoch 32/200, Loss: 0.3751\n",
      "Epoch 33/200, Loss: 0.3788\n",
      "Epoch 34/200, Loss: 0.3786\n",
      "Epoch 35/200, Loss: 0.3785\n",
      "Epoch 36/200, Loss: 0.3761\n",
      "Epoch 37/200, Loss: 0.3771\n",
      "Epoch 38/200, Loss: 0.3790\n",
      "Epoch 39/200, Loss: 0.3754\n",
      "Epoch 40/200, Loss: 0.3776\n",
      "Epoch 41/200, Loss: 0.3748, Validation Accuracy: 0.8411\n",
      "Epoch 42/200, Loss: 0.3760\n",
      "Epoch 43/200, Loss: 0.3759\n",
      "Epoch 44/200, Loss: 0.3746\n",
      "Epoch 45/200, Loss: 0.3778\n",
      "Epoch 46/200, Loss: 0.3757\n",
      "Epoch 47/200, Loss: 0.3752\n",
      "Epoch 48/200, Loss: 0.3761\n",
      "Epoch 49/200, Loss: 0.3743\n",
      "Epoch 50/200, Loss: 0.3763\n",
      "Epoch 51/200, Loss: 0.3744, Validation Accuracy: 0.8455\n",
      "Epoch 52/200, Loss: 0.3732\n",
      "Epoch 53/200, Loss: 0.3749\n",
      "Epoch 54/200, Loss: 0.3733\n",
      "Epoch 55/200, Loss: 0.3732\n",
      "Epoch 56/200, Loss: 0.3743\n",
      "Epoch 57/200, Loss: 0.3734\n",
      "Epoch 58/200, Loss: 0.3720\n",
      "Epoch 59/200, Loss: 0.3734\n",
      "Epoch 60/200, Loss: 0.3756\n",
      "Epoch 61/200, Loss: 0.3740, Validation Accuracy: 0.8443\n",
      "Epoch 62/200, Loss: 0.3732\n",
      "Epoch 63/200, Loss: 0.3737\n",
      "Epoch 64/200, Loss: 0.3711\n",
      "Epoch 65/200, Loss: 0.3737\n",
      "Epoch 66/200, Loss: 0.3726\n",
      "Epoch 67/200, Loss: 0.3725\n",
      "Epoch 68/200, Loss: 0.3728\n",
      "Epoch 69/200, Loss: 0.3745\n",
      "Epoch 70/200, Loss: 0.3725\n",
      "Epoch 71/200, Loss: 0.3724, Validation Accuracy: 0.8447\n",
      "Epoch 72/200, Loss: 0.3725\n",
      "Epoch 73/200, Loss: 0.3708\n",
      "Epoch 74/200, Loss: 0.3726\n",
      "Epoch 75/200, Loss: 0.3726\n",
      "Epoch 76/200, Loss: 0.3715\n",
      "Epoch 77/200, Loss: 0.3746\n",
      "Epoch 78/200, Loss: 0.3712\n",
      "Epoch 79/200, Loss: 0.3707\n",
      "Epoch 80/200, Loss: 0.3725\n",
      "Epoch 81/200, Loss: 0.3714, Validation Accuracy: 0.8455\n",
      "Epoch 82/200, Loss: 0.3706\n",
      "Epoch 83/200, Loss: 0.3726\n",
      "Epoch 84/200, Loss: 0.3708\n",
      "Epoch 85/200, Loss: 0.3698\n",
      "Epoch 86/200, Loss: 0.3710\n",
      "Epoch 87/200, Loss: 0.3696\n",
      "Epoch 88/200, Loss: 0.3709\n",
      "Epoch 89/200, Loss: 0.3704\n",
      "Epoch 90/200, Loss: 0.3710\n",
      "Epoch 91/200, Loss: 0.3707, Validation Accuracy: 0.8459\n",
      "Epoch 92/200, Loss: 0.3703\n",
      "Epoch 93/200, Loss: 0.3703\n",
      "Epoch 94/200, Loss: 0.3695\n",
      "Epoch 95/200, Loss: 0.3704\n",
      "Epoch 96/200, Loss: 0.3693\n",
      "Epoch 97/200, Loss: 0.3687\n",
      "Epoch 98/200, Loss: 0.3709\n",
      "Epoch 99/200, Loss: 0.3700\n",
      "Epoch 100/200, Loss: 0.3697\n",
      "Epoch 101/200, Loss: 0.3702, Validation Accuracy: 0.8439\n",
      "Training stopped due to early stopping at epoch 100\n"
     ]
    }
   ],
   "source": [
    "num_conv_layers = [1, 2, 3]\n",
    "num_linear_layers = [1, 2, 3]\n",
    "dropout_rates = [0, 0.1, 0.2, 0.3]\n",
    "\n",
    "results_df = pd.DataFrame(columns=['num_conv_layers', 'num_linear_layers', 'dropout_rate', 'validation_accuracy'])\n",
    "\n",
    "for num_conv in num_conv_layers:\n",
    "    for num_linear in num_linear_layers:\n",
    "        for dropout_rate in dropout_rates:\n",
    "            model = CNN(num_conv_layers=num_conv, num_linear_layers=num_linear, dropout_rate=dropout_rate)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "            valid_accuracy = train(model, n_epochs=200)\n",
    "            results_df.loc[len(results_df)] = [num_conv, num_linear, dropout_rate, valid_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_conv_layers</th>\n",
       "      <th>num_linear_layers</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.846707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_conv_layers  num_linear_layers  dropout_rate  validation_accuracy\n",
       "27              3.0                1.0           0.3             0.846707"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[results_df['validation_accuracy'] == results_df['validation_accuracy'].max()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('grid_search.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
