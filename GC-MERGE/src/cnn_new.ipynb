{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime, date\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import load_npz\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_line = 'E116'\n",
    "max_epoch = 1000\n",
    "learning_rate = 1e-4\n",
    "num_graph_conv_layers = 2\n",
    "graph_conv_embed_size = 256\n",
    "num_lin_layers = 3\n",
    "lin_hidden_size = 256\n",
    "regression_flag = 0\n",
    "random_seed = 0\n",
    "\n",
    "chip_res = 10000\n",
    "hic_res = 10000\n",
    "num_hm = 6\n",
    "num_feat = int((hic_res/chip_res)*num_hm)\n",
    "num_classes = 2 if regression_flag == 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.getcwd()\n",
    "save_dir = os.path.join(base_path, 'data', cell_line, 'saved_runs')\n",
    "hic_sparse_mat_file = os.path.join(base_path, 'data', cell_line, 'hic_sparse.npz')\n",
    "np_nodes_lab_genes_file = os.path.join(base_path, 'data',  cell_line, \\\n",
    "    'np_nodes_lab_genes_reg' + str(regression_flag) + '.npy')\n",
    "np_hmods_norm_all_file = os.path.join(base_path, 'data', cell_line, \\\n",
    "    'np_hmods_norm_chip_' + str(chip_res) + 'bp.npy')\n",
    "df_genes_file = os.path.join(base_path, 'data', cell_line, 'df_genes_reg' + str(regression_flag) + '.pkl')\n",
    "df_genes = pd.read_pickle(df_genes_file)\n",
    "\n",
    "mat = load_npz(hic_sparse_mat_file)\n",
    "allNodes_hms = np.load(np_hmods_norm_all_file)\n",
    "hms = allNodes_hms[:, 1:] #only includes features, not node ids\n",
    "X = torch.tensor(hms).float().reshape(-1, num_feat) \n",
    "allNodes = allNodes_hms[:, 0].astype(int)\n",
    "geneNodes_labs = np.load(np_nodes_lab_genes_file)\n",
    "\n",
    "geneNodes = geneNodes_labs[:, -2].astype(int)\n",
    "allLabs = -1*np.ones(np.shape(allNodes))\n",
    "\n",
    "targetNode_mask = torch.tensor(geneNodes).long()\n",
    "\n",
    "if regression_flag == 0:\n",
    "    geneLabs = geneNodes_labs[:, -1].astype(int)\n",
    "    allLabs[geneNodes] = geneLabs\n",
    "    Y = torch.tensor(allLabs).long()\n",
    "else:\n",
    "    geneLabs = geneNodes_labs[:, -1].astype(float)\n",
    "    allLabs[geneNodes] = geneLabs\n",
    "    Y = torch.tensor(allLabs).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05972332496441421"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16699 / allLabs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.,  0.,  1.]), array([262907,   8847,   7852]))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(allLabs, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3734, 0.1118, 0.1085, 0.2100, 0.4372, 0.1698]],\n",
       "\n",
       "        [[0.0263, 0.2426, 0.0643, 0.0446, 0.1653, 0.0377]],\n",
       "\n",
       "        [[0.0592, 0.0870, 0.2615, 0.5445, 0.2337, 0.0248]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0009, 0.0110, 0.0024, 0.0000, 0.0026, 0.0032]],\n",
       "\n",
       "        [[0.0083, 0.3929, 0.0428, 0.0320, 0.0372, 0.0700]],\n",
       "\n",
       "        [[0.1449, 0.1034, 0.3776, 0.1160, 0.3957, 0.1118]]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_idx_shuff = torch.randperm(targetNode_mask.shape[0])\n",
    "fin_train = np.floor(0.7*pred_idx_shuff.shape[0]).astype(int)\n",
    "fin_valid = np.floor(0.85*pred_idx_shuff.shape[0]).astype(int)\n",
    "train_idx = pred_idx_shuff[:fin_train]\n",
    "valid_idx = pred_idx_shuff[fin_train:fin_valid]\n",
    "test_idx = pred_idx_shuff[fin_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LENGTH = 6\n",
    "NUM_CLASSES = 2 \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_conv_layers, num_linear_layers, dropout_rate=0.2):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        in_channels = 1\n",
    "        out_channels = 16\n",
    "        for i in range(num_conv_layers):\n",
    "            conv = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "            )\n",
    "            self.conv_layers.append(conv)\n",
    "            \n",
    "            in_channels = out_channels\n",
    "            out_channels *= 2\n",
    "        \n",
    "\n",
    "        self.linear_layers = nn.ModuleList()\n",
    "\n",
    "        if num_linear_layers > 1:\n",
    "            first_linear_out_size = 2**(4 + num_linear_layers)\n",
    "            for i in range(num_linear_layers-1):\n",
    "                fc = nn.Sequential(\n",
    "                    nn.LazyLinear(first_linear_out_size),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                self.linear_layers.append(fc)\n",
    "\n",
    "                first_linear_out_size /= 2\n",
    "        \n",
    "        self.final_linear = nn.LazyLinear(NUM_CLASSES)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv_layers[0](x)\n",
    "        \n",
    "        for layer in self.conv_layers[1:]:\n",
    "            out = layer(out)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "        out = self.flatten(out)\n",
    "        for layer in self.linear_layers:\n",
    "            out = layer(out)\n",
    "        \n",
    "        out = self.final_linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def calculate_accuracy(self, dataset):\n",
    "        data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        num_correct = 0\n",
    "        with torch.no_grad(:)\n",
    "            for batch_inputs, batch_labels in data_loader:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                output = model(batch_inputs)\n",
    "                pred = torch.argmax(output, dim=1)\n",
    "                num_correct += torch.sum(pred == batch_labels)\n",
    "            \n",
    "        return float(num_correct / len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X[targetNode_mask][train_idx]\n",
    "train_labels = torch.tensor(geneNodes_labs[train_idx][:, 1]).long()\n",
    "\n",
    "valid_data = X[targetNode_mask][valid_idx]\n",
    "valid_labels = torch.tensor(geneNodes_labs[valid_idx][:, 1]).long()\n",
    "\n",
    "test_data = X[targetNode_mask][test_idx]\n",
    "test_labels = torch.tensor(geneNodes_labs[test_idx][:, 1]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.unsqueeze(1)\n",
    "test_data = test_data.unsqueeze(1)\n",
    "valid_data = valid_data.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vrisandubey/Desktop/Capstone_MT_Q1_Proj-master/capstone_venv/lib/python3.7/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.4617\n",
      "Epoch 2/100, Loss: 0.3871\n",
      "Epoch 3/100, Loss: 0.3825\n",
      "Epoch 4/100, Loss: 0.3823\n",
      "Epoch 5/100, Loss: 0.3791\n",
      "Epoch 6/100, Loss: 0.3783\n",
      "Epoch 7/100, Loss: 0.3770\n",
      "Epoch 8/100, Loss: 0.3776\n",
      "Epoch 9/100, Loss: 0.3774\n",
      "Epoch 10/100, Loss: 0.3757\n",
      "Epoch 11/100, Loss: 0.3758\n",
      "Epoch 12/100, Loss: 0.3761\n",
      "Epoch 13/100, Loss: 0.3750\n",
      "Epoch 14/100, Loss: 0.3743\n",
      "Epoch 15/100, Loss: 0.3746\n",
      "Epoch 16/100, Loss: 0.3742\n",
      "Epoch 17/100, Loss: 0.3725\n",
      "Epoch 18/100, Loss: 0.3740\n",
      "Epoch 19/100, Loss: 0.3731\n",
      "Epoch 20/100, Loss: 0.3733\n",
      "Epoch 21/100, Loss: 0.3722\n",
      "Epoch 22/100, Loss: 0.3710\n",
      "Epoch 23/100, Loss: 0.3705\n",
      "Epoch 24/100, Loss: 0.3711\n",
      "Epoch 25/100, Loss: 0.3712\n",
      "Epoch 26/100, Loss: 0.3700\n",
      "Epoch 27/100, Loss: 0.3700\n",
      "Epoch 28/100, Loss: 0.3700\n",
      "Epoch 29/100, Loss: 0.3710\n",
      "Epoch 30/100, Loss: 0.3688\n",
      "Epoch 31/100, Loss: 0.3689\n",
      "Epoch 32/100, Loss: 0.3692\n",
      "Epoch 33/100, Loss: 0.3672\n",
      "Epoch 34/100, Loss: 0.3683\n",
      "Epoch 35/100, Loss: 0.3679\n",
      "Epoch 36/100, Loss: 0.3676\n",
      "Epoch 37/100, Loss: 0.3675\n",
      "Epoch 38/100, Loss: 0.3685\n",
      "Epoch 39/100, Loss: 0.3676\n",
      "Epoch 40/100, Loss: 0.3675\n",
      "Epoch 41/100, Loss: 0.3659\n",
      "Epoch 42/100, Loss: 0.3666\n",
      "Epoch 43/100, Loss: 0.3672\n",
      "Epoch 44/100, Loss: 0.3676\n",
      "Epoch 45/100, Loss: 0.3675\n",
      "Epoch 46/100, Loss: 0.3644\n",
      "Epoch 47/100, Loss: 0.3648\n",
      "Epoch 48/100, Loss: 0.3665\n",
      "Epoch 49/100, Loss: 0.3654\n",
      "Epoch 50/100, Loss: 0.3644\n",
      "Epoch 51/100, Loss: 0.3657\n",
      "Epoch 52/100, Loss: 0.3647\n",
      "Epoch 53/100, Loss: 0.3627\n",
      "Epoch 54/100, Loss: 0.3639\n",
      "Epoch 55/100, Loss: 0.3645\n",
      "Epoch 56/100, Loss: 0.3625\n",
      "Epoch 57/100, Loss: 0.3634\n",
      "Epoch 58/100, Loss: 0.3624\n",
      "Epoch 59/100, Loss: 0.3630\n",
      "Epoch 60/100, Loss: 0.3631\n",
      "Epoch 61/100, Loss: 0.3624\n",
      "Epoch 62/100, Loss: 0.3618\n",
      "Epoch 63/100, Loss: 0.3626\n",
      "Epoch 64/100, Loss: 0.3622\n",
      "Epoch 65/100, Loss: 0.3599\n",
      "Epoch 66/100, Loss: 0.3599\n",
      "Epoch 67/100, Loss: 0.3611\n",
      "Epoch 68/100, Loss: 0.3610\n",
      "Epoch 69/100, Loss: 0.3616\n",
      "Epoch 70/100, Loss: 0.3612\n",
      "Epoch 71/100, Loss: 0.3595\n",
      "Epoch 72/100, Loss: 0.3592\n",
      "Epoch 73/100, Loss: 0.3595\n",
      "Epoch 74/100, Loss: 0.3583\n",
      "Epoch 75/100, Loss: 0.3589\n",
      "Epoch 76/100, Loss: 0.3586\n",
      "Epoch 77/100, Loss: 0.3582\n",
      "Epoch 78/100, Loss: 0.3585\n",
      "Epoch 79/100, Loss: 0.3582\n",
      "Epoch 80/100, Loss: 0.3572\n",
      "Epoch 81/100, Loss: 0.3569\n",
      "Epoch 82/100, Loss: 0.3582\n",
      "Epoch 83/100, Loss: 0.3560\n",
      "Epoch 84/100, Loss: 0.3578\n",
      "Epoch 85/100, Loss: 0.3560\n",
      "Epoch 86/100, Loss: 0.3559\n",
      "Epoch 87/100, Loss: 0.3549\n",
      "Epoch 88/100, Loss: 0.3560\n",
      "Epoch 89/100, Loss: 0.3542\n",
      "Epoch 90/100, Loss: 0.3535\n",
      "Epoch 91/100, Loss: 0.3537\n",
      "Epoch 92/100, Loss: 0.3544\n",
      "Epoch 93/100, Loss: 0.3524\n",
      "Epoch 94/100, Loss: 0.3531\n",
      "Epoch 95/100, Loss: 0.3532\n",
      "Epoch 96/100, Loss: 0.3533\n",
      "Epoch 97/100, Loss: 0.3547\n",
      "Epoch 98/100, Loss: 0.3533\n",
      "Epoch 99/100, Loss: 0.3524\n",
      "Epoch 100/100, Loss: 0.3524\n"
     ]
    }
   ],
   "source": [
    "model = CNN(num_conv_layers=3, num_linear_layers=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch_inputs, batch_labels in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(batch_inputs)\n",
    "        loss = criterion(output, batch_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_data_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8411177396774292\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = TensorDataset(valid_data, valid_labels)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8439121842384338"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.calculate_accuracy(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2505"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
